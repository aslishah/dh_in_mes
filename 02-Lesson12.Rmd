# Lesson 12: Topic Modeling *&* TF-IDF (automatic text analysis)

## Goals:

- Introduction to topic modeling, or how to classify texts by shared content (“topics”).

## Software

* python
* jupyter notebook
* other python libraries
  - `wheel` (this package is helpful for the installation of `gensim` and `pyLDAvis`)
  - `nltk`
  - `gensim`
  - `spacy`
  - `pyLDAvis`
  - `matplotlib`
  - `numpy`
  - `pandas`
  - `plotly`
  - `pprint`

## Workbooks (jupyter notebooks)

- [for Windows](https://www.dropbox.com/s/b1jg5r3bmgforu5/TnT_L12_materials_final_Win.zip?dl=0)
- [for Mac and Linux](https://www.dropbox.com/s/zey9qtuht1tnsku/TnT_L12_materials_final.zip?dl=0)

## Installing: on Windows

On Mac and Linux things are easy, just follow the commands below; for Windows things are trickier and the easiest way would be to use Anaconda <https://www.anaconda.com/distribution/#download-section>.

Please, download and install. Most packages will come with Anaconda distribution; others you can install through its interface.

**NB:** After Anaconda is installed, it is still better to install libraries from the terminal opened directly from Anaconda and using the following command `conda install -c conda-forge gensim` (the latest version is not available via Anaconda interface).
More details: <https://radimrehurek.com/gensim/install.html>


## Installing: on Mac and Linux

## python libraries and additional data

``` bash

pip install nameOfLibrary

```

Lemmatization library (although we are not going to be using it in the tutorial)

``` bash
python -m spacy download en
```

## jupyter notebook

From command line (in your working folder)

``` bash

# installing
pip install jupyter

# starting
jupyter notebook

```

## installing from a jupyter notebook

The required libraries can also be installed directly from your Jupyter notebook as shown below---note `!` in front of `pip`. (*Note*: You might need to use `pip3` instead of `pip`, depending on your overall `python` setup)

```python
# installing

!pip install nltk
!pip install gensim
!pip install spacy
!pip install pyLDAvis

```

Your default browser should open something like this:

![](/images/L12/jupyter.png)

Click on an `*.ipynb` file to open a notebook.

## Files & Scripts

* Dispatch CSV files
* [Jupyter notebook](link)





## Thinking about themes and topics

Let's take a look at the following three examples and think about what themes and topics they cover. More importantly, let's think about *how* we "assign" those themes and topics. What is out thinking process? What textual elements do we keep in mind when we argue that such and such text is about such and such topics?

**Example 1**: *Thursday, March 27, 1862* --- LIGHT ARTILLERY

I am authorized by the Governor of Virginia to raise a Company of Light Artillery for the war. All those desirous of enlisting in this the most effective arm of the service, would do well to call at once at the office of Johnson & Guigon, Whig Building.

Uniforms and subsistence furnished.

A. B. GUIGON. mh 25—6t

**Example 2**: *Wednesday, August 17, 1864* --- Royal Marriages.

There is a story circulated in Germany, and some in Paris, that the match between the heir-apparent of the Imperial throne of Russia and the Princess Dagmar of Denmark having been definitively broken off, another is in the course of negotiation between His Imperial Highness and the Princess Helens of England.

**Example 3**: *Monday, June 22, 1863* --- NEWS FROM EUROPE.

The steamship Scotia arrived at New York on Thursday from Europe, with foreign news to the 7th inst. The news is not important. The Confederate steamer Lord Clyde was searched by order of the British Government, but nothing contraband being found on board her she was permitted to sail. The Russians have been defeated near Grochoury by the Polish insurgents. The three Powers have sent an earnest note to Russia, asking for a representative Government, a general amnesty, and an immediate cessation of hostilities in Poland.

## Topic Modeling

The formal definition of *topic modeling* is "..." (Source: ...)

## Implementation

**1. Preparing Data.** First, we need to make sure to convert the Dispatch into a suitable format. As I stressed before, TSV/CSV tabular format is one of the most universal. Let's go back to our reformatting script and change it in such a way that it collects data in the TSV format (TSV will help us to avoid the issue with commas in the CSV format) and aggregates all articles published in the same year in one file. You can think of writing a function that takes three arguments:

- (1) a path to the folder with downloaded initial XML files;
- (2) a path to the folder where you want to save the results; and
- (3) the year value. You will need to add some lines of code that test if each specific newspaper issue has been published in a particular year (your third argument): if yes, you collect it; else, you skip it.

This script will allow us to aggregate quite a significant amount if text into more manageable chunks (it will give us 5 files instead of 1300+). The solution script will be added to the end of this lesson later.

**2. Necessary Libraries** ....

...

**3. Loading Data** ...

...

**4. Preprocessing Data** ...

...

**5. Training a Model** ...


**

## TF-IDF

TF-IDF is the most common automatic method for identifying keywords in texts. This method is the standard approach for identifying keywords. It was first proposed almost fifty years ago by @SPARCKJONESStatistical1972; @RamsayReading2011, in Chapter 1, offers a detailed humanistic explanation of this approach.

This approach is statistical in its nature and therefore requires a sizable collection of texts for meaningful results. In the case of small collections you are likely to observe that the selection of keywords for the same text change significantly with every new addition to the collection of texts. This is because the method takes into account frequencies of each word and the number of documents in which this word occurs. For example, if your collection includes 10 articles that discuss different aspects of the ʿAbbāsid caliphate, with some focused on politics, some on economics, and some on culture, the word "ʿAbbāsid" will most likely not be included into the list of suggested keywords (since all articles have this term), but words like "politics", "economics", and "culture" will be in this list, since they appear only in some articles. If you add ten other articles to your collection---and these will deal, say, with the Ottomans---then the word "ʿAbbāsid" will crawl up in the list of keywords for articles dealing with the ʿAbbasids. In other words, if your collection includes texts on the same broad topic (say, the ʿAbbāsids), the TF-IDF algorithm will assign high *keywordness* to those terms that point to narrower subjects within the broader subject of the ʿAbbāsids.

In cases when one has to run the topic modeling algorithm on a very large collection of texts, TF-IDF can be used in order to reduce the size of the corpus. That is to say, the corpus is first reduced to the TF-IDF abstractions and then the topic modeling algorithm is applied to these abstractions rather than to the complete initial texts.

TF-IDF abstractions can also be used to identify texts on similar topics across a large corpus of texts. In this approach one would calculate the distance between the vectors of TF-IDF abstractions. This approach mathematically checks whether to what extent two different documents share keywords and to what extent the numeric values of these keywords are similar: the more keywords they share and the more similar the keyword values are, the more similar the texts will be. ([More details...](https://maximromanov.github.io/univie2020/lesson-10.html).)

## Homework{#HWL12}

- Finish the assigned task;
- Annotate your script (i.e., add comment to every line of code describing what is happenning there);

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfolder in your repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)


## Solution{#SolutionL12}

The solutions will be added soon...

```{r engine='python', highlight=TRUE, eval=FALSE}
# REFORMATTING THE DISPATCH INTO TSV FILES

import re
import os

source = "./Dispatch/"
target = "./Dispatch_Processed_TSV/"  # needs to be created beforehand!

def convertDispatchToCSV(source, target, YEAR):
    print("Collecting data for year: %s" % YEAR)
    issueVar = []
    lof = os.listdir(source)
    for f in lof:
        if f.startswith("dltext"):  # fileName test
            c = 0  # technical counter
            with open(source + f, "r", encoding="utf8") as f1:
                text = f1.read()
                date = re.search(r'<date value="([\d-]+)"', text).group(1)

                if date[:4] == str(YEAR):
                    split = re.split("<div3 ", text)

                    for s in split[1:]:
                        s = "<div3 " + s  # a step to restore the integrity of items

                        try:
                            unitType = re.search(r'type="([^\"]+)"', s).group(1)
                        except:
                            unitType = "noType"

                        try:
                            header = re.search(r'<head.*</head>', s).group(0)
                            header = re.sub("<[^<]+>", "", header)

                        except:
                            header = "NO HEADER"

                        text = s
                        text = re.sub("<[^<]+>", " ", text)
                        text = re.sub(" +\n|\n +", "\n", text)
                        text = text.strip()
                        text = re.sub("\n+", ";;; ", text)
                        text = re.sub(" +", " ", text)
                        text = re.sub(r" ([\.,:;!])", r"\1", text)

                        itemID = date + "_" + unitType + "_%03d" % c

                        if len(re.sub("\W+", "", text)) != 0:
                            var = "\t".join([itemID, date, unitType, header, text])
                            issueVar.append(var)

    print("\tcollected: %d items" % len(issueVar))
    issueNew = "\n".join(issueVar)
    header = "\t".join(["id", "date", "type", "header", "text"])
    with open(target + "Dispatch_%s.tsv" % str(YEAR), "w", encoding="utf8") as f9:
        f9.write(header + "\n" + issueNew)

convertDispatchToCSV(source, target, 1860)
convertDispatchToCSV(source, target, 1861)
convertDispatchToCSV(source, target, 1862)
convertDispatchToCSV(source, target, 1863)
convertDispatchToCSV(source, target, 1864)
convertDispatchToCSV(source, target, 1865)
```


This function will preprocess data for topic modeling: each text is converted into a list and high frequency words are filtered out; also, empty articles (`ad-blank`) are removed.

```{r engine='python', highlight=TRUE, eval=FALSE}
from gensim.utils import simple_preprocess
import gensim.corpora
import gensim
import pandas as pd
import re

def remove_words(texts, word_list_filter):
    return [[word for word in simple_preprocess(str(doc)) if word not in word_list_filter] for doc in texts]

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations


# MAIN FUNCTION
def prepDispatchLightByYears(dispatchSubfolder, dispatchFile):
    print(">> preparing data for: %s" % dispatchFile)

    df = pd.read_csv(dispatchSubfolder + dispatchFile, sep="\t", header=0)

    dispatch = df
    # drop=True -- use it to avoid creating a new column with the old index values
    dispatch = dispatch.reset_index(drop=True)

    # add a column with all dates of each month changed to 1 (we can use that to aggregate our data into months)
    dispatch["month"] = [re.sub("-\d\d$", "", str(i)) for i in dispatch["date"]]

    # reorder columns
    dispatch = dispatch[["id", "month", "date", "type", "header", "text"]]

    dispatch["month"] = pd.to_datetime(dispatch["month"], format="%Y-%m")
    dispatch["date"] = pd.to_datetime(dispatch["date"], format="%Y-%m-%d")

    dispatch_light = dispatch[dispatch.type != "ad-blank"]
    dispatch_light = dispatch_light.reset_index(drop=True)

    dispatch_light["textData"] = dispatch_light["text"]
    dispatch_light["textData"] = [re.sub("\W+", " ", str(i).lower()) for i in dispatch_light["textData"]]
    dispatch_light["textData"] = [re.sub(" +", " ", str(i).lower()) for i in dispatch_light["textData"]]

    dispatch_light["textDataLists"] = list(sent_to_words(dispatch_light["textData"].copy()))

    # you can expand the stop word list by adding more high frequency words
    stop_words_custom = ["the", "of", "and", "to", "in", "a", "that", "for", "on", "was", "is", "at", "be", "by",
                    "from", "his", "he", "it", "with", "as", "this", "will", "which", "have", "or", "are",
                    "they", "their", "not", "were", "been", "has", "our", "we", "all", "but", "one", "had",
                    "who", "an", "no", "i", "them", "about", "him", "two", "upon", "may", "there", "any",
                    "some", "so", "men", "when", "if", "day", "her", "under", "would", "c", "such", "made",
                    "up", "last", "j", "time", "years", "other", "into", "said", "new", "very", "five",
                    "after", "out", "these", "shall", "my", "w", "more", "its", "now", "before", "three",
                    "m", "than", "h", "o'clock", "old", "being", "left", "can", "s", "man", "only", "same",
                    "act", "first", "between", "above", "she", "you", "place", "following", "do", "per",
                    "every", "most", "near", "us", "good", "should", "having", "great", "also", "over",
                    "r", "could", "twenty", "people", "those", "e", "without", "four", "received", "p", "then",
                    "what", "well", "where", "must", "says", "g", "large", "against", "back", "000", "through",
                    "b", "off", "few", "me", "sent", "while", "make", "number", "many", "much", "give",
                    "1", "six", "down", "several", "high", "since", "little", "during", "away", "until",
                    "each", "5", "year", "present", "own", "t", "here", "d", "found", "reported", "2",
                    "right", "given", "age", "your", "way", "side", "did", "part", "long", "next", "fifty",
                    "another", "1st", "whole", "10", "still", "among", "3", "within", "get", "named", "f",
                    "l", "himself", "ten", "both", "nothing", "again", "n", "thirty", "eight", "took",
                    "never", "came", "called", "small", "passed", "just", "brought", "4", "further",
                    "yet", "half", "far", "held", "soon", "main", "8", "second", "however", "say",
                    "heavy", "thus", "hereby", "even", "ran", "come", "whom", "like", "cannot", "head",
                    "ever", "themselves", "put", "12", "cause", "known", "7", "go", "6", "once", "therefore",
                    "thursday", "full", "apply", "see", "though", "seven", "tuesday", "11", "done",
                    "whose", "let", "how", "making", "immediately", "forty", "early", "wednesday",
                    "either", "too", "amount", "fact", "heard", "receive", "short", "less", "100",
                    "know", "might", "except", "supposed", "others", "doubt", "set", "works"]

    # TEXT CLEANING
    dispatch_light["textDataListsFiltered"] = remove_words(dispatch_light["textDataLists"], stop_words_custom)

    dispatch_light = dispatch_light[["id", "textDataListsFiltered"]]
    dispatch_light.to_csv("./Dispatch_Processed_TSV/%s_Light_Preprocessed_New.tsv" % dispatchFile[:-4], sep="\t", index=False)


dispatchSubfolder = "./Dispatch_Processed_TSV/"

prepDispatchLightByYears(dispatchSubfolder, "Dispatch_1860.tsv")
prepDispatchLightByYears(dispatchSubfolder, "Dispatch_1861.tsv")
prepDispatchLightByYears(dispatchSubfolder, "Dispatch_1862.tsv")
prepDispatchLightByYears(dispatchSubfolder, "Dispatch_1863.tsv")
prepDispatchLightByYears(dispatchSubfolder, "Dispatch_1864.tsv")
prepDispatchLightByYears(dispatchSubfolder, "Dispatch_1865.tsv")
```


```{r engine='python', highlight=TRUE, eval=FALSE}


```

```{r engine='python', highlight=TRUE, eval=FALSE}


```

```{r engine='python', highlight=TRUE, eval=FALSE}


```

```{r engine='python', highlight=TRUE, eval=FALSE}


```
