# Lesson 12: Topic Modeling *&* TF-IDF (automatic text analysis)

## Goals:

- Introduction to topic modeling, or how to classify texts by shared content (“topics”).

## Software

* python
* other python libraries
  - `wheel` (this package is helpful for the installation of `gensim` and `pyLDAvis`)
  - `nltk`
  - `gensim`
  - `spacy`
  - `pyLDAvis`
  - `matplotlib`
  - `numpy`
  - `pandas`
  - `plotly`
  - `pprint`
* *alternatively*: jupyter notebook (see the last part of the Lesson)

<!--
## Workbooks (jupyter notebooks)

- [for Windows](https://www.dropbox.com/s/b1jg5r3bmgforu5/TnT_L12_materials_final_Win.zip?dl=0)
- [for Mac and Linux](https://www.dropbox.com/s/zey9qtuht1tnsku/TnT_L12_materials_final.zip?dl=0)
-->

### Installing: on Windows

On Mac and Linux things are easy, just follow the commands below; for Windows things are trickier and the easiest way would be to use Anaconda <https://www.anaconda.com/distribution/#download-section>.

Please, download and install. Most packages will come with Anaconda distribution; others you can install through its interface.

**NB:** After Anaconda is installed, it is still better to install libraries from the terminal opened directly from Anaconda and using the following command `conda install -c conda-forge gensim` (the latest version is not available via Anaconda interface).
More details: <https://radimrehurek.com/gensim/install.html>


#### Installing: on Mac and Linux

Python libraries and additional data can be installed in the following manner

```bash
pip install nameOfLibrary
```

Although a better way would be (where `python3` is the exact version of `python` that you are using):

```bash
python3 -m pip install nameOfLibrary
```

Lemmatization library (although we are not going to be using it in the tutorial)

``` bash
python -m spacy download en
```

## Thinking about themes and topics

### First set of Examples

Let's take a look at the following three examples and think about what themes and topics they cover. More importantly, let's think about *how* we "assign" those themes and topics. What is out thinking process? What textual elements do we keep in mind when we argue that such and such text is about such and such topics?

**Example 1**: *March 27, 1862* --- Light Artillery

I am authorized by the Governor of Virginia to raise a Company of Light Artillery for the war. All those desirous of enlisting in this the most effective arm of the service, would do well to call at once at the office of Johnson & Guigon, Whig Building. Uniforms and subsistence furnished. A.B.GUIGON. mh 25—6t

**Example 2**: *August 17, 1864* --- Royal Marriages.

There is a story circulated in Germany, and some in Paris, that the match between the heir-apparent of the Imperial throne of Russia and the Princess Dagmar of Denmark having been definitively broken off, another is in the course of negotiation between His Imperial Highness and the Princess Helens of England.

**Example 3**: *June 22, 1863* --- News from Europe.

The steamship Scotia arrived at New York on Thursday from Europe, with foreign news to the 7th inst. The news is not important. The Confederate steamer Lord Clyde was searched by order of the British Government, but nothing contraband being found on board her she was permitted to sail. The Russians have been defeated near Grochoury by the Polish insurgents. The three Powers have sent an earnest note to Russia, asking for a representative Government, a general amnesty, and an immediate cessation of hostilities in Poland.

### Second Set of Examples

**Example 1**: *March 27, 1862* --- Light Artillery

I am authorized by the Governor of Virginia to raise a Company of Light Artillery for the war. All those desirous of enlisting in this the most effective arm of the service, would do well to call at once at the office of Johnson & Guigon, Whig Building. Uniforms and subsistence furnished. A.B.Guigon. mh 25—6t


**Example 2**: *July 17, 1861* --- Another improvement in fire-arms

Mr. T.W.Cofer, of Portsmouth, Va., has just completed an improvement in revolving fire-arms by which the process of loading is so much facilitated that the ordinary Colt or other Revolver, with this improvement attached, may be loaded and discharged with fourfold rapidity. Mr.Cofer left that place for Richmond yesterday for the purpose of securing a patent for his invention.


**Example 3**: *January 21, 186!* --- [Advertisement]

Just received and for sale low, a complete assortment of Colt's, Sharpe's, Smith *&* Wesson's, and other Revolvers. A call is solicited from those in want of anything in the Gun way. T.W.Tignor, Gun Maker, Main st., below the Market.


**Example 4**: *December 17, 1861* --- Unlawful shooting

On Saturday night, while Peter Padractti was pursuing the even tenor of his way on 11th and Cary streets, a man in a sky blue coat approached and discharged a pistol at him, and made use of most fearful expletives, which were quite enough to alarm a peaceful citizen. The strange individual, who was soon afterward arrested, gave his name as William Leftwich. When arraigned before the Mayor yesterday, he represented that he was a soldier on furlough, and was quite oblivious when he performed the deed which he now deeply deplored. The Mayor remanded him for indictment.

**Example 5**: *January 18, 1861* --- South Carolina Legislature

Charleston, S.C., January 17. The Senate report of the Military Committee, for raising four companies of artillery to meet the exigencies of the times, that demand South Carolina to be on a war footing, meets no opposers from any quarter, it being the general impression that she should have permanent military establishments for garrison purposes in the State fortifications. This establishment, the Committee recommend, to consist of a regiment of four companies, as it will form the nucleus around which volunteers and militia can rally, and will, besides, be a peace establishment, or furnish South Carolina 's quota in the army of the Southern Confederacy. The Senate went into secret session on the proposition to lay a submarine telegraph between Charleston, Morris' Island, Forts Moultrie, Johnson, Castle Pinckney, *&*c.

**Example 6**: *May 29, 1861* --- For the defence of Virginia

Twenty five men are wanted to enlist in the ranks of the Henrico Artillery under the command of Major Johnson H. Sands now encamped at Richmond College, one mile from this city, already mustered into service, and daily expecting orders to go into active duty. Rendezvous on Main street, between 7th and 8th. Hours between 10 A.M. and 3 P.M. H.Lansing Burrows, Recruiting Officer.

<!--
## Topic Modeling

The formal definition of *topic modeling* is "..." (Source: ...)
-->

## Implementation

**1. Preparing Data.** First, we need to make sure to convert the Dispatch into a suitable format. As I stressed before, TSV/CSV tabular format is one of the most universal. Let's go back to our reformatting script and change it in such a way that it collects data in the TSV format (TSV will help us to avoid the issue with commas in the CSV format) and aggregates all articles published in the same year in one file. You can think of writing a function that takes three arguments:

- (1) a path to the folder with downloaded initial XML files;
- (2) a path to the folder where you want to save the results; and
- (3) the year value. You will need to add some lines of code that test if each specific newspaper issue has been published in a particular year (your third argument): if yes, you collect it; else, you skip it.

This script will allow us to aggregate quite a significant amount if text into more manageable chunks (it will give us 5 files instead of 1300+). The solution script will be added to the end of this lesson later.

"./files/tm/tm_1_processing_dispatch_data_to_TSV.py"

<!--
```{python eval=FALSE, file="./files/tm/tm_1_processing_dispatch_data_to_TSV.py", include=TRUE}
```
-->


```{r comment=""}
cat(readLines('tm_1_processing_dispatch_data_to_TSV.py'), sep = '\n')
```


**2. Necessary Libraries** ....

...

**3. Loading Data** ...

...

**4. Preprocessing Data** ...

...

**5. Training a Model** ...


**

## TF-IDF

TF-IDF is the most common automatic method for identifying keywords in texts. This method is the standard approach for identifying keywords. It was first proposed almost fifty years ago by @SPARCKJONESStatistical1972; @RamsayReading2011, in Chapter 1, offers a detailed humanistic explanation of this approach.

This approach is statistical in its nature and therefore requires a sizable collection of texts for meaningful results. In the case of small collections you are likely to observe that the selection of keywords for the same text change significantly with every new addition to the collection of texts. This is because the method takes into account frequencies of each word and the number of documents in which this word occurs. For example, if your collection includes 10 articles that discuss different aspects of the ʿAbbāsid caliphate, with some focused on politics, some on economics, and some on culture, the word "ʿAbbāsid" will most likely not be included into the list of suggested keywords (since all articles have this term), but words like "politics", "economics", and "culture" will be in this list, since they appear only in some articles. If you add ten other articles to your collection---and these will deal, say, with the Ottomans---then the word "ʿAbbāsid" will crawl up in the list of keywords for articles dealing with the ʿAbbasids. In other words, if your collection includes texts on the same broad topic (say, the ʿAbbāsids), the TF-IDF algorithm will assign high *keywordness* to those terms that point to narrower subjects within the broader subject of the ʿAbbāsids.

In cases when one has to run the topic modeling algorithm on a very large collection of texts, TF-IDF can be used in order to reduce the size of the corpus. That is to say, the corpus is first reduced to the TF-IDF abstractions and then the topic modeling algorithm is applied to these abstractions rather than to the complete initial texts.

TF-IDF abstractions can also be used to identify texts on similar topics across a large corpus of texts. In this approach one would calculate the distance between the vectors of TF-IDF abstractions. This approach mathematically checks whether to what extent two different documents share keywords and to what extent the numeric values of these keywords are similar: the more keywords they share and the more similar the keyword values are, the more similar the texts will be. ([More details...](https://maximromanov.github.io/univie2020/lesson-10.html).)

## Implementation in Jupyter Notebook

Jupyter notebooks is a very popular alternative to regular python scripts. You can try the same code with a jupyter notebook as well. There are some advantages and disadvantages to using them. JN can be quite useful when you are experimenting with some fast operations when you explore your dataset and want to try different things like graphs or data reshaping. If your code requires long time to run, JN maybe rather inconvenient to use. 

### jupyter notebook

From command line (in your working folder)

``` bash
# installing
pip install jupyter

# starting
jupyter notebook
```

<!--
## installing from a jupyter notebook

The required libraries can also be installed directly from your Jupyter notebook as shown below---note `!` in front of `pip`. (*Note*: You might need to use `pip3` instead of `pip`, depending on your overall `python` setup)

```python
# installing
!pip install nltk
!pip install gensim
!pip install spacy
!pip install pyLDAvis
```
-->

Your default browser should open something like this:

![](./files/jupyter01.png)

Click on an `*.ipynb` file to open a notebook.

## Files & Scripts

* [Script that converts the Dispatch to TSV](./files/tm/0_processing_dispatch_data_to_TSV.py) files necessary for the Jupyter Notebook; generate these TSV files first, then you can start the Notebook
* [TM - Jupyter Notebook](./files/tm/0_tModeling_Dispatch_clean.ipynb)
  * For Windows: you will need to deactivate `%time` in those blocks of code that have it (either delete it, or comment it out with `#`).

## Homework{#HWL12}

- Finish the assigned task;
- Annotate your script (i.e., add comment to every line of code describing what is happenning there);

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfolder in your repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)


## Solution{#SolutionL12}

The solutions will be added soon...

```{r engine='python', highlight=TRUE, eval=FALSE}
# the script converts Dispatch data into TSV and prepares text for topic modeling

import re, os, io
import gensim
from gensim.utils import simple_preprocess
import pandas as pd

source = "./Dispatch/"
target = "./Dispatch_Processed_TSV/"  # needs to be created beforehand!

def remove_words(texts, word_list_filter):
    return [[word for word in simple_preprocess(str(doc)) if word not in word_list_filter] for doc in texts]

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

def convertDispatchToCSV(source, target, YEAR):
    print("Collecting data for year: %s" % YEAR)
    issueVar = []
    lof = os.listdir(source)
    for f in lof:
        if f.startswith("dltext"):  # fileName test
            c = 0  # technical counter
            with open(source + f, "r", encoding="utf8") as f1:
                text = f1.read()
                date = re.search(r'<date value="([\d-]+)"', text).group(1)

                if date[:4] == str(YEAR):
                    split = re.split("<div3 ", text)

                    for s in split[1:]:
                        c += 1
                        s = "<div3 " + s  # a step to restore the integrity of items

                        try:
                            unitType = re.search(
                                r'type="([^\"]+)"', s).group(1)
                        except:
                            unitType = "noType"

                        try:
                            header = re.search(
                                r'<head.*</head>', s).group(0)
                            header = re.sub("<[^<]+>", "", header)

                        except:
                            header = "NO HEADER"

                        text = s
                        text = re.sub("<[^<]+>", " ", text)
                        text = re.sub(r"\t", " ", text)
                        text = re.sub(" +\n|\n +", "\n", text)
                        text = text.strip()
                        text = re.sub("\n+", ";;; ", text)
                        text = re.sub(" +", " ", text)
                        text = re.sub(r" ([\.,:;!])", r"\1", text)

                        itemID = date + "_" + unitType + "_%04d" % c

                        if len(re.sub("\W+", "", text)) != 0:
                            var = "\t".join(
                                [itemID, date, unitType, header, text])
                            issueVar.append(var)

    print("\tcollected: %d items" % len(issueVar))
    issueNew = "\n".join(issueVar)
    header = "\t".join(["id", "date", "type", "header", "text"])
    final = header + "\n" + issueNew


    # Now, we prepare text data for TM (into a separate column)
    entitiesFinalStringIO = io.StringIO(final)
    df = pd.read_csv(entitiesFinalStringIO, sep="\t", header=0)

    dispatch = df
    # drop=True -- use it to avoid creating a new column with the old index values
    dispatch = dispatch.reset_index(drop=True)

    # add a column with all dates of each month changed to 1 (we can use that to aggregate our data into months)
    dispatch["month"] = [re.sub("-\d\d$", "", str(i)) for i in dispatch["date"]]

    # reorder columns
    dispatch = dispatch[["id", "month", "date", "type", "header", "text"]]

    dispatch["month"] = pd.to_datetime(dispatch["month"], format="%Y-%m")
    dispatch["date"] = pd.to_datetime(dispatch["date"], format="%Y-%m-%d")

    dispatch = dispatch[dispatch.type != "ad-blank"]
    dispatch = dispatch.reset_index(drop=True)

    dispatch["textData"] = dispatch["text"]
    dispatch["textData"] = [re.sub("\W+", " ", str(i).lower()) for i in dispatch["textData"]]
    dispatch["textData"] = [re.sub(" +", " ", str(i).lower()) for i in dispatch["textData"]]

    dispatch["textDataLists"] = list(sent_to_words(dispatch["textData"].copy()))

    # you can expand the stop word list by adding more high frequency words
    stop_words_custom = ["the", "of", "and", "to", "in", "a", "that", "for", "on", "was", "is", "at", "be", "by", "from", "his", "he", "it", "with", "as", "this", "will", "which", "have", "or", "are", "amp",
                    "they", "their", "not", "were", "been", "has", "our", "we", "all", "but", "one", "had", "who", "an", "no", "i", "them", "about", "him", "two", "upon", "may", "there", "any",
                    "some", "so", "men", "when", "if", "day", "her", "under", "would", "c", "such", "made", "up", "last", "j", "time", "years", "other", "into", "said", "new", "very", "five",
                    "after", "out", "these", "shall", "my", "w", "more", "its", "now", "before", "three", "m", "than", "h", "th", "o'clock", "o", "old", "being", "left", "can", "s", "man", "only", "same",
                    "act", "first", "between", "above", "she", "you", "place", "following", "do", "per", "every", "most", "near", "us", "good", "should", "having", "great", "also", "over",
                    "r", "could", "twenty", "people", "those", "e", "without", "four", "received", "p", "then", "what", "well", "where", "must", "says", "g", "large", "against", "back", "through",
                    "b", "off", "few", "me", "sent", "while", "make", "number", "many", "much", "give", "six", "down", "several", "high", "since", "little", "during", "away", "until",
                    "each", "year", "present", "own", "t", "here", "d", "found", "reported", "right", "given", "age", "your", "way", "side", "did", "part", "long", "next", "fifty",
                    "another", "1st", "whole", "10", "still", "among", "3", "within", "get", "named", "f", "l", "himself", "ten", "both", "nothing", "again", "n", "thirty", "eight", "took",
                    "never", "came", "called", "small", "passed", "just", "brought", "4", "further", "yet", "half", "far", "held", "soon", "main", "8", "second", "however", "say",
                    "heavy", "thus", "hereby", "even", "ran", "come", "whom", "like", "cannot", "head", "ever", "themselves", "put", "12", "cause", "known", "7", "go", "6", "once", "therefore",
                    "thursday", "full", "apply", "see", "though", "seven", "tuesday", "11", "done", "whose", "let", "how", "making", "immediately", "forty", "early", "wednesday",
                    "either", "too", "amount", "fact", "heard", "receive", "short", "less", "100",
                    "know", "might", "except", "supposed", "others", "doubt", "set", "works"]

    # TEXT CLEANING
    dispatch["textDataLists"] = remove_words(dispatch["textDataLists"], stop_words_custom)
    dispatch = dispatch[["id", "month", "date", "type", "header", "text", "textDataLists"]]
    dispatch.to_csv(target + "Dispatch_%s_tmReady.tsv" % str(YEAR), sep="\t", index=False)


convertDispatchToCSV(source, target, 1860)
convertDispatchToCSV(source, target, 1861)
convertDispatchToCSV(source, target, 1862)
convertDispatchToCSV(source, target, 1863)
convertDispatchToCSV(source, target, 1864)
convertDispatchToCSV(source, target, 1865)
```


```{r engine='python', highlight=TRUE, eval=FALSE}


```