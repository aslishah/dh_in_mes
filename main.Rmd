--- 
title: "070172-1 UE Methodological course - Introduction to DH: Tools & Techniques (2020W) `Memex Edition`"
author: "Maxim G. Romanov"
date: "`r Sys.Date()`"
description: "This is a collection of relevant materials for the class '070172-1 UE Methodological course - Introduction to DH: Tools & Techniques (2020W) `Memex Edition`', offered at the University of Vienna and taught by Maxim G. Romanov"
url: 'https\://maximromanov.github.io/univie2020/'
github-repo: "maximromanov/univie2020"
apple-touch-icon: "touch-icon.png"
apple-touch-icon-size: 120
favicon: "favicon.ico"
documentclass: book
link-citations: yes
bibliography:
- book.bib
- packages.bib
biblio-style: apalike
---

# Preliminaries{-}

This is a collection of relevant materials for a DH course by the Department of History, the University of Vienna.

* Course: 070172-1 UE Methodological course - Introduction to DH: Tools & Techniques (2020W) `Memex Edition`
* u:find Link: <https://ufind.univie.ac.at/en/course.html?lv=070172&semester=2020W>
* Meeting time: Tu 09:00-10:30
* Meeting place: Seminarraum Geschichte 3 Hauptgebäude, 2.Stock, Stiege 9; due to COVID, all meetings will be held online
* Instructor: Dr. Maxim Romanov, [maxim.romanov@univie.ac.at](maxim.romanov@univie.ac.at)
* Language of instruction: English
* Office hours: Tu 14:00-15:00 (on Zoom; please, contact beforehand!)
* Office: Department of History,  Maria-Theresien-Straße 9, 1090 Wien, Room 1.10

![](./images/memex01.jpg)

Back in 1945, Vannevar Bush, a Director of the US Office of Scientific Research and Development, proposed a device, which he called memex:

> Consider a future device ... in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory. ... The owner of the memex, let us say, is interested in the origin and properties of the bow and arrow. Specifically he is studying why the short Turkish bow was apparently superior to the English long bow in the skirmishes of the Crusades. He has dozens of possibly pertinent books and articles in his memex. First he runs through an encyclopedia, finds an interesting but sketchy article, leaves it projected. Next, in a history, he finds another pertinent item, and ties the two together. Thus he goes, building a trail of many items. Occasionally he inserts a comment of his own, either linking it into the main trail or joining it by a side trail to a particular item. When it becomes evident that the elastic properties of available materials had a great deal to do with the bow, he branches off on a side trail which takes him through textbooks on elasticity and tables of physical constants. He inserts a page of longhand analysis of his own. Thus he builds a trail of his interest through the maze of materials available to him. And his trails do not fade. Several years later, his talk with a friend turns to the queer ways in which a people resist innovations, even of vital interest. He has an example, in the fact that the outraged Europeans still failed to adopt the Turkish bow. In fact he has a trail on it. A touch brings up the code book. Tapping a few keys projects the head of the trail. A lever runs through it at will, stopping at interesting items, going off on side excursions. It is an interesting trail, pertinent to the discussion. ... — [The Atlantic, July 1945](https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/).

![](./images/memex02.jpg)
![](./images/memex03.jpg)

<!--chapter:end:index.Rmd-->

# Memex and Zettelkasten{-}

The *memex* machine is often thought of as a precursor of the Internet, where information is interconnected. Vannevar Bush, however, seems to have viewed more as a personal machine knowledge organization system, to use a modern term. A system that would facilitate coordination of relevant pieces of information into organized sequences that he himself called "trails".

The idea of a personal knowledge device is still of great relevance and of great importance to scholars and scientists whose job is to construct such trails on a daily basis. Needless to say that historians will particulary benefit from having such a machine at their disposal (as Bush's example about the Turkish bow and the English longbow indicates, see [@BushWe1945]).

While it is not necessary to adhere slavishly to Bush's vision, we can definitely use his vision as a springboad to develop something similar; something that will allow us to navigate the massive volumes of information, which grew significantly since the 1940s.

In the following two sections you will find some relevant materials on memex and the history of this idea. Here, however, I want to take some time to think about how the design of our own memex should look like. What do we want from it? What can we reasonably achieve?

Before I procede to that, I would like to dwell on another relevant and, in my opinion, closely connected idea --- that of *Zettelkasten*. On its own, there is nothing particularly interestin and exciting about it, as the word refers to a rather unexciting piece of furniture: a "slip-box", or a "card-box". However, this term became closely associated with Niklas Luhmann, a German professor of sociology (U Bielefeld), who is considered one of the most prolific scholars of the 20th century. Like others, Luhmann himself attributed his productivity to his working method and the knowledge organization system which he implemented and systematically used throughout his career [See, @LuhmannKommunikation1982; @LuhmannArchimedes1987].

*NB:* Detailed bibliography can be found at the end of the section and in the References section (see, TOC).

## On Memex{-}

- [@BushWe1945] is the first article---"As We May Think"---that Vannevar Bush published on memex in *The Atlantic*. This [animation](https://www.youtube.com/watch?v=c539cK58ees) imagines the way memex would have functioned (produced by the organizers of the Brown/MIT symposium).
- [@NyceMemex1991] is the book that came out of the symposium held at the The Brown/MIT Vannevar Bush Symposium in 1995, celebrating the 50th anniversary of Bush's groundbreaking article "As We May Think". The book includes several Bush's articles on memex that show the evolution of his thinking about this device. This book is difficult to find; someone made [an EPUB version of it](https://monoskop.org/File:Nyce_James_Kahn_Paul_eds_From_Memex_to_Hypertext_Vannevar_Bush_and_the_Minds_Machine.epub) (also shared via *Slack*). Recordings of the symposium are available on YouTube:; other videos of this symposium can also be found at the Video Archive of *The MIT/Brown Vannevar Bush Symposium* [https://www.dougengelbart.org/content/view/258/000/](https://www.dougengelbart.org/content/view/258/000/). I highly recommend you watch [*Paul Kahn: A Visual Tour of Vannevar Bush's Work*](https://www.youtube.com/watch?v=BwwvMUzasU0&t=1853s); other presentations are very interesting as well.
- [@ParkMemex2014] is an recent experiment (MA Thesis in Design), trying to create a version of memex.

## On Zettelkasten{-}


### Niklas Luhmann on his *Zettelkasten*{-}

- Niklas Luhmann was open about his working method, which he discussed in his interviews [@LuhmannArchimedes1987], and in some of his academic articles[@LuhmannKommunikation1982].

### Others on Luhmann's *Zettelkasten*{-}

<https://niklas-luhmann-archiv.de/>

...

<!--chapter:end:01-Memex-and-ZK.Rmd-->

# Work Plan{-}

A place for the detailed description of how we are going to construct our memex machine.

## Memex-Building Cycle{-}

```{r echo=FALSE}

DiagrammeR::grViz("
digraph research_cycle {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10, rankdir = LR]

  # several 'node' statements
  node [shape = box,
        fontname = Baskerville,
        fontsize = 10,
        style = rounded,
        penwidth = 0.1]
  
  DL [label = 'digital\nlibraries']
  
  # several 'node' statements
  node [shape = box,
      fontname = Baskerville,
      fontsize = 10,
      style = solid,
      penwidth = 1]
  ZOT [label = 'bibliography\nmanager']
  MEM [label = 'memex']

  subgraph cluster0 {  
    DL->ZOT
    ZOT->MEM [label = 'algorithmic\ntransformations', fontsize= 7]
    MEM->DL
    color = gray;
    style = rounded;
    label = 'memex-building cycle';
    fontcolor = gray;
  }
  
}",height = 200)

```

## Research Cycle{-}

```{r echo=FALSE}

DiagrammeR::grViz("
digraph research_cycle {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10, rankdir = LR]
  
  # several 'node' statements
  node [shape = box,
      fontname = Baskerville,
      fontsize = 10,
      style = solid,
      penwidth = 1]
  MEM [label = 'memex']
  ZK [label = 'zettelkasten'] 

  subgraph cluster1 {
    MEM->ZK [label = 'trails', fontsize= 7];
    ZK->MEM [label = 'folgezettel', style=radial, fontsize = 7];
    color = gray;
    style = rounded;
    label = 'research cycle';
    fontcolor = gray;
  }

}",height = 200)

```

### Building Trails{-}

At first, all publication are converted into chain of minimal information units (*MIU*); ideally, these units should be paragraphs, in practice---pages). MIUs are sequentially connected with each other and can be read in their natural order. The graph below gives a visual representation of such organization: *a*, *b*, *c*, ..., and *h* are publications, where numbers indicate MIU sequences.

```{r echo=FALSE}

DiagrammeR::grViz("
digraph research_cycle {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10, rankdir = LR]

  node [shape = circle, color=grey, fontname = Baskerville, style = solid, penwidth = 0.5]  
  edge [color=grey, arrowhead=none]
  
  a1; a2; a3; a4; a5; a6; a7; a8; a9; a10
  a1->a2->a3->a4->a5->a6->a7->a8->a9->a10
  
  b1; b2; b3; b4; b5; b6; b7; b8; b9; b10
  b1->b2->b3->b4->b5->b6->b7->b8->b9->b10

  c1; c2; c3; c4; c5; c6; c7; c8; c9; c10
  c1->c2->c3->c4->c5->c6->c7->c8->c9->c10
  
  d1; d2; d3; d4; d5; d6; d7; d8; d9; d10
  d1->d2->d3->d4->d5->d6->d7->d8->d9->d10

  e1; e2; e3; e4; e5; e6; e7; e8; e9; e10
  e1->e2->e3->e4->e5->e6->e7->e8->e9->e10
  
  f1; f2; f3; f4; f5; f6; f7; f8; f9; f10
  f1->f2->f3->f4->f5->f6->f7->f8->f9->f10

  g1; g2; g3; g4; g5; g6; g7; g8; g9; g10
  g1->g2->g3->g4->g5->g6->g7->g8->g9->g10
  
  h1; h2; h3; h4; h5; h6; h7; h8; h9; h10
  h1->h2->h3->h4->h5->h6->h7->h8->h9->h10
  
  
}",height = 500)

```

When we have such a structure, we can apply different analytical methods and connect MIUs that exhibit some measure of similarity. As a result, we may get a very different graph reresentation of connections. Some MIUs get connected into long chains; others---into clusters; yet others float completely disconnected. These are the new connections that we would want to explore in order to find new connections in the information that we study. These are Vannevar Bush's *trails*.

```{r echo=FALSE}

DiagrammeR::grViz("
digraph research_cycle {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10, rankdir = BT]

  node [shape = circle, color=grey, fontname = Baskerville, style = solid, penwidth = 0.5]  
  edge [color=grey, arrowhead=none]
  
  a1; a2; a3; a4; a5; a6; a7; a8; a9; a10
  b1; b2; b3; b4; b5; b6; b7; b8; b9; b10
  c1; c2; c3; c4; c5; c6; c7; c8; c9; c10
  d1; d2; d3; d4; d5; d6; d7; d8; d9; d10
  e1; e2; e3; e4; e5; e6; e7; e8; e9; e10
  f1; f2; f3; f4; f5; f6; f7; f8; f9; f10
  g1; g2; g3; g4; g5; g6; g7; g8; g9; g10
  h1; h2; h3; h4; h5; h6; h7; h8; h9; h10

  # MEMEX SUGGESTIONS
  g9->h6; c1->a3; e5->g1; b2->e6; g3->e9; b4->a2; b6->d6; c9->a5; h7->a8; h8->e4; e1->c2; a8->e10; g5->c8; e4->c10; a4->d5; b9->f2; f2->e6; d8->a4; f8->a4; e8->d3; h3->g7; c4->a3; e10->f10; a3->h3; d10->b2; e9->b9; d5->h8; d3->a10; g7->a1; f5->g6; f7->c6; g8->f10; a7->c8; a1->f10; a5->c6; g10->h1; b8->c2; d7->a8; a2->d10; h10->e2; h2->e4; c6->b1; f4->g2; e6->g7; g1->e10; a10->e6; g2->e7; h1->a3; h4->d3; b5->g10; d4->a4; f9->h10; g6->c8; d6->b7; b1->g4; h6->c2; d9->f2; b3->f5; d1->a4; c2->a9; e7->f5; d2->g2; a6->e9; h5->g9; c3->g6; c8->g8; f1->b6; b10->g8; f10->e6; h9->a6; b7->c4; c5->h9
  
  
}",height = 500)

```

As we work our way through these *trails*, we discover that some are not particularly interesting, others are dead ends, but some are illuminating. We want to annotate and preserve them for later. Red lines indicate those vetted trails. These are Niklas Luhmann's *Folgezettel*.

```{r echo=FALSE}

DiagrammeR::grViz("
digraph research_cycle {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10, rankdir = BT]

  node [shape = circle, color=grey, fontname = Baskerville, style = solid, penwidth = 0.5]  
  edge [color=grey, arrowhead=none]
  
  a1; a2; a3; a4; a5; a6; a7; a8; a9; a10
  b1; b2; b3; b4; b5; b6; b7; b8; b9; b10
  c1; c2; c3; c4; c5; c6; c7; c8; c9; c10
  d1; d2; d3; d4; d5; d6; d7; d8; d9; d10
  e1; e2; e3; e4; e5; e6; e7; e8; e9; e10
  f1; f2; f3; f4; f5; f6; f7; f8; f9; f10
  g1; g2; g3; g4; g5; g6; g7; g8; g9; g10
  h1; h2; h3; h4; h5; h6; h7; h8; h9; h10

  # MEMEX SUGGESTIONS
  g9->h6; c1->a3; e5->g1; b2->e6; b4->a2; b6->d6; c9->a5; h7->a8; e1->c2; a8->e10; g5->c8; e4->c10; d8->a4; f8->a4; e8->d3; h3->g7; c4->a3; e10->f10; a3->h3; d10->b2; d3->a10; g7->a1; f7->c6; a7->c8; a1->f10; a5->c6; g10->h1; b8->c2; d7->a8; a2->d10; h10->e2; c6->b1; e6->g7; g1->e10; a10->e6; h1->a3; h4->d3; b5->g10; d4->a4; f9->h10; d6->b7; b1->g4; h6->c2; d9->f2; b3->f5; c2->a9; d2->g2; a6->e9; h5->g9; c3->g6; f1->b6; b10->g8; h9->a6; b7->c4; c5->h9

  edge [color=red, arrowhead=none, penwidth=5]
  f2->e6; b9->f2; e9->b9; f10->e6; g8->f10; c8->g8; g6->c8; f5->g6; e7->f5; g2->e7; f4->g2; g3->e9;
  h2->e4; h8->e4; d5->h8; a4->d5; d1->a4;
  
  
}",height = 500)

```

<!--chapter:end:01-WorkPlan.Rmd-->

# Lesson 01

## Bibliography Managers

Bibliography managers make your life easier when it comes to collectin, organizing and maintaining bibliographical references and your library of electronic publications (most commonly as PDFs). Additionally, they are an indispensable writing tool as they take care of formatting (and reformatting) references and bibliographies in any writing project that you might undertake. There are plenty of different programs out there with their advantages and disadvantages (for example, Mendeley, RefWorks, Citavi, Endnote, Papers, Zotero, and quite a few more). We will use Zotero---it is being developed by scholars for scholars; it is free and open source; it does pretty much everything you might possibly need from a program of this kind.

## Zotero

### Getting Started

- Zotero can be installed from here: <https://www.zotero.org/download/>; the page will offer you a version suitable for your operating system, but you should also see the links to versions for specific systems (Mac OS, Windows, Linux).
- During installation Zotero should automatically integrate into your browser (like Chrome or Firefox) and into your word processor (MS Word, LibreOffice, GoogleDocs are supported). It is possible that you may have to do that manually.
  - Zotero Connector for Chrome can be installed from the same page (<https://www.zotero.org/download/>)
  - detailed explanations on how to use word processor plugins can be found [here](https://www.zotero.org/support/word_processor_integration); you can use Zotero with [MS Word](https://www.zotero.org/support/word_processor_plugin_usage), [LibreOffice](https://www.zotero.org/support/libreoffice_writer_plugin_usage) and [Google Docs](https://www.zotero.org/support/google_docs); in case you cannot get your plugin activated, check the [Troubleshooting Section](https://www.zotero.org/support/word_processor_plugin_troubleshooting).

    
### Main Functionality

You need to be able to do the following tasks with your Zotero in order to take full advantage of its functionality.

*Online Tutorials*: If you prefer video tutorials, you can check a series of tutorials prepared by the [McGill Library](https://www.youtube.com/playlist?list=PL4asXgsr6ek5H5mM9GlA1d-YCb9KvP3Ja) (there are also plenty other tutorials on YouTube :); if you prefer to read, you can check a series of tutorials prepared by the [UC Berkley Library](https://guides.lib.berkeley.edu/zotero).

1. **Adding bibliographical records (and PDFs)**
  - *Using Zotero Connector*: the easiest way to add a reference is from a browser with Zotero connector. This can be done practically from any library or journal database (e.g., Uni Wien Library, Worldcat.org, JSTOR); simply click the connector button while you are on a page with a publication that you want to add to your Zotero database. PDF may be automatically downloaded, if available; keep in mind that in places like JSTOR you need to agree to terms before this function will work; what you need to do is to download one PDF manually from a JSTOR page, where you will be asked to agree to terms of their services;
  - *Drag-and-dropping PDFs into Zotero*; this however works only when Zotero can parse relevant bibliographical information from a PDF; This might be a good way to start if you already have lots of PDFs that you want to add to Zotero.
  - *Using Unique Identifiers*: you can use ISBN or DOI numbers.
  - *Using Import*: you can import bibliographical data from another application or from bibliographical files (formats, like RIS, which you can download from most libraries as well).
  - *Manually*: you can manually add and fill in a record as well.
2. **Write-and-cite**
  - *Detailed Instructions*: [MS Word](https://www.zotero.org/support/word_processor_plugin_usage), [LibreOffice](https://www.zotero.org/support/libreoffice_writer_plugin_usage) and [Google Docs](https://www.zotero.org/support/google_docs); you can also check the video tutorial.
  - *Add a citation*
  - *Customize a citation* (by adding prefixes, suffixes, page range for a specific reference, etc.).
  - *Change citation style*.
    - For example, change from *Chicago Manual of Style* to *Universität Wien - Institut für Geschichte* (Yes, there is this specific citation style for Zotero: <https://www.zotero.org/styles?q=id%3Auniversitat-wien-institut-fur-geschichte>); in order to do that you need to download the IfG style and install it into Zotero.
    - You can find lots of different citation styles here: <https://www.zotero.org/styles>; to add a new style to Zotero:
      - download the style you want.
      - Open Zotero. Go to Preferences (under Zotero, Edit, or Tools --- depending on your system).
      - Click the "Cite" button.
      - Click the "Styles" tab.
      - Click the `+` button at the bottom right.
      - Select the style file you saved in the first step.
  - *Generate and update bibliography* in your paper.
  - **NB:** If you use Zotero plugin for adding your citations, they remain connected to Zotero and can be automatically reformatted; you can also drag-and-drop any bibliographical record into any text editor---the reference will be formatted according to the currently selelected style, but it will not be connected to Zotero and cannot be reformatted automatically later.
3. **General Maintenance and Organization**
  - Zotero can [automatically] rename PDFs using metadata, although the default function is not very robust (see, *Zotfile* plugin below).
  - You can create “collections” and drag-and-drop publications relevant to a specific topic or project you are working on.

### Additional Functionality: Plug-Ins

There is a variety of third-party plugins that you can add to Zotero for additional functionality. The list of plugins can be found at <https://www.zotero.org/support/plugins>. To install a plugin, you need to download its `.xpi` file to your computer. Then, in Zotero, click “Tools → Add-Ons”, then drag the `.xpi` for the plugin onto the Add-Ons window that opens. Two plugins will be of particular interest to us: `Zotfile` and `BetterBibTeX`.

### Zotfile

Zotfile (<http://zotfile.com/>) is a Zotero plugin to manage your attachments: automatically rename, move, and attach PDFs (or other files) to Zotero items, sync PDFs from your Zotero library to your (mobile) PDF reader (e.g. an iPad, Android tablet, etc.) and extract annotations from PDF files.

This plugin is particularly helpful for organizing PDFs on your hard drive. By default, Zotero saves PDFs in a computationally safe, but humanely incomprehensible manner: each PDF, even if it is renamed from bibliographical metadata and is human readable, it is still placed into a folder whose name is a random sequence of characters. Zotfile allows you to organize PDFs in a more human-friendly manner. The first screenshot below shows Zotero default mode, while the second one shows Zotfile mode: essentially, Zotfile creates a folder for each author and PDFs of all publications by that author get placed in that folder. You can sync this folder with Dropbox or other cloud service and access it from your tablet or phone.

![Zotero default organization.](./images/zotero_default.png)

![Zotfile organization.](./images/zotero_zotfile.png)

### Better BibTeX for Zotero

For a moment this will not be an immediately useful plug-in, but it is the most important one for our Memex project. This plugin exports bibliographical data into a `bibTeX` format, which is very easy to process with python scripts (it also generates *citation keys* which can be used for citation in markdown, which we will cover later). The two screenshots below show how the same record looks in Zotero preview and in the `bibTeX` format.

![A Record in Zotero.](./images/rec_zotero.png)

![The Same Record in `BibTeX` Format.](./images/rec_bibtex.png)

## Homework{#HWL01}

- collect 30-50 bibliographic records into your Zotero (ideally with PDFs); the number may seem like a lot, but you will see that you can do that it will take only about 30 mins on JSTOR; those of you who are already using Zotero must already have more than 50 records in your databases. 
- clearly, you should be collecting items that are relevant to your fields of study and your research; organize them into folders, if that is necessary;
- **create Bibliography and email it to me** (this is one-click operation; try to figure on your own how to do this; asking on *Slack* counts);
- make sure that you are comfortable with the main functionality of Zotero; that you have the discussed plugins installed; to get comfortable with the main functionality, you should practice each listed procedure at least a couple of times.
- **in preparation for the next class**, please, watch the following  two short videos from Dr. Paul Vierthaler's *Hacking the Humanities* series:
  - [Episode 1: Introduction to the Hacking the Humanities Tutorial Series](https://www.youtube.com/watch?v=fhsH4ua9zP8&list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17&index=1) and install Python via Anaconda; you can also install Python directly from <https://www.python.org/>, but Anaconda distribution might make your life easier, especially if you are on Windows.
  - [Episode 2: The Command Prompt](https://www.youtube.com/watch?v=q5b19xMb97I&list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17&index=2&t=206s).

**Submitting homework:**

- Homework assignment must be submitted by the beginning of the next class;
- Email your homework to the instructor as attachments.
		* In the subject of your email, please, add the following: `CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber`, where `CCXXXXX` is the numeric code of the course; `LXX` is the lesson for which the homework is being submitted; `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.

<!--chapter:end:02-Lesson01.Rmd-->

# Lesson 02

## Command Line

The knowledge of “command line” opens a whole new world of opportunities, as the number of interface-less programs and applications is significantly larger; command line also offers a more robust and direct controls over a computer. The main goal is to learn the basics of this indispensable tool.   

We can use `Terminal` on Mac (installed), `Powershell` on MS Windows (should be installed), although other command line tools will work as well. 

Before we proceed, however, let's discuss a few concepts:

* What a *filesystem* is
* How to run a program from the command line
* What it means to run a program
* How the computer knows what program to run
* How to refer to a file from the command line

### The filesystem

Every disk contains a filesystem and information about where disk data is stored and how it may be accessed by a user or application. A filesystem typically manages operations, such as storage management, file naming, directories/folders, metadata, access rules and privileges. Commonly used file systems include File Allocation Table 32 (FAT 32), New Technology File System (NTFS) and Hierarchical File System (HFS).

All the files and programs on your computer are organized into folders; all these folders are in some other folders all the way down to your hard drive, which we call the **root** of your filesystem. Every hard drive, USB drive, DVD, and CD-ROM has its own filesystem.

You normally look at the contents of your filesystem via the Finder (on Mac) or the Explorer (on Windows). Open a window there now.

The Finder / Explorer window opens in some folder, which might be different depending on what computer operating system you’re using. But you’ll usually have a navigation bar to the left, that will let you go to different places. You see *folders*, also known as *directories*, and you might see *files* too.

One thing that computer OSes like to hide from you is the fact that you have a home directory, where all your personal files and folders should live. This makes it easier for multiple users to use a single computer. You can find your home directory like this:

* On **Mac**, select `Go` > `Home` in the menu.

![](./images/L02/mac_finder.png)

* On **Windows**, click on `Local Drive (C:)`, then click on `Users`, then click on your login name.

![](./images/L02/windows_explorer.png)

You’ll see that your home directory has several folders in it already, that were created automatically for you when you first made a user account.

Now how can you tell where you are, with respect to the root of your drive?

* On Mac, select `View` > `Show Path Bar` in the menu.

![](./images/L02/mac_path.png)

On Windows, look:

![](./images/L02/windows_path.png) 

The Finder / Explorer will also show you where in your computer’s filesystem you are. This is called the path—it shows you the path you have to take from the root of your filesystem to the folder you are in.

Now if you are on Windows, click on that bar and you’ll see something surprising.

![](./images/L02/windows_realpath.png) 

This is your real path. The `C:\` is how Windows refers to the root of your filesystem. Also note that, even if your OS is not in English, the path may very well be!

### Getting started with the command line

Now that you have a hint of what is going on behind the scenes on your computer, let’s dive into the command line. Here is how you get there:

* On Mac, look for a program called `Terminal.app`

![](./images/L02/mac_terminal.png) 

* On Windows, look for a program called `Powershell`

![](./images/L02/windows_powershell.png) 

By default, these shells open in your home directory. On Windows this is easy to see, but on Mac it is less clear—that is, until you know that this `~` thing is an alias for your home directory.

### Components of the command line

The command line consists of a *prompt* where you type your commands, the *commands* and *arguments* that you type, and the *output* that results from those commands.

The *prompt* is the thing that looks like (where `user` is your username):

```
MacBook-Pro:~ user$
```

or

```
PS C:\Users\user>
```

You will never need to type the prompt. That means that, if you are noting down what we do in class for future reference, you should not copy this part!

The prompt actually gives you a little bit of information.

* On Mac, it has the name of the computer, followed by a `:`, followed by the directory where you are, followed by your username, with `$` at the end.
* On Windows, it has `PS` for `PowerShell`, followed by the name of the drive (`C` for most of you), followed by a `:`, followed by the full path to where you are, with `>` at the end.

When you type a command, nothing happens until you press the `Return/Enter` key. Some commands have output (more text that appears after you press `Return/Enter`) and others don’t. You cannot run another command until the prompt is given again.

**NOTE**: From this point on, you will be running the commands that are run here!

Let’s first make sure we are in our home directory by typing `cd ~`. For most of you this should change nothing, but now you know your first shell command. The `cd` stands for `change directory`, and what follows is the directory you want to go to.

```
cd ~
```

Now let’s have a look around. The command to show what is in any particular directory is called ls, which stands for list. Try running it.

```
ls
```

If you are on Windows, what you get will look more like this:

```
PS C:\Users\user> ls
```

You should then see something like:

```
Verzeichnis: C:\Users\user


Mode                LastWriteTime     Length Name
----                -------------     ------ ----
d----        23.02.2016     21:18            .oracle_jre_usage
d-r--        23.02.2016     20:40            Contacts
d-r--        23.02.2016     20:40            Desktop
d-r--        23.02.2016     21:11            Documents
d-r--        23.02.2016     21:16            Downloads
d----        23.02.2016     21:24            exist
d-r--        23.02.2016     20:40            Favorites
d-r--        23.02.2016     20:40            Links
d-r--        23.02.2016     20:40            Music
d-r--        23.02.2016     20:40            Pictures
d-r--        23.02.2016     20:40            Saved Games
d-r--        23.02.2016     20:40            Searches
d-r--        23.02.2016     20:40            Videos


PS C:\Users\user>
```

Now go into your documents folder and look around.

```
cd Documents
ls
```

How does this compare to what you see in the Finder / Explorer window, if you click on the Documents folder?

Another important command, which tells you where you are at any given time, is `pwd`. This means print working directory. Try it now and see what you get.

```
pwd
```

If ever you get lost on the command line, `pwd` will always help you find your way.

### File paths and path notations

By now you will have noticed that I've mentioned the **path** a few times, and that it seems to have something to do with this thing that **pwd** prints out. (And, most annoyingly, that it looks different on `Mac` and `Windows`) The bit of text that you get from `pwd` is what is called path notation, and it is very important that you learn it if you want to do anything with your own digital data. Here are some rules:

* The `/` (or `\\` on Windows) separates folder names. So `Desktop/Video` means “the thing called Video inside the Desktop folder”.
* The `/` all by itself refers to the base of your hard drive (usually `Macintosh HD` or `C:\`.)
* The `~` refers to your home folder.
* These things can be combined; `~/Documents` means “the Documents folder in my home folder.”
* The `.` means “the current working directory”, i.e. what you would get if you ran the command `pwd`.
* The `..` means “one directory back”—if `pwd` gives you `/Users/user`, then `..` means `/Users`.
* If the path does not start with a `.` or a `/` or a `~`, then it will be assumed to start with a `./`, that is, “start from the current working directory.”

Let’s wander around a bit. But, first, let's download [a zip file with somoe materials for this class](./files/command_line_practice.zip). Unzip it somewhere and go to that folder in your `Terminal` or `Powershell`.

```
cd /path/to/the/folder/tnt_practice_materials
pwd
```

```
cd ./cd 02_CommandLine/
pwd
```

```
ls
```

Try the following if you are on Mac
```
ls -lh
```

```
cd ..
pwd
```

**NB:** you can use `TAB` to autocomplete the path: type `ls` to see what folders are in `Documents`, then go to any one of them by typing `cd` (space) and then the first two letters > after that use `TAB` and the name will be complete automatically. 

```
cd 03[TAB]
pwd
```

```
cd ../01[TAB]
pwd
```

```
ls McCarty_Modeling.pdf
cd ..
```

### Command line arguments

So far we have learned three commands: `cd`, `ls`, and `pwd`. These are useful for navigation, but we can run a lot more commands once we learn them, and have a need for them!

What are we doing, exactly?

* First word is the `command`
* All other words are the `arguments`
* Words **must be** separated by `spaces`

`cd` is a command that expects an argument: the name of the directory you want to go to. But what if the name has a space in it?

**NB:** You may think of most commands as sentences with subject, predicate, and object (or multiple objects).

```
cd ./01_Zotero_Word/Green Eggs and Ham
```

What happened there?

Well, we have a folder called **Green Eggs and Ham** in our example, and we tried to go there. But since the command line works with arguments, and since arguments are separated by space, the machine interpreted this as if we were saying “Change to the `./01_Zotero_Word/Green` folder, and then `Eggs`, `and`, `Ham`, whatever that means.” And it gave us an error, because we don’t have a folder called `Green` in our example.

You can get around this. How you get around it depends on whether you’re on Windows or not. One way to get around it that should work both places is like this:

On Windows:
```
cd './01_Zotero_Word/Green Eggs and Ham'
```

On Mac (you need to *escape* spaces by adding a `backslash` in front of them):
```
cd ./01_Zotero_Word/Green\ Eggs\ and\ Ham/
```

**NB:** The easiest solution is to use `TAB` for autocomplete!

### More commands

With command line you can do everything that you became accustomed to be doing in a graphical interface of your favorite file manager. For example, you can `copy`, `move`, and `delete` files and folders.

You can use:

* `mv` to move files
* `rm` (on Windows also: `del`) to remove/delete files
* `cp` to copy files

In all cases you need to state which files you want to `mv`, `rm`, or `cp`. In some cases you also need to point where you want to `mv` or `cp` your files.

**NB:** Syntax on `Mac` and `Windows` will vary slightly, but if you keep using `[TAB]` for autocompletion, there will be no different in the process of typing the command, so let’s try to do it this way. 

To start, let’s go to the root directory of our course materials. From there, let’s do the following:

```
cd 01[TAB]
ls
cp Mc[TAB] Green[TAB]
cd Green[TAB]
ls
```

**NB:** when you hit `[TAB]` after `Mc` you are not going to get the full autocomplete, because there are two files that start with `McCarty_Modeling`—one is `pdf` and another—`txt`. You will need to type one more letter `p` and then hit `[TAB]` again to get the file name that you need. Thus, the command can be transcribed as: `M[TAB]p[TAB]`

Now let’s `rm` (*delete*) the `McCarty_Modeling.pdf` from this folder, then go to the folder where we copied it, and then `mv` (*move*) it back to where it was in the first place.

```
rm M[TAB]p[TAB]
ls
cd G[TAB]
mv Mc[TAB] ../
cd ..
ls
```

Tada! The `McCarty_Modeling.pdf` should now be back where it was.

If you want to learn about new commands, try to `google`. Googling things like this is a very big part of being a DH scholar! You will most likely find your answers on <https://stackoverflow.com/>, which will become your most frequented resource, if you embark on the DH path. 

## Homework{#HWL02}

**Command line**

* Watch again a short video on `Command Prompt` Dr. Vierthaler's *Hacking the Humanities* series: [Episode 2: The Command Prompt](https://www.youtube.com/watch?v=q5b19xMb97I&list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17&index=2&t=206s).
* Work through the following materials on command line which is relevant to your operating system.
  * Ted Dawson, "Introduction to the Windows Command Line with PowerShell," The Programming Historian 5 (2016), <https://programminghistorian.org/en/lessons/intro-to-powershell>.
  * Ian Milligan and James Baker, "Introduction to the Bash Command Line," The Programming Historian 3 (2014), <https://programminghistorian.org/en/lessons/intro-to-bash>.

**Python**

* Work through Chapter I of Zelle's book; read the entire chapter; retype and run all code snippets as described in the book; work through the chapter summary and exercises; complete all programming exercises;
  * For submission: email me the results of "Programming Exercises". In your submission there should be text files or python script files for exercises 1 (results of print function), 3, 4, 5, 7. Each python script should be working, i.e. you should be able to run it and get relevant results. You are welcome to discuss any of these assignments on Slack.
* Work through the following videos from Dr. Vierthaler's *Hacking the Humanities* series:
  * [Episode 3: The Very Basics of Python](https://www.youtube.com/watch?v=4oSiCLq3AWo)
  * [Episode 4: Strings](https://www.youtube.com/watch?v=NtJiVvs96zY)
  * [Episode 5: Integers, Floats, and Math in Python](https://www.youtube.com/watch?v=Y_OmmuNA_NE)
  * **NB:** The best way to work through these tutorials is to repeat all steps after the instructor. You can find the scripts at <https://github.com/vierth/humanitiesTutorial>.

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Email your homework to the instructor as attachments.
	* In the subject of your email, please, add the following: `CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber`, where `CCXXXXX` is the numeric code of the course; `LXX` is the lesson for which the homework is being submitted; `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.

<!--chapter:end:02-Lesson02.Rmd-->

# Lesson 03

## Version Control and Collaboration

Version control systems are extremely helpful for the development of DH projects, which are often lengthy and complex and require organic collaboration. `Git` and `GitHub` are currently the most popular tools of this kind. It is difficult to imagine a DH project that would not rely on the use of `git` and `GitHub`. 

Before we begin, make sure to:

* Create a github account at <https://github.com/>, if you do not have one yet.
* Download and install `git` software:
	* for Windows:
		* you can download it from <https://git-scm.com/download/win>. Please, choose **64-bit Git for Windows Setup**.
			* you can also install a portable version of `git` which does not require installation <https://git-scm.com/download/win>. For this, choose **64-bit Git for Windows Portable**. Simply download and unzip (*Suggestion*: move that unzipped folder to the folder where you keep all class-related files and materials). In the folder, run `git-bash.exe` (for a more Unix-like command line) or `git-cmd.exe` (for Windows command line).
	* for Mac: try to run `git --version` from Terminal. If `git` is not installed, you will be prompted to install `Xcode Command Line Tools` which comes with `git` among other things. This is the easiest way.
	* **Note:** there are also interface tools for *github*. We will not be working with them in the class, but you are welcome to test them on your own at home. See, <https://desktop.github.com>. The main reason for this is because interface tools will be different for different operating systems, while the command line usage will be exactly the same across all platforms.

In class we will cover the following:

* Basic `git` functionality;
* Starting a `github`-based website;
* Basics of `markdown`;

## Setting-up `git`

 - `git config --global user.name "YourName"`
 - `git config --global user.email "YourEmail"`

## General `git` workflow

* *In `Terminal` (on Mac) or `Git-Bash` (on Windows)*
	1. create a repository under your account online at <https://github.com>.
	2. Alternatively, you can also `fork` somebody else's repository.^[**NB:** this is done on <https://github.com>); forking means creating your own copy of some one's repository at that specific moment in time]
	3. `clone` (**NB:** this is done on <https://github.com>!)
	4. *work*
	5. `add`
	6. `commit`
	7. `push` / `pull`
	8. send `pull request` (**NB:** this is done on <https://github.com>)

**Note:** Steps 2 and 8 are relevant only when you work on a project (*repository*) that is owned by somebody else. If you work on a *repository* that you created under your account, you only need steps 1, 3-7. Below is a visual representation of this cycle.

```{r echo=FALSE}

DiagrammeR::grViz("
digraph research_cycle {

  # a 'graph' statement
  graph [overlap = false, fontsize = 10, rankdir = TB]

  # several 'node' statements
  node [shape = box,
        fontname = Baskerville,
        fontsize = 10,
        style = rounded,
        penwidth = 0.1]
  github;
  create;
  fork;

  # several 'node' statements
  work [shape=box, regular=1, style=rounded, fillcolor=grey50, label = 'working...']

  # several 'node' statements
  node [shape = box,
        fontname = Courier,
        fontsize = 10,
        style = filled,
        penwidth = 0.1]  
  clone [label = 'git clone <repository_link>'];
  add [label = 'git add .'];
  commit [label = 'git commit -m “message”'];
  push [label = 'git push origin master'];
  pull [label = 'git pull origin master'];
  
  subgraph cluster1 {
  github->create; github->fork;
    color = gray;
    style = rounded;
    label = 'github.com';
    fontcolor = gray;
  }
  
  subgraph cluster2 {
  clone->work;
  work->add; add->commit;
  commit->push;
  push->github;
  github->pull; pull->work;
    color = black;
    style = rounded;
    label = 'work cycle';
    fontcolor = gray;
    fillcolor = gray;
  }
  
  create->clone; fork->clone;

}",height = 600)
```


## Main `git` Commands

* `git clone <link>`
	- clones/downloads a repository on you machine
* `git status`
	- shows the current status of the repository (new, changed, deleted)
* `git add .`
	- adds all new files and modified files to the repository
* `git commit -m "message"`
	- saves all files in their current state into the repository, and created a milestone
* `git push origin master`
	- uploads changes to <https://github.com>
	- `origin` is a specific repository you are pushing your changes to; it is automatically set up, when you clone a repository on your computer.
	- `master` is the *branch* you are pushing to the repository; `master` is the default name of the main branch in a git repository. To check the names of your branches, you can type `git branch`.
	- **NB:** sometimes you may get an error, which in most cases means that you need to `pull` first
* `git pull origin master`
	- downloads changes from <https://github.com>
* `git log`
	- shows the history of `commit`s; here you can choose where you want to roll back, in case of troubles.

## Some useful command line commands to remember

* `pwd`
	- shows you where you are on a drive (gives you path)
* `ls` / `dir` [on Windows]
	- shows everything in the your current location/folder
* `cd <name of the folder>`
	- takes you to that folder
* `cd ..` 
	- takes you one level up in the tree structure of your computer

## Practice

- Under your GitHub account, create repository `HW070172`;
- clone it to your computer (use command line: `git clone LinkToYourRepository`);
- Now, in the repository:
  - let's edit `README.md` (create it, if you have not yet); add some text into this file
  - create subfolders for Lessons, like `L01`, `L02`, `L03`, etc.
  - copy/paste your homework files in respective subfolders.
  - Now, do the `add`-`commit`-`push` routine to upload the files to your repository
- Now, online:
  - check if your files are there
  - let's do some edits to the `README.md` file (markdown basics / *github flavor*)
- `pull` / `push`

## Homework{#HWL03}

**Git and GitHub**

* Watch a video on `Git & GitHub` in Dr. Vierthaler's *Hacking the Humanities* series: [Supplement 1: A quick Git and Github Tutorial](https://www.youtube.com/watch?v=YetC-gxgIVY). This will help you to go over the new material and pick up a few more useful `git` *&* `gitHub` tricks.
* There is an interface for github that you can also use, but I strongly recommend to use command line; interfaces change, but commandline commands remain the same!
  - Daniel van Strien. 2016. "An Introduction to Version Control Using GitHub Desktop," The Programming Historian 5, [https://programminghistorian.org/](https://programminghistorian.org/en/lessons/getting-started-with-github-desktop).
* Please, also read (for `markdown`): Simpkin, Sarah. 2015. “Getting Started with Markdown.” Programming Historian, November. [https://programminghistorian.org/](https://programminghistorian.org/lessons/getting-started-with-markdown).
  * More on github-flavored `markdown`: <https://guides.github.com/features/mastering-markdown/>.
  * On `markdown` for academic writing, see <https://pandoc.org/MANUAL.html>.
  * A cheat-sheet & interactive tutorial for your practice: <https://commonmark.org/help/>.

*Extra*: you can build and host a website on `github.com`; your website will have the name: `YourUserName.github.io` --- you can create a repository with that name and build your website there using Jekyll and GitHub Pages. Any other repository may also be converted into a part of your website, which will be accessible at `YourUserName.github.io/YourRepository/`

* Visconti, Amanda. 2016. “Building a Static Website with Jekyll and GitHub Pages.” Programming Historian, April. [https://programminghistorian.org/](https://programminghistorian.org/lessons/building-static-sites-with-jekyll-github-pages).


**Python**

- Work through Chapter II of Zelle's book; read the entire chapter; retype and run all code snippets as described in the book; work through the chapter summary and exercises; complete all programming exercises;
- Watch [Dr. Vierthaler's videos](https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17):
  - Episode 04: Strings;
  - Episode 05: Integers, Floats, and Math in Python;
  - Episode 06: Lists

**Submitting homework**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfoler in your `HW070172` repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)
	* In the subject of your email, please, add the following: `CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber`, where `CCXXXXX` is the numeric code of the course; `LXX` is the lesson for which the homework is being submitted; `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.

<!--chapter:end:02-Lesson03.Rmd-->

# Lesson 04

## Sustainable Academic Writing with `markdown`, `pandoc`, and the `*TeX` family

Introduction to sustainable academic writing that avoids any proprietary software solutions and formats. Before the class, make sure to install the following.

* `pandoc` (follow instructions on [https://pandoc.org/installing.html](https://pandoc.org/installing.html))
* `LaTeX` engine (install from here: [https://miktex.org/](https://miktex.org/))
	* `LaTeX` for Mac: MikTeX seems to be very finicky with Macs. The following solution proves to be more manageable and stable (you need to run this command in Terminal): `brew install librsvg python homebrew/cask/basictex` (from Pandoc page); after that missing packages might have to be installed manually, but that is relatively easy --- the sustem will prompt you to install them, and that needs to be done only once; alternatively, one can install MacTeX (this one is quite large, about 4Gb).
* `markdown`


```{r echo=FALSE}

DiagrammeR::grViz("
digraph writing_cycle {

  # a 'graph' statement
  graph [overlap = false, fontsize = 10, rankdir = TB]

  # several 'node' statements
  node [shape = box,
        fontname = Baskerville,
        fontsize = 10,
        style = rounded,
        penwidth = 0.1]
  bibtex [label = 'bibTex bibliography']
  markdown [shape=box, style=filled, fillcolor=grey50, fontcolor=white, fontname='courier-bold', label = 'textfile in markdown'] ;
  zotero[label = 'Zotero']
  pandoc[label='pandoc engine']
  style[label='citation style']
  latex[label='TeX engine']
  html[label='HTML']
  doc[label='MS Word']
  pdf[label='PDF']
  
  
  zotero->bibtex; bibtex->markdown; bibtex->pandoc;
  markdown->pandoc; style->pandoc;
  pandoc->html; pandoc->doc; pandoc->latex->pdf;

}",height = 600)
```


## Class Notes

*Files*: Download the following archive file: [sustainable_writing.zip](./files/sustainable_writing.zip). Make sure to unzip it! It contains the following files:

* `biblio.bib`—a bibliography file;
* `cms-fullnote.csl`—a citation style;
* `main.md`—the main text file (its contents are also shown below);

**NB:** remember that all files must be in the same folder; it makes sense to put folders into a subfolder (not to overcrowd your main folder), but then do not forget to change the path in your `image` code. 

*TEXT for your `main.md` file.*

```
---
title: |
    *From*: "Modeling: A Study in Words and Meanings" by Willard McCarty
subtitle: 
author: 
date: \today
bibliography: biblio.bib
csl: cms-fullnote.csl
---

>> Out on site, you were never parted from your plans.
They were your Bible. They got dog-eared, yellowed,
smeared with mud, peppered with little holes from
where you had unrolled them on the ground. But
although so sacred, the plans were only the start.
Once you got out there on the site everything was
different. No matter how carefully done, the plans
could not foresee the *variables*. It was always
interesting, this moment when you saw for the first
time the actual site rather than the idealised
drawings of it.

>> Kate Grenville, *The Idea of Perfection*
(Sydney: Picador, 1999): 62–3


# Introduction

The question of modeling arises naturally for
humanities computing from the prior question
of what its practitioners across the disciplines
have in common. What are they all doing with
their computers that we might find in their
diverse activities indications of a coherent or
cohesible practice? How do we make the best,
most productive sense of what we observe? There
are, of course, many answers: practice varies 
from person to person, from project to project, 
and ways of construing it perhaps vary even more. 
In this chapter I argue for modeling as a model 
of such a practice. I have three confluent goals: 
to identify humanities computing with an intellectual 
ground shared by the older disciplines, so that we 
may say how and to what extent our field is of as 
well as *in* the humanities, how it draws from and 
adds to them; at the same time to reflect experience 
with computers "in the wild"; and to aim at the
most challenging problems, and so the most
intellectually rewarding future now imaginable.

My primary concern here is, as Confucius almost 
said, that we use *the correct word* for the 
activity we share lest our practice go awry 
for want of understanding (*Analects 13.3*). 
Several words are on offer. By what might be 
called a moral philology I examine them, arguing 
for the most popular of these, "modeling." The 
nominal form, "model", is of course very useful 
and even more popular, but for reasons I will 
adduce, its primary virtue is that properly 
defined it defaults to the present participle, 
its semantic lemma. Before getting to the 
philology I discuss modeling in the light of 
the available literature and then consider 
the strong and learned complaints about the 
term.

# Background

Let me begin with provisional definitions[^1]. 
By "modeling" I mean *the heuristic process of 
constructing and manipulating models*, a "model" 
I take to be either *a representation of something 
for purposes of study*, or *a design for realizing 
something new*. These two senses follow Clifford 
Geertz's analytic distinction between a denotative 
"model *of*" such as a grammar describing the 
features of a language, and an exemplary "model 
*for*" such as an architectural plan
[@geertz_interpretation_2017, 93][^2]. In both 
cases, as the literature consistently emphasizes, 
a model is by nature a simplified and therefore 
fictional or idealized representation, often taking 
quite a rough-and-ready form: hence the term 
"tinker toy" model from physics, accurately suggesting 
play, relative crudity, and heuristic purpose 
[@cartwright_how_1984, 158]. By nature modeling 
defines a ternary relationship in which it mediates 
epistemologically, between modeler and modeled, 
researcher and data or theory and the world 
[@morgan_models_1999]. Since modeling is 
fundamentally relational, the same object may 
in different contexts play either role: thus, 
e.g., the grammar may function prescriptively, 
as a model for correct usage, the architectural 
plan descriptively, as a model of an existing style. 
The distinction also reaches its vanishing point in 
the convergent purposes of modeling: the model of 
exists to tell us that we do not know, the model 
for to give us what we do not yet have. Models 
*realize*.

[^1]: My definitions reflect the great majority of 
the literature explicitly on modeling in the history 
and philosophy of the natural sciences, especially 
of physics. The literature tends to be concerned 
with the role of modeling more in formal scientific 
theory than in experiment. The close relationship 
between modeling and experimenting means that the 
rise of a robust philosophy of experiment since 
the 1980s is directly relevant to our topic; see 
[@hacking_stability_1988]. Quite helpful in 
rethinking the basic issues for the humanities 
are the writings from the disciplines other 
than physics, e.g., [@clarke_models_2015] on 
archaeology; on the social sciences, the essays 
by de Callatay, Mironesco, Burch, and Gardin 
in [@franck_explanatory_2011]. For interdisciplinary 
studies see Shanin (1972) and [@morgan_models_1999], 
esp. "Models as Mediating Instruments" (pp. 10–37).

[^2]: Cf. Goodman's distinction between 
"denotative" and "exemplary" models, respectively 
(1976: 172–3); H. J. Groenewold's "more or less 
poor substitute" and "more or less exemplary ideal" 
(1960: 98). Similar distinctions are quite common 
in the literature.

# Bibliography
```

## `pandoc` *Commands*

**NB:** On Windows, you may see a pop-up Windows from `MikTex` asking to download a missing package for `LaTeX`. This means that some package is missing and you need to download it (or several of them). Uncheck a birdie to install all necessary packages at once. After that everything should work.

First try to convert to `docx` or `html`. These two formats do not require

```
pandoc -f markdown -t docx -o main.docx --filter pandoc-citeproc main.md
pandoc -f markdown -t html -o main.html --filter pandoc-citeproc main.md
pandoc -f markdown -t epub -o main.epub --filter pandoc-citeproc main.md
pandoc -f markdown -t latex -o main.pdf --filter pandoc-citeproc main.md
```

**NB:** it may so happen that your version of `pandoc` will complain about `--filter pandoc-citeproc`. If that happens, your commands should look like the following:

```
pandoc -f markdown -t docx -o main.docx --citeproc main.md
pandoc -f markdown -t html -o main.html --citeproc main.md
pandoc -f markdown -t epub -o main.epub --citeproc main.md
pandoc -f markdown -t latex -o main.pdf --citeproc main.md
```


**Comment:**

* `-f` means “convert `from` a specific format”.
* `-t` means “convert `to` a specific format”.

Thus, the whole command (say, the first one) reads as follows: `pandoc converts from (-f) markdown to (-t) latex, then outputs (-o) main.pdf, to which a ‘filter’ that processes citations (--filter pandoc-citeproc) is applied; and the file to which this all is applied is main.md`.

## Writing with markdown in Atom

Atom (<https://atom.io/>) allows one to integrate writing in markdown with the helpfulness of Zotero, as well as offers quite a few other nice features. You can find how to set everything up in Scott Selisker's blog post: <http://u.arizona.edu/~selisker/post/workflow/>.

## Reference Materials:

* Simpkin, Sarah. 2015. “Getting Started with Markdown.” Programming Historian, November. [https://programminghistorian.org/lessons/getting-started-with-markdown](https://programminghistorian.org/lessons/getting-started-with-markdown).
* Tenen, Dennis, and Grant Wythoff. 2014. “Sustainable Authorship in Plain Text Using Pandoc and Markdown.” Programming Historian, March. [https://programminghistorian.org/lessons/sustainable-authorship-in-plain-text-using-pandoc-and-markdown](https://programminghistorian.org/lessons/sustainable-authorship-in-plain-text-using-pandoc-and-markdown).

## Homework{#HWL04}

* Convert a plain text paper into markdown and convert it with Pandoc into a PDF, MS Word, and HTML documents.
	* Plain text file for the task: [McCarty_Modeling.txt](./files/McCarty_Modeling.txt)
	* Use this PDF file as a guide for your formatting: [McCarty_Modeling.pdf](./files/McCarty_Modeling.pdf) 
	* Convert only the first 7 pages. You can skip up to 1/3 of bibliographical records, if you cannot find them online.
	* Alternatively, you can use any of your own papers that you have already written: 5 pages, 10 footnotes, 5 bibliography items.

**Python**

- Work through Chapters 3 and 5 of Zelle's book; read chapters carefully; work through the chapter summaries and exercises; complete the following programming exercises: 1-8 in Chapter 3; 1-7 in Chapter 5;
- Watch [Dr. Vierthaler's videos](https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17):
	- Episode 07: Booleans (and Boolean Operators)
	- Episode 08: Loops (and file objects)

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfoler in your `HW070172` repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)
	* In the subject of your email, please, add the following: `CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber`, where `CCXXXXX` is the numeric code of the course; `LXX` is the lesson for which the homework is being submitted; `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.

<!--chapter:end:02-Lesson04.Rmd-->

# Lesson 05

## Constructing Robust Searches

## `regular expressions`

In this lesson we will learn about `regular expressions`, an important semi-language for constructing complex searches. Any text editor that supports `regular expressions` will work fine for this lesson, but let's all use [Sublime Text](https://www.sublimetext.com/) (both Mac and Windows).

Let's use the following practicum files (`Right Click > Save File as ...`) for the in-class practice: 1) [version for training](https://raw.githubusercontent.com/maximromanov/re_tutorial/master/re_practucum_text_western.txt); 2) [version with answers](https://raw.githubusercontent.com/maximromanov/re_tutorial/master/re_practucum_text_western_answers.txt).

Open the practicum file in `Sublime Text`.

### What are `regular expressions`?**

- very small language for describing textual patterns
- not a programming language, yet a part of each one
- incredibly powerful tool for find/replace operations
- old (1950s-60s)
- “arcane art”
- ubiquitous

![**Source**: <https://xkcd.com/208/>](https://imgs.xkcd.com/comics/regular_expressions.png)

### What would we use `regular Expressions` for?

**to search**:

  * all spelling variations of the same word:
    - Österreich, Osterreich or Oesterreich..
  * words of specific morphological patterns:
    - [*search*], [*search*]er, [*search*]ed, [*search*]ing, [*search*]es: all derivatives from the same root/word 
  * entities that may be referred to differently: 
    - references to Vienna in different languages? (Wien, Vienna, Вена, فيينا, etc.) 
    - references to Austria? (Vienna, Graz, Linz, Salzburg, Innsbruck, etc.)
  * references to concepts:
    - references to education in biographies: "s/he graduated from", "s/he studied", etc. 

**to search and replace**:

  * reformat “dirty”/inconsistent data (OCR output, for example)

**to tag**:

  * make texts navigable and more readable
  * tag information relevant to your research 

**and many other uses…**

### The Basics 

A *regular expression* (can be shortened as *regex* or *regexp*) is a sequence of symbols and characters expressing a string or pattern to be searched for within a longer piece of text. In this sequence there are characters that match themselves (most characters) and there are characters that activate special functionality (*special characters*). For example: 

- `Vienna` is a regular expression that matches “Vienna”;
- “`Vienna`” is a pattern;

**Question:** if the pattern `at` matches strings with “a” followed by “t”, which of the following strings will it match?[^Q1]

[]() |     |      |       |     |        |
-----|-----|------|-------|-----|--------|
at   | hat | that | atlas | aft | Athens |

[^Q1]: Matches are highlighted: **at**, h**at**, th**at**, **at**las, aft, Athens.

### Characters *&* Special Characters

- most characters match themselves.
- matching is case sensitive.
- special characters: `()^${}[]\|.+?*`. 
- to match a special character in your text, you need to “escape it”, i.e. precede it with “\” in your pattern: 
	– `Osterreich [sic]` **does not* match “Osterreich [sic]”.
	– `Osterreich \[sic\]` matches “Osterreich [sic]”.
	

### Character Classes: `[]` 

- characters within `[]` are choices for a single-character match; think of this as a type of **either or**.
- the order within `[]` is unimportant.
	- `x[01]` matches “x0” and “x1”.
	- `[10][23]` matches “02”, “03”, “12” and “13”.
- initial `^` negates the class: 
	– `[^45]` matches any character except 4 or 5.

**Question:** if the pattern `[ch]at` matches strings with “c” or “h” followed by “a”, and then by “t”, which of the following strings will this regular expression match?[^Q2]:

[]() |    |      |     |     |      |
-----|----|------|-----|-----|-------
that | at | chat | cat | fat | phat |

[^Q2]: Matches are highlighted: t**hat**, at, c**hat**, **cat**, fat, p**hat**.

### Ranges (within classes) 

- Ranges define sets of characters within a class. 
	– `[1-9]` matches any number in the range from 1 to 9 (i.e., any non-zero digit)
	– `[a-zA-Z]` matches any letter of the English alphabet (ranges for specific languages will vary)
	– `[12][0-9]` matches numbers between 10 and 29 (i.e., the first digit is either 1 or 2; the second one---any digit) 

**Ranges shortcuts**

Shortcut | Name | Equivalent Class  
:---------|:------|:----------------
`\d` | digit       | `[0-9]`  
`\D` | not digit   | `[^0-9]`  
`\w` | word        | `[a-zA-Z0-9_]` (*actually more*) 
`\W` | not word    | `[^a-zA-Z0-9_]` (*actually more*)
`\s` | space       | `[\t\n\r\f\v ]`  
`\S` | not space   | `[^\t\n\r\f\v ]`  
`.`  | everything  | `[^\n]` (depends on mode)  

**Question:** if the pattern `/\d\d\d[- ]\d\d\d\d/` matches strings with a group of three digits, followed by a space or a dash, and then---by another group of four digits, which of the following strings will this regular expression match?[^Q3]:


[]() |    |      |     |     |      |
-----|----|------|-----|-----|-------
501-1234 | 234 1252 | 652.2648 | 713-342-7452 | PE6-5000 | 653-6464x256 |

[^Q3]: Matches are highlighted: **501-1234**, **234 1252**, 652.2648, 713-**342-7452**, PE6-5000, **653-6464**x256.

### Repeaters

- these special characters indicate that the preceding element of the pattern can be repeated in a particular manner: 
	- `runs?` matches “runs” or “run”
	- `1\d*` matches any number beginning with “1”

repeater | count 
:--------|:------
`?`        | zero or one
`+`        | one or more
`*`        | zero or more
`{n}`      | exactly *n* times
`{n,m}`    | between *n* and *m* times
`{,m}`     | no more than *m* times
`{n,}`     | no less than *n* times

**Question:** We have several patterns, which strings will they match?[^Q4]

Patterns  |           |               |           |           |       
:---------|:----------|:--------------|:----------|:----------|:----------
A) `ar?t` | B) `ar*t` |  C) `a[fr]?t` | D) `ar+t` | E) `a.*t` | F) `a.+t`

Strings |          |             |
:-------|:---------|:------------|:----------
1) “at” | 2) “art” | 3) “arrrrt” | 4) “aft” 

[^Q4]: `ar?t` matches “at” and “art” but not “arrrt”; `a[fr]?t` matches “at”, “art”, and “aft”; `ar*t` matches “at”, “art”, and “arrrrt”; `ar+t` matches “art” and “arrrt” but not “at”; `a.*t` matches anything with an ‘a’ eventually followed by a ‘t’.

### Lab: *Intro*  (in the *practicum file*).

![](./images/regex01.png)

### Anchoring

- anchors match *between* characters.
- anchors are used to assert that the characters you’re matching must appear in a certain place.
- for example, `\bat\b` matches “**at** work” but not “b**at**ch”.

Anchor | matches...
:------|:----------
`^`    | the beginning of a line or a string
`$`    | the end of a line of a string
`\b`   | *word boundary*
`\B`   | *not word boundary*

### Alternation: “|” (*pipe*)

- in `regex`, “|” means “or”
  - on the US keyboard layout, this character is in the vicinity of “Enter” and “Right Shift”.
- you can put a full expression to the left of the *pipe* and another full expression to the right, so that either one could match:
  - `seek|seeks|sought` matches “seek”, “seeks”, or “sought”. 
  - `seeks?|sought` matches “seek”, “seeks”, or “sought”. 

### Grouping 

- everything within `( … )` is grouped into a single element for the purposes of repetition or alternation:
  - the expression `(la)+` matches “la”, “lala”, “lalalala” (but not “all”). 
  - `schema(ta)?` matches “schema” and “schemata” but not “schematic”. 
- grouping example: what regular expression would match “eat”, “eats”, “ate” and “eaten”? 
  - `eat(s|en)?|ate`
  - **NB**: we can make it more precise by adding word boundary anchors to exclude what we do not need, like, for example, words “sate” and “eating”: `\b(eat(s|en)?|ate)\b`.
  

### Lab: Part I (in the *practicum file*).

![](./images/regex02.png)

### Replacement

- `regular expressions` are most often used for search/replace operations
- in *text editors*:
  - *Search Window*: search pattern
  - *Replace Window*: replacement pattern
  
### Capture

- during searches, `( … )` groups capture patterns for use in replacement.
- special variables `\1`, `\2`, `\3`, etc. contain the capture (in some text editors: `$1`, `$2`, `$3`).
- if we apply `(\d\d\d)-(\d\d\d\d)` to “123-4567”:
  – `\1` (or, `$1`) captures “123”
  – `\2` (or, `$2`) captures “4567”

### Capture *&* Reformat

- How to convert “Schwarzenegger, Arnold” to “Arnold Schwarzenegger”?
  - Search: `(\w+), (\w+)`
  - Replace (a): `\2 \1`
  - Replace (b): `$2 $1`
- **NB:** (!) Before hitting “Replace”, make sure that your match does not catch what you do NOT want to change

### Lab: Part II (in the *practicum file*).

![](./images/regex02.png)

- Finding toponyms (placenames):
  - *very simple:* Construct regular expressions that find references to *all* Austrian cities.[^QII1]
  - *a bit tricky*: Construct regular expression that finds only cities from 1) Lower Austria; 2) Salzburg.[^QII2]

[^QII1]: *Solution*: Simply connect all toponyms from the list with a pipe symbol “|”
[^QII2]: *Solution 1*: `\b([\w ]+) \(Lower Austria\)` --- for Lower Austria; `\b([\w ]+) \(Salzburg\)` --- for Salzburg; *Solution 2 (cooler):* `\b([\w ]+)(?=( \(Lower Austria\)))` --- for Lower Austria; `\b([\w ]+)(?=( \(Salzburg\)))` --- for Salzburg.

### *To keep in mind*

- `regular expressions` are “greedy,” i.e. they tend to catch more than you may need. Always test!
- test before applying! (In text editors Ctrl+Z (Win), Cmd+Z (Mac) can help to revert changes)
- check the language/application-specific documentation: some common shortcuts are not universal (for example, some languages/applications use `\1` to refer to groups, while others use `$1` for the same purpose).

## Class materials

* Presentation with all the slides:
	- [PDF](https://github.com/maximromanov/re_tutorial/blob/master/RegularExpressions_Western.pdf?raw=true) (Windows PowerPoint Format)

## Digital materials

* Online references:
	- <http://www.regular-expressions.info/>
	- <http://ruby.bastardsbook.com/chapters/regexes/>
* Interactive tutorial: <http://regexone.com/>
* Cheat Sheets:
	- <http://krijnhoetmer.nl/stuff/regex/cheat-sheet/>
	- <http://www.rexegg.com/regex-quickstart.html>


## Reference Materials

* Goyvaerts, J. and Levithan, S. (2012). *Regular Expressions Cookbook*. Second edition. Beijing: O’Reilly. [Amazon Link](http://www.amazon.com/Regular-Expressions-Cookbook-Jan-Goyvaerts/dp/1449319432/).
* Friedl, J. E. F. (2006). *Mastering Regular Expressions*. 3rd ed. Sebastapol, CA: O’Reilly. [Amazon Link](http://www.amazon.com/Mastering-Regular-Expressions-Jeffrey-Friedl/dp/0596528124/)

(I will share PDFs of these books via Slack; I strongly recommend to flip through the first book just to get an idea of what kind of things one can do with regular expressions.)

## Homework{#HWL05}

- Finish the practicum; push your answers to your github repository.

**Python**

- Work through Chapters 6 and 7 of Zelle's book; read chapters carefully; work through the chapter summaries and exercises; complete the following programming exercises: 1-8 in Chapter 6; 1-9 in Chapter 7;
- Watch [Dr. Vierthaler's videos](https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17):
	- Episode 09: Dictionaries
	- Episode 10: Putting it Together (Analyses)
	- Episode 11: Errors (reading and handling)
- **Note:** the sequences are somewhat different in Zelle's textbook and Vierthaler's videos. I would recommend you to always check Vierthaler's videos and also check videos which cover topics that you read about in Zelle's book.

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfoler in your `HW070172` repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)
	* In the subject of your email, please, add the following: `CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber`, where `CCXXXXX` is the numeric code of the course; `LXX` is the lesson for which the homework is being submitted; `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.

<!--chapter:end:02-Lesson05.Rmd-->

# Lesson 06

## Understanding Structured Data

Doing Digital Humanities practically always means working with structured data of some kind. In most general terms, structured data means some explicit annotation or classification that the machine can understand, and therefore --- effectively use. When we see the word “Vienna”, we are likely to automatically assume that this is the name of the capital of Austria. The machine cannot know that, unless there is something else in the data that allows it to figure it out (here, an XML tag): `<settlement country="Austria" type="capital city">Vienna</settlement>` --- from this *annotation* (and its attributes) the machine can be instructed to interpret the string `Vienna` as a *settlement* of the type *capital city* in the country of *Austria*. It is important to understand most common data formats in order to be able to create and generate them as well as to convert between different formats.

When we decide which format we want to work with, we need to consider the following: the ease of working with a given format (manual editing); suitability for specific analytical software; human-friendliness and readability; open vs. proprietary. In general, it does not make any sense to engage in *the format wars* (i.e., one format is better than another); one should rather develop an understanding that almost every format has its use and value in specific contexts or for specific tasks. What we also want is not to stick to a specific format and try to do everything with it and only it, but rather to be able to write scripts with which we can generate data in suitable formats or convert our data from one format into another.

Let's take a look at a simple example in some most common formats.

### XML (*Extensible Markup Language*)

``` xml
<note>
  <to>Tove</to>
  <from>Jani</from>
  <heading>Reminder</heading>
  <body>Don’t forget me this weekend!</body>
</note>
```

### CSV/TSV (*Comma-Separated Values*/ *Tab-Separated Values*)

```
to,from,heading,body
Tove,Jani,Reminder,Don’t forget me this weekend!
```

``` cs
to,from,heading,body
"Tove","Jani","Reminder","Don’t forget me this weekend!"
```

### JSON (*JavaScript Object Notation*)

``` json
{
  "to": "Tove",
  "from": "Jani",
  "heading": "Reminder",
  "body": "Don’t forget me this weekend!"
}
```

### YML or YAML (*Yet Another Markup Language* > *YAML Ain't Markup Language*)

``` yml
to: Tove
from: Jani
heading: Reminder
body: Don’t forget me this weekend
```

### `BibTeX`: *most common bibliographic format*

We have already used this format in our lesson on sustainable writing. If you take a closer look at the record below, you may see that this format contains lots of valuable information. Most of this we will need for our project.

```bibtex
@incollection{LuhmannKommunikation1982,
  title = {Kommunikation mit Zettelkiisten},
  booktitle = {Öffentliche Meinung und sozialer Wandel: Für Elisabeth Noelle-Neumann = Public opinion and social change},
  author = {Luhmann, Niklas},
  editor = {Baier, Horst and Noelle-Neumann, Elisabeth},
  date = {1982},
  pages = {222--228},
  publisher = {{westdt. Verl}},
  location = {{Opladen}},
  annotation = {OCLC: 256417947},
  file = {Absolute/Path/To/PDF/Luhmann 1982 - Kommunikation mit Zettelkiisten.pdf},
  isbn = {978-3-531-11533-7},
  langid = {german}
}
```

## Larger Examples

**NB** data example from [here](https://gist.github.com/Miserlou/c5cd8364bf9b2420bb29).

There are some online converters that can help you to convert one format into another. For example: <http://www.convertcsv.com/>.

### `CSV` / `TSV`

```
city,growth_from_2000_to_2013,latitude,longitude,population,rank,state
New York,4.8%,40.7127837,-74.0059413,8405837,1,New York
Los Angeles,4.8%,34.0522342,-118.2436849,3884307,2,California
Chicago,-6.1%,41.8781136,-87.6297982,2718782,3,Illinois
```

`TSV` is a better option than a `CSV`, since `TAB` characters are very unlikely to appear in values.

Neither `TSV` not `CSV` are good for preserving *new line characters* (`\n`)---or, in other words, text split into multiple lines. As a workaround, one can convert `\n` into some unlikely-to-occur character combination (for example, `;;;`), which would allow to restore `\n` later , if necessary.

### `JSON`

``` json
[
    {
        "city": "New York", 
        "growth_from_2000_to_2013": "4.8%", 
        "latitude": 40.7127837, 
        "longitude": -74.0059413, 
        "population": "8405837", 
        "rank": "1", 
        "state": "New York"
    }, 
    {
        "city": "Los Angeles", 
        "growth_from_2000_to_2013": "4.8%", 
        "latitude": 34.0522342, 
        "longitude": -118.2436849, 
        "population": "3884307", 
        "rank": "2", 
        "state": "California"
    }, 
    {
        "city": "Chicago", 
        "growth_from_2000_to_2013": "-6.1%", 
        "latitude": 41.8781136, 
        "longitude": -87.6297982, 
        "population": "2718782", 
        "rank": "3", 
        "state": "Illinois"
    }
]
```


### `YML`/`YAML`

YAML is often used only for a single set of parameters.

``` yml
city: New York 
growth_from_2000_to_2013: 4.8% 
latitude: 40.7127837 
longitude: -74.0059413
population: 8405837 
rank: 1 
state: New York
```

But it can also be used for storage of serialized data. It has advantages of both JSON and CSV: the overall simplicity of the format (no tricky syntax) is similar to that of CSV/TSV, but it is more readable than CSV/TSV in any text editor, and is more difficult to break—again, due to the simplicity of the format.

``` yml
New York:
  growth_from_2000_to_2013: 4.8% 
  latitude: 40.7127837 
  longitude: -74.0059413 
  population: 8405837 
  rank: 1 
  state: New York 
Los Angeles:
  growth_from_2000_to_2013: 4.8% 
  latitude: 34.0522342 
  longitude: -118.2436849 
  population: 3884307 
  rank: 2 
  state: California
Chicago:
  growth_from_2000_to_2013: -6.1% 
  latitude: 41.8781136 
  longitude: -87.6297982 
  population: 2718782 
  rank: 3 
  state: Illinois
```

YAML files can be read with Python into `dictionaries` like so:

```py
import yaml
dictionary = yaml.load(open(pathToFile))
```

You will most likely need to install `yaml` library; it is also quite easy to write a script that would read such serialized data.

**Note on installing libraries for `python`**. In general, it should be as easy as running the following command in your command line tool:

```
pip install --upgrade libraryName
```

- `pip` is the standard package installer for `python`; if you are running version 3.xx of `python`, it may be `pip3` instead of `pip`. If you have Anaconda installed, you can also use Anacodnda interface to install packages;
- `install` is the command to install a package that you need;
- `--upgrade` is an optional argument that you would need only when you upgrade already installed package;
- `libraryName` is the name of the library that you want to install.

This should work just fine, but sometimes it does not---usually when you have multiple versions of `python` installed and they may start conflicting with each other (another good reason to handle your `python` installations via Anaconda). There is, luckily, a workaround that seems to do the trick.You can modify your command in the following manner:

```
python -m pip install --upgrade libraryName
```

- `python` here is whatever alias you are using for running `python` (e.g., in my case it is `python3`, so the full command will look: `python3 -m pip install --upgrade libraryName`)

## In-Class Practice

Let's try to convert this [`bibTex` file](./files/zotero_biblatex_sample.bib) into other formats. Before we begin, however, let's break down this task into smaller tasks and organize them together in some form of `pseudocode`.

- Which of the above-discussed formats would be most suitable? Why yes, why no?

## Homework{#HWL06}

- Take your bibliography in `bibTeX` format and convert it into: `csv`/`tsv`, `json`, and `yaml`;
  - *Hint*: you should load your data into a `dictionary`;
  - additionally, you might want to create (manually) a dictionary of `bibTeX` field: some of the fields are named differently, while they contain the same type of information --- you want to identify those fields and unify them for your output format, which will improve the quality of your data (the process usually called *normalization*); *hint:* in order to figure out how to identify those fields, you may want to look into the *Word Frequency* program in Chapter 11. You can use this approach to identify all fields and count their frequencies, which will help you to determine which fields to keep and which to normalize (i.e., merge low-frequency fields into high-frequency fields).
- upload your results together with scripts to your homework github repository

**Python**

- Work through Chapters 8 and 11 of Zelle's book; read chapters carefully; work through the chapter summaries and exercises; complete the following programming exercises: 1-8 in Chapter 8 and 1-11 in Chapter 11;
- Watch [Dr. Vierthaler's videos](https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17):
	- Episode 12: Functions
	- Episode 13: Libraries and NLTK
	- Episode 14: Regular Expressions
- **Note:** the sequences are somewhat different in Zelle's textbook and Vierthaler's videos. I would recommend you to always check Vierthaler's videos and also check videos which cover topics that you read about in Zelle's book.

**Webscraping (optional)**

- if you are interested in *webscraping*, you can check the following tutorials:
  - Milligan, Ian. 2012. “Automated Downloading with Wget.” Programming Historian, June. <https://programminghistorian.org/lessons/automated-downloading-with-wget>.
  - Kurschinski, Kellen. 2013. “Applied Archival Downloading with Wget.” Programming Historian, September. <https://programminghistorian.org/lessons/applied-archival-downloading-with-wget>.
  - Baxter, Richard. 2019. “How to download your website using WGET for Windows.” <https://builtvisible.com/download-your-website-with-wget/>.
  - Alternatively, this operation can be done with a Python script: Turkel, William J., and Adam Crymble. 2012. “Downloading Web Pages with Python.” Programming Historian, July. <https://programminghistorian.org/lessons/working-with-web-pages>.

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfoler in your `HW070172` repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)
	* In the subject of your email, please, add the following: `CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber`, where `CCXXXXX` is the numeric code of the course; `LXX` is the lesson for which the homework is being submitted; `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.
	
## Homework Solution{#HWL06S}

Before we proceed, let's make sure that you have the same folder structure on you machine. This will help to ensure that we will not run into other issiues and can focus on solving one problem at a time. (Please, download [these files](./files/L06_Conversion.zip) for `L06_Conversion` folder: unzip and move them all into the folder). The structure should be as follows:

```
.
├── MEMEX_SANDBOX
│   └── data
├── L06_Conversion
│   ├── comments.md
│   ├── pseudocode.md
│   ├── z_1_preliminary.py
│   ├── z_2_conversion_simple.py
│   ├── z_config.yml
│   └── zotero_biblatex_sample.bib
└── L07_Memex_Step1
    ├── ... your scripts ...
    └── ... your scripts ...

```

**NB:** `./MEMEX_SANDBOX/data/` is the target folder for all other assignments to follow. This is where we will be creating our memex.

### Pseudocode solution


0. look at the file, i.e. check the file in order to understand it structure
1. create a holder for our data, which will be `dictionary` (dic, list, etc.)
2. read as one big string
3. split into records using `\n@`
	- we will get a list of strings
4. loop through all the records:
	NB: each records is a string that needs to be converted into something else.
	- we need to split each record using `,\n`
	- now we loop through the list of "key-value" pairs
	- `type{citationkey` element:
		- grab list element with index 0 (`citationkey = record[0]`)
		- split the element on `{`
			- recordType = element[0]
			- citationKey = element[1]
	- add a record into our `dictionary` using citationKey as a key value
	- add recordType into the newly created record
	- process the rest of the record:
		- loop through the record, starting with 1:
			- `for r in record[1:]:`
		- split every element on `=`
			- key = element[0].strip()
			- value = element[1].strip()
			- add our key-value pair into the `dictionary`
5. Save `dictionary` into CSV, JSON, YAML


### Scripts

**Script 1: analyzing bibTex data (`z_1_preliminary.py`)**

```py
import os, yaml

###########################################################
# VARIABLES ###############################################
###########################################################

settingsFile = "z_config.yml"
vars = yaml.load(open(settingsFile))

###########################################################
# FUNCTIONS ###############################################
###########################################################

# analyze bibTeX data; identify what needs to be fixed

def bibAnalyze(bibTexFile):

    tempDic = {}

    with open(bibTexFile, "r", encoding="utf8") as f1:
        records = f1.read()
        records = records.split("\n@")

        for record in records[1:]:
            # let process ONLY those records that have PDFs
            if ".pdf" in record.lower():
                record = record.strip()
                record = record.split("\n")[:-1]

                for r in record[1:]:
                    r = r.split("=")[0].strip()

                    if r in tempDic:
                        tempDic[r] += 1
                    else:
                        tempDic[r] = 1

    results = []
    for k,v in tempDic.items():
        result = "%010d\t%s" % (v, k)
        results.append(result)

    results = sorted(results, reverse=True)
    results = "\n".join(results)

    with open("bibtex_analysis.txt", "w", encoding="utf8") as f9:
        f9.write(results)

bibAnalyze(vars['bib_all'])

```

This script will create the file `bibtex_analysis.txt`, which will be a frequency list of keys from all `bibTeX` records. We would want to convert this frequency list into a YML file which we can then load with `yaml` library (make sure to install it!). Loading yml data into a python dictionary is as easy as: `dictionary = yaml.load(open(fileNameYml))`.

You can convert the frequency list into a proper yml file using regular expressions:

![](./images/reformatToYML.gif)

**Script 2: loading bibTeX data and converting to other formats (`z_2_conversion_simple.py`)**

```python

import re
import yaml

"""
1. load bibtex file
    - bibliography should be curated in Zotero (one can program cleaning procedures into the script, but this is not as reliable);
    - loading bibtex data, keep only those records that have PDFs;
    - some processing might be necessary (like picking one file out of two and more)
2. convert into other formats
    - csv
    - json
    - yml
"""

###########################################################
# VARIABLES ###############################################
###########################################################

settingsFile = "z_config.yml"
settings = yaml.load(open(settingsFile))
bibKeys = yaml.load(open("zotero_biblatex_keys.yml"))

###########################################################
# FUNCTIONS ###############################################
###########################################################

# load bibTex Data into a dictionary
def bibLoad(bibTexFile):

    bibDic = {}

    with open(bibTexFile, "r", encoding="utf8") as f1:
        records = f1.read().split("\n@")

        for record in records[1:]:
            # let process ONLY those records that have PDFs
            if ".pdf" in record.lower():

                record = record.strip().split("\n")[:-1]

                rType = record[0].split("{")[0].strip()
                rCite = record[0].split("{")[1].strip().replace(",", "")

                bibDic[rCite] = {}
                bibDic[rCite]["rCite"] = rCite
                bibDic[rCite]["rType"] = rType

                for r in record[1:]:
                    key = r.split("=")[0].strip()
                    val = r.split("=")[1].strip()
                    val = re.sub("^\{|\},?", "", val)

                    fixedKey = bibKeys[key]

                    bibDic[rCite][fixedKey] = val


    print("="*80)
    print("NUMBER OF RECORDS IN BIBLIGORAPHY: %d" % len(bibDic))
    print("="*80)
    return(bibDic)

###########################################################
# CONVERSION FUNCTIONS ####################################
###########################################################

import json
def convertToJSON(bibTexFile):
    data = bibLoad(bibTexFile)
    with open(bibTexFile.replace(".bib", ".json"), 'w', encoding='utf8') as f9:
        json.dump(data, f9, sort_keys=True, indent=4, ensure_ascii=False)


import yaml
def convertToYAML(bibTexFile):
    data = bibLoad(bibTexFile)
    with open(bibTexFile.replace(".bib", ".yaml"), 'w', encoding='utf8') as f9:
        yaml.dump(data, f9)

# CSV is the trickest because bibTeX is not symmetrical
def convertToCSV(bibTexFile):
    data = bibLoad(bibTexFile)
    # let's handpick fields that we want to save: citeKey, type, author, title, date
    headerList = ['citeKey', 'type', 'author', 'title', 'date'] 
    header = "\t".join(headerList)

    dataNew = [header]

    for k,v in data.items():
        citeKey = k

        if 'rType' in v:
            rType = v['rType']
        else:
            rType = "NA"

        if 'author' in v:
            author = v['author']
        else:
            author = "NA"

        if 'title' in v:
            title = v['title']
        else:
            title = "NA"

        if 'date' in v:
            date = v['date']
        else:
            date = "NA"

        tempVal = "\t".join([citeKey, rType, author, title, date])
        dataNew.append(tempVal)

    finalData = "\n".join(dataNew)
    with open(bibTexFile.replace(".bib", ".csv"), 'w', encoding='utf8') as f9:
        f9.write(finalData)


###########################################################
# RUN EVERYTHING ##########################################
###########################################################

print(settings["bib_all"])

#convertToJSON(settings["bib_all"])
#convertToYAML(settings["bib_all"])
#convertToCSV(settings["bib_all"])


print("Done!")

```

<!--chapter:end:02-Lesson06.Rmd-->

# Lesson 07

## Building Memex - Step 1

First and foremost, we need to build the structure for our memex. This involves the following steps:

1. loading our bibTeX data and parsing it properly into a dictionary (you can build on the solution to the previous lesson);
2. looping through the dictionary of our bibliographical data in order to save relevant information into the structure of our memex, which shoould be as shown on the screenshot below. Such folder structure is very easy to generate computationally knowing only the citation key of each publication, which will allow to manipulate our data with ease, no matter how massive it becomes. 

![](./images/memexFolderStructure.png)

### Pseudocode

- load bibliographical data into a dictionary
  - some data will need to be filtered, fixed, and probably reformatted slightly, etc. (build on the previously created function); use your Zotero data;
- loop through your dictionary and process each record:
  - generate relative path for your record
    - this path will consist of two parts:
      - the path to your memex folder (either relative or absolute); this one you can define in your yml settings file (something like `./MEMEX_SANDBOX/data/`).
      - the relative path to a publucation within your memex folder
        - the relative path must be generated from the citation key of each publication like this --- `./1stLetter/2firstLetters/citeKey/`
        - thus, if the key is `McCartyHumanities2014`, the path must be `./m/mc/McCartyHumanities2014/` (subFolder = `key.lower()[0]`;
 subSubFolder = `key.lower()[:2]`; subSubSubFolder = `key`; **Question:** Why would we need to `.lower()`?)
	- create the subfolder, using this path
	- save into this folder a bibTeX record for the publication with the name `key.bib` (i.e., `McCartyHumanities2014.bib`);
	- copy the PDF file into this folder and save it in a similar manner: `key.pdf` (i.e., `McCartyHumanities2014.pdf`).

### Code snippets

Some of the following code snippets should be helpful (although you may find different ways of getting to the same results, which is, of course, totally fine).

**Snippet 1: Load YML File into a Dictionary**

```python
import yaml

settingsFile = "z_config.yml"
settings = yaml.load(open(settingsFile))

```

**Snippet 2: Create Folder, if Does Not Exist**

```python
import os

if not os.path.exists(folder):
  os.makedirs(folder)
```

**Snippet 3: Copy File to a Distination**

```python
import shutil

if not os.path.isfile(dst):
  shutil.copyfile(src, dst)
```

where `scr` is the path to where the file was, and `dst` is the path to where you want your file. Keep in mind, that in `dst` you can use the name of the file that you want! 

**Snippet 4: Making Path Properly**

As you remember from our introduction to command line, Mac and Windows format paths differently, which will create issues if *manually* construct the path. The best way to avoid any issues is to construct paths with a proper library.

```python
import os

yourPath = os.path.join(folder, subfolder, subsubfolder, fileName)

````

## Homework{#HWL06}

- the assignment is described above
- additionally, take the solution scripts from the previous lesson and annotate every line of code; submit your annnotations together with the main assignment
- upload your results to your homework github repository


## Homework Solution{#HWL07S}

### Pseudocode

- creating the overall structure for our MEMEX on HDD
	1) using out bibliographical data, we algorithmically generate paths for each publication
	2) we create those paths
	3) we copy relevant data into those paths:
		1) bibliographical record (bib);
		2) PDF files

Let’s break it down into smaller steps:

- first of all, it will actually be easier of take a look at this task from another perspective.
	- we should start from a single publication, or, better, a single bibliographical record.
	- after we figure out how to process a single publication, we will just need to figure out how to process all of them

- so, let's start with a single bib record:
	- (we have written a function before that creates a dictionary from every bib record (let's call it **loadBib()**); so let our function take such a single-record dictionary as its argument);
	- what will we need from our function (let's call it **processBibRecord()**)? it must do the following:
		- to generate a unique path for the publication
		- check if that path exists, if not: create it
		- save bib record into that folder
		- copy PDF to that folder
	- what do we need to consider:
		- we need a simple and transparent way to algorithmically generate all this data, which will ensure easy access, browsability, and overall transparency of our approach. In order to achieve it, we can use *a single* element from each record to generate all necessary paths and filenames. Namely, the *citation key*.

	- **path:** our path will be a concatenation of the path to memex (for example, `./MEMEX_SANDBOX/data/`) and a subpath unique to each publication, which we can generate from the citation key.
		- for example, if the citation key is `SavantMuslims2017`, the publication path should be `/s/sa/SavantMuslims2017/`
		- the concatenated path thus will be: `./MEMEX_SANDBOX/data/s/sa/SavantMuslims2017/`
		- **NB:** it may be a good idea to have a small function that takes paths to memex and the citation key as its arguments and returns the final path (let's call it **generatePublPath()**).
	- now, that we have **the path**, we can check whether it exists, and, if not, create it
	- now, we have our path and the actual folder on HDD, it is easy-peasy-lemon-squeezy to save a bibliographical record and a PDF into that folder.
		- for the same considerations of machine-readability, we should use use the citation key to name our files: for example, `SavantMuslims2017.bib` for our bibliographical record, and `SavantMuslims2017.pdf` for our PDF file.

- now, we have a function that does all that we need from a single record, we can write another function (let's call it **processAllRecords()**) that will
	- take the large bibliographical dictionary generated with **loadBib()** as its argument
	- loops through this dictionary, and processes each record with **processBibRecord()**.


### Note on Folder/File Organization

It is crucial that all our files have their places and there are only as many files as we need. Otherwise, things get unmanageable very quickly.

1. Create `memex_sandbox` repository on github
	1. add `.gitignore` with template for Python
	2. edit `.gitignore` on github.com, by adding the following two lines to the top

```
_data/
.DS_Store
```

2. Clone the repository to your machine
3. Let’s organize everything nicely here. Like this:

```sh
_bib/
_data/
_misc/
1_build_structure.md
1_build_structure.py
2_OCR_test.py
settings.yml
```

- `_bib/` : all bibliography-related files
	- Zotero bibliography
	- language codes dictionary
	- etc.
- `_data/` : folder where we will be generating all the data; this folder is added to `.gitignore` so that none of the data would be uploaded to github (reason: copyright)
- `_misc` : all other miscellaneous files go here; we can also move all the files we no longer need into this folder (keeping the main folder clean).
- `settings.yml` should look something like:

```yml
path_to_memex: ./_data/
bib_all: ./_bib/zotero_bibliography.bib
bibtex_keys: ./_bib/bibtex_keys.yml
```

### Script (`1_build_structure.py`)

```python

import os, shutil, re
import yaml

###########################################################
# VARIABLES ###############################################
###########################################################

settingsFile = "./settings.yml"
settings = yaml.load(open(settingsFile))

memexPath = settings["path_to_memex"]

###########################################################
# FUNCTIONS ###############################################
###########################################################

# load bibTex Data into a dictionary
def loadBib(bibTexFile):

    bibDic = {}

    with open(bibTexFile, "r", encoding="utf8") as f1:
        records = f1.read().split("\n@")

        for record in records[1:]:
            # let process ONLY those records that have PDFs
            if ".pdf" in record.lower():
                completeRecord = "\n@" + record

                record = record.strip().split("\n")[:-1]

                rType = record[0].split("{")[0].strip()
                rCite = record[0].split("{")[1].strip().replace(",", "")

                bibDic[rCite] = {}
                bibDic[rCite]["rCite"] = rCite
                bibDic[rCite]["rType"] = rType
                bibDic[rCite]["complete"] = completeRecord

                for r in record[1:]:
                    key = r.split("=")[0].strip()
                    val = r.split("=")[1].strip()
                    val = re.sub("^\{|\},?", "", val)

                    bibDic[rCite][key] = val

                    # fix the path to PDF
                    if key == "file":
                        if ";" in val:
                            #print(val)
                            temp = val.split(";")

                            for t in temp:
                                if t.endswith(".pdf"):
                                    val = t

                            bibDic[rCite][key] = val

    print("="*80)
    print("NUMBER OF RECORDS IN BIBLIGORAPHY: %d" % len(bibDic))
    print("="*80)
    return(bibDic)

# generate path from bibtex code, and create a folder, if does not exist;
# if the code is `SavantMuslims2017`, the path will be pathToMemex+`/s/sa/SavantMuslims2017/`
def generatePublPath(pathToMemex, bibTexCode):
    temp = bibTexCode.lower()
    directory = os.path.join(pathToMemex, temp[0], temp[:2], bibTexCode)
    return(directory)

# process a single bibliographical record: 1) create its unique path; 2) save a bib file; 3) save PDF file 
def processBibRecord(pathToMemex, bibRecDict):
    tempPath = generatePublPath(pathToMemex, bibRecDict["rCite"])

    print("="*80)
    print("%s :: %s" % (bibRecDict["rCite"], tempPath))
    print("="*80)

    if not os.path.exists(tempPath):
        os.makedirs(tempPath)

        bibFilePath = os.path.join(tempPath, "%s.bib" % bibRecDict["rCite"])
        with open(bibFilePath, "w", encoding="utf8") as f9:
            f9.write(bibRecDict["complete"])

        pdfFileSRC = bibRecDict["file"]
        pdfFileDST = os.path.join(tempPath, "%s.pdf" % bibRecDict["rCite"])
        if not os.path.isfile(pdfFileDST): # this is to avoid copying that had been already copied.
            shutil.copyfile(pdfFileSRC, pdfFileDST)


###########################################################
# PROCESS ALL RECORDS #####################################
###########################################################

def processAllRecords(bibData):
    for k,v in bibData.items():
        processBibRecord(memexPath, v)

bibData = loadBib(settings["bib_all"])
processAllRecords(bibData)

print("Done!")

```

<!--chapter:end:02-Lesson07.Rmd-->

# Lesson 08

## Building Memex - Step 2

Now that we have all files and foolders created, we need to extract text from PDFs. We can use OCR, or ([*optical character recognition*](https://en.wikipedia.org/wiki/Optical_character_recognition)), --- a method that allows us to extract editable text from images containing text.

## OCR software: Tesseract

In order to run OCR, we need to have a suitable software installed on our computers. Tesseract is one of such options---it is free and open source OCR engine. We will need to install Tesseract and some Python libraries that "cooperate" with Tesseract. Keep in mind that usually OCR is language specific, i.e. you also need to have additional files for relevant languages installed; and when you run an OCR procedure, you need to explicitely declare the language that you want to recognize (details below). Language codes used in Tesseract can be found at the end of this lesson (you can also find suggestions on how to deal with automatic language selection in your Python code).

General information on how to make Tesseract work on your machine can be found [here](https://tesseract-ocr.github.io/tessdoc/Installation.html).


#### Installing on Mac

We can use `brew` to install everything we need on Mac:

``` shell
brew install tesseract
```

Support for additional languages can be installed in the following manner (`lang` is the language code that Tesseract uses to refer to a specific language): 

``` shell
brew install tesseract-lang
```

The following command will list all available languages (as language codes). This will work only after you have installed Tesseract.

``` shell 
tesseract --list-langs
```

#### Installing on Linux

``` shell
sudo apt install tesseract-ocr
sudo apt install libtesseract-dev
```

On how to install additional languages, see [here](https://tesseract-ocr.github.io/tessdoc/Installation.html).

#### Installing on Windows

(**Source:** <https://automaticaddison.com/how-to-set-up-anaconda-for-windows-10/>)

- Go to [Tesseract at UB Mannheim](https://github.com/UB-Mannheim/tesseract/wiki).
- Download the Tesseract installer (64-bit).
- Run the installe and follow the the prompts. (You can choose additional languages during the installation.)

![](./images/tesseract01.jpg)

- Once Tesseract OCR is downloaded and installed, find it on your system.
- Copy the name of the folder where it is located. The default path is `C:\Program Files\Tesseract-OCR`
- Search for “Environment Variables” on your computer. (“Environment Variable” is a button in “System Properties”, Tab “Advanced”)

![](./images/tesseract02.jpg)

- Under “System Variables,” select “Path”, and then click “Edit”.
- Add the path: `C:\Program Files\Tesseract-OCR` (or whatever it is on you machine)

![](./images/tesseract03.jpg)

- Click “OK” a few times to close all windows.
- Open up the Anaconda Prompt or Anaconda Powershell Prompt
- Type `tesseract` and hit “Enter”. If everything is installed correctly, you should see:

``` shell
Usage:
  tesseract --help | --help-extra | --version
  tesseract --list-langs
  tesseract imagename outputbase [options...] [configfile...]

OCR options:
  -l LANG[+LANG]        Specify language(s) used for OCR.
NOTE: These options must occur before any configfile.

Single options:
  --help                Show this help message.
  --help-extra          Show extra help for advanced users.
  --version             Show version information.
  --list-langs          List available languages for tesseract engine.
```

#### For all systems after Tesserract is installed

After Tesseract is installed we need to install a couple of Python libraries:

``` shell
python -m pip install Pillow
python -m pip install pytesseract
```

Just remember that you need to use the Python command that you are using to run your scripts (for example, on Mac `python` is for Python 2.7, and `python3` is commonly for Python 3.X)

Alternatively, if you use Anaconda:

``` shell
conda install -c conda-forge Pillow
conda install -c conda-forge pytesseract
```

#### Final Test

In you command line tool, type `tesseract`. If everything is installed correctly, you shoudl see the following:

``` shell
user % tesseract

Usage:
  tesseract --help | --help-extra | --version
  tesseract --list-langs
  tesseract imagename outputbase [options...] [configfile...]

OCR options:
  -l LANG[+LANG]        Specify language(s) used for OCR.
NOTE: These options must occur before any configfile.

Single options:
  --help                Show this help message.
  --help-extra          Show extra help for advanced users.
  --version             Show version information.
  --list-langs          List available languages for tesseract engine.
```

## Task: General Map Pseudocode

Like in the previous lesson, let’s start with processing a single publication (using a dictionary with the data on a single record). We want to process PDF page by page, save an image of each page, extract text from every page and save the extracted text into some format that would keep the entire text of our publication, preserving separation into pages (*Hint:* dictionary > json). After we can do that, we will need to write a little bit of code that will process all our publicationa. What we want to have in the end is each and every publication in our memex processed this way. For consistency, let's call this script `2_OCR.py`

## Code snippets & functions

### New libraries

We will need the following new libraries that you will need to install, if you have not already:

``` python
pdf2image    # extracts images from PDF
pytesseract  # interacts with Tesseract, which extracts text from images
PyPDF2       # cleans PDFs
```

### Code Reuse

We have already discussed functions as a useful mechanism for code reuse; additionally, use can place often-used functions into a separate file (let's call it `functions.py`) and import it with `import functions` in our other script. (**IMPORTANT:** `functions.py` **must** be in the same folder as you other script that imports it). For example:

```python
#############################
# STORING FUNCTIONS #########
#############################

import os

# generate path from bibtex code:
def generatePublPath(pathToMemex, bibTexCode):
    temp = bibTexCode.lower()
    directory = os.path.join(pathToMemex, temp[0], temp[:2], bibTexCode)
    return(directory)
```

```python
#############################
# REUSING FUNCTIONS #########
#############################

import functions

pathToMemex = "pathToMemex"
citationKey = "Bush_AsWeMayThink_1945"

publPath = functions.generatePublPath(pathToMemex, citationKey)
```

**NB:** In general, you can also package your functions into a library and use it as a regular library. For more details, check Paul Vierthaler’s [“Studium Digitale” Series](https://www.youtube.com/playlist?list=PL6kqrM2i6BPKxJN3610r1NzT7u235hEjM) (particularly Video 5).

## Clean Temporary Copy of a PDF

Why do we need this function? You may have PDFs that you have already annotated with highlights, comments, etc. Highlights will interfere with the OCR process and will most likely be omitted from the final results. For this reason, we will want to clean all PDFs before we OCR them. In order not to delete your annotations, we will generate a temporary PDF file, which we can safely delete after processing it. 

The functions take a path to a PDF as its argument; creates a clean copy of it and saves it with a different name, replacing the suffix `.pdf` with `_TEMP.pdf`; and then returns the path to this temporary PDF. We can easily process it and delete after completion. 

``` python
import PyPDF2

def removeCommentsFromPDF(pathToPdf):
    with open(pathToPdf, 'rb') as pdf_obj:
        pdf = PyPDF2.PdfFileReader(pdf_obj)
        out = PyPDF2.PdfFileWriter()
        for page in pdf.pages:
            out.addPage(page)
            out.removeLinks()
        tempPDF = pathToPdf.replace(".pdf", "_TEMP.pdf")
        with open(tempPDF, 'wb') as f: 
            out.write(f)
    return(tempPDF)
```

### OCR-ing PDF

The following function takes three arguments: path to memex, citation key, and the language of the publication. The function processes PDFs: extracts images, OCRs them, collects results into a dictionary, saves OCR results into a JSON file.

``` python
import os, json
import pdf2image, pytesseract

def ocrPublication(pathToMemex, citationKey, language):
    publPath = functions.generatePublPath(pathToMemex, citationKey)
    pdfFile  = os.path.join(publPath, citationKey + ".pdf")
    jsonFile = os.path.join(publPath, citationKey + ".json")
    saveToPath = os.path.join(publPath, "pages")

    pdfFileTemp = removeCommentsFromPDF(pdfFile)

    if not os.path.isfile(jsonFile):
        if not os.path.exists(saveToPath):
            os.makedirs(saveToPath)
        
        print("\t>>> OCR-ing: %s" % citationKey)

        textResults = {}
        images = pdf2image.convert_from_path(pdfFileTemp)
        pageTotal = len(images)
        pageCount = 1
        for image in images:
            image = image.convert('1')
            finalPath = os.path.join(saveToPath, "%04d.png" % pageCount)
            image.save(finalPath, optimize=True, quality=10)

            text = pytesseract.image_to_string(image, lang=language)
            textResults["%04d" % pageCount] = text

            print("\t\t%04d/%04d pages" % (pageCount, pageTotal))
            pageCount += 1

        with open(jsonFile, 'w', encoding='utf8') as f9:
            json.dump(textResults, f9, sort_keys=True, indent=4, ensure_ascii=False)
    
    else:
        print("\t>>> %s has already been OCR-ed..." % citationKey)

    os.remove(pdfFileTemp)
```

### Missing functions

You can reuse these functions to assemble the entire process needed for this lesson. You will need to write a couple of your own though:

- first, you will need to solve the problem of picking the right language (see my comments below in *Tesseract Language Codes*)
- you will need to write the final block that processes all PDFs.

## One more note on OCR

Unfortunately, OCR-ing is a rather slow process, taking 10-30 seconds per page (depending on the speed of your computer). So, you will have to run it for some time in order to process everything---the more you have the longer it will take to process all. Good news is that this operation needs to be done only once per publication. Additionally, we can use some tricks to speed up the process, which we will discuss next time.

## Homework{#HWL08}

- the task is described above.
- additionally, take the solution scripts from the previous lesson and annotate every line of code; submit your annotations together with the main assignment; if you have any suggestions for improvements, please share them (this will count as extra points :).
- upload your results to your memex github repository
  - place annotated scripts into `_misc` subfolder


## Homework Solution{#HWL08S}

**Note**: all this new code is available here as well: <https://github.com/maximromanov/memex_sandbox>

First, a little update: I have added a function that will load settings from our YAML file. The main goals here are twofold: first, to avoid using YAML libraries (as we discovered, they function somewhat differently and one is not compartible with the latest versions of Python); second, to add a bit of flexibility to our settings file, which now can include annotations/comments that YAML files do not exactly support.

Here is an example of our new settings file:

```yaml
# !!! DO NOT CHANGE EXISTING KEYS !!! ONLY VALUES

# PATH TO MEMEX
path_to_memex: ./_data/

# BIBLIOGRAPHICAL FILES
bib_all: ./_bib/zotero_bibliography.bib
bibtex_keys: ./_bib/bibtex_keys.yml
language_keys: ./_bib/bibtex_languages.yml

# TEMPLATES AND CONTENT FILES FOR THE INTERFACE
template_page: ./_misc/template_page.html
template_index: ./_misc/template_index.html
content_index: ./_misc/content_index.html

# GENERATED FILES AND EXTENSIONS
memexDF_file: ./_data/memex.documentFrequencies
tfPubl_ext: .publTF # term frequencies per entire publication
tfPage_ext: .pageTF # term frequencies per each page
tfIdfPubl_ext: .publTFIDF # tfIdf frequencies per entire publication
tfIdfPage_ext: .pageTFIDF # tfIdf frequencies per each page
```

In this file you can add comments (starting with `#`); you can visually separate blocks of variables so that they are more readable. Make sure not to change the `keys` --- only the values may be changed.

Our loading function looks as shown in the block below. Essentially, it does the following: first, it removes all non-YAML elements; second, it splits the cleaned text into units that contain key-value pairs; and, third, it splits these units into keys and values and adds them into a dictionary, which is then "returned".

```python
def loadYmlSettings(ymlFile):
    with open(ymlFile, "r", encoding="utf8") as f1:
        data = f1.read()
        data = re.sub(r"#.*", "", data) # remove comments
        data = re.sub(r"\n+", "\n", data) # remove extra linebreaks used for readability
        data = re.split(r"\n(?=\w)", data) # splitting
        dic = {}
        for d in data:
            if ":" in d:
                d = re.sub(r"\s+", " ", d.strip())
                d = re.split(r"^([^:]+) *:", d)[1:]
                key = d[0].strip()
                value = d[1].strip()
                dic[key] = value
    return(dic)
```

The main OCR-ing Script:

```python
# NEW LIBRARIES
import pdf2image
import pytesseract

import os, yaml, json, random

import functions

###########################################################
# VARIABLES ###############################################
###########################################################

settingsFile = "settings.yml"
settings = yaml.load(open(settingsFile))

memexPath = settings["path_to_memex"]
langKeys = yaml.load(open(settings["language_keys"]))

###########################################################
# TRICKY FUNCTIONS ########################################
###########################################################

def ocrPublication(pathToMemex, citationKey, language):
    publPath = functions.generatePublPath(pathToMemex, citationKey)
    pdfFile  = os.path.join(publPath, citationKey + ".pdf")
    jsonFile = os.path.join(publPath, citationKey + ".json")
    saveToPath = os.path.join(publPath, "pages")

    if not os.path.isfile(jsonFile):
        if not os.path.exists(saveToPath):
            os.makedirs(saveToPath)
        
        print("\t>>> OCR-ing: %s" % citationKey)

        textResults = {}
        images = pdf2image.convert_from_path(pdfFile)
        pageTotal = len(images)
        pageCount = 1
        for image in images:
            text = pytesseract.image_to_string(image, lang=language)
            textResults["%04d" % pageCount] = text

            image = image.convert('1') # binarizes image, reducing its size
            finalPath = os.path.join(saveToPath, "%04d.png" % pageCount)
            image.save(finalPath, optimize=True, quality=10)

            print("\t\t%04d/%04d pages" % (pageCount, pageTotal))
            pageCount += 1

        with open(jsonFile, 'w', encoding='utf8') as f9:
            json.dump(textResults, f9, sort_keys=True, indent=4, ensure_ascii=False)
    
    else:
        print("\t>>> %s has already been OCR-ed..." % citationKey)

def identifyLanguage(bibRecDict, fallBackLanguage):
    if "langid" in bibRecDict:
        try:
            language = langKeys[bibRecDict["langid"]]
            message = "\t>> Language has been successfuly identified: %s" % language
        except:
            message = "\t>> Language ID `%s` cannot be understood by Tesseract; fix it and retry\n" % bibRecDict["langid"]
            message += "\t>> For now, trying `%s`..." % fallBackLanguage
            language = fallBackLanguage
    else:
        message = "\t>> No data on the language of the publication"
        message += "\t>> For now, trying `%s`..." % fallBackLanguage
        language = fallBackLanguage
    print(message)
    return(language)


###########################################################
# PROCESS ALL RECORDS: APPROACH 2 #########################
###########################################################

# Why this way? Our computers are now quite powerful; they
# often have multiple cores and we can take advantage of this;
# if we process our data in the manner coded below --- we shuffle
# our publications and process them in random order --- we can
# run multiple instances fo the same script and data will
# be produced in parallel. You can run as many instances as
# your machine allows (you need to check how many cores
# your machine has). Even running two scripts will cut
# processing time roughly in half.

def processAllRecords(bibData):
    keys = list(bibData.keys())
    random.shuffle(keys)

    for key in keys:
        bibRecord = bibData[key]

        functions.processBibRecord(memexPath, bibRecord)

        language = identifyLanguage(bibRecord, "eng")
        ocrPublication(memexPath, bibRecord["rCite"], language)


bibData = functions.loadBib(settings["bib_all"])
processAllRecords(bibData)
```

## Tesseract Language Codes

As you can see below there are quite a few languages. You may need only those languages that you can read. The question is how to determine the language of a publication? Usually, when you add data to Zotero, there will be a field that states the language. There are two problems, however. First, the language field is often empty. Second, the same language can be indicated in a variety of ways (for example, `en`, `eng`, `English`, `Englisch`, etc.). Tesseract uses triliteral [ISO 639-2](https://www.loc.gov/standards/iso639-2/php/code_list.php) standard for language codes (`eng` for English). How to resolve this issue? There are three things to consider:

1) you can curate you Zotero data, manually fixing language codes;
2) you can create a dictionary (yaml file) that will have most common spellings of languages that occure in your data (you can build a frequency list in order to identify them; then convert that data into YAML format with regular expressions; then manually fix all irregularities);
3) you can write a function that checks if the language code in your data corresponds to any code in Tesseract: if true, the function will return it; if not, it defaults to the most common language in your library.

In general, the most optimal approach will have elements of all three.

``` shell
user % tesseract --list-langs

List of available languages (162):
afr
amh
ara
asm
aze
aze_cyrl
bel
ben
bod
bos
bre
bul
cat
ceb
ces
chi_sim
chi_sim_vert
chi_tra
chi_tra_vert
chr
cos
cym
dan
deu
div
dzo
ell
eng
enm
epo
est
eus
fao
fas
fil
fin
fra
frk
frm
fry
gla
gle
glg
grc
guj
hat
heb
hin
hrv
hun
hye
iku
ind
isl
ita
ita_old
jav
jpn
jpn_vert
kan
kat
kat_old
kaz
khm
kir
kmr
kor
kor_vert
lao
lat
lav
lit
ltz
mal
mar
mkd
mlt
mon
mri
msa
mya
nep
nld
nor
oci
ori
osd
pan
pol
por
pus
que
ron
rus
san
script/Arabic
script/Armenian
script/Bengali
script/Canadian_Aboriginal
script/Cherokee
script/Cyrillic
script/Devanagari
script/Ethiopic
script/Fraktur
script/Georgian
script/Greek
script/Gujarati
script/Gurmukhi
script/HanS
script/HanS_vert
script/HanT
script/HanT_vert
script/Hangul
script/Hangul_vert
script/Hebrew
script/Japanese
script/Japanese_vert
script/Kannada
script/Khmer
script/Lao
script/Latin
script/Malayalam
script/Myanmar
script/Oriya
script/Sinhala
script/Syriac
script/Tamil
script/Telugu
script/Thaana
script/Thai
script/Tibetan
script/Vietnamese
sin
slk
slv
snd
snum
spa
spa_old
sqi
srp
srp_latn
sun
swa
swe
syr
tam
tat
tel
tgk
tha
tir
ton
tur
uig
ukr
urd
uzb
uzb_cyrl
vie
yid
yor

```

<!--chapter:end:02-Lesson08.Rmd-->

# Lesson 09

## Building Memex - Step 3

Now, as we have images of pages extracted and OCRed, we may want to create some intefrace that would allow us to move around our memex and do rather traditional reading of publications in our library. Perhaps the easiest way for building a simple interface would be to use HTML, which allows to easily link everything together and can be easily used in any browser.

What do we need to do? We want a function that creates interconnected HTML pages for each publication---with all the relevant bibliographical information included into each and every page. The working function is actually given below: your main task here will be to figure out how it works; and to write a bit of code that processes every publication in your memex (essentially, you will need to slightly modify a "processAll" function from previous assignments).

Your other, more difficult task will be to generate a starting page and a page that lists all publications in your memex and allows one to navigate from there to any of listed publications. Details are given below. Keep reading :)

(You do not really need to know any HTML to complete this assignment, but, just in case you are curious, you can find links to relevant materials in *Additional Materials* below).

## Code snippets & functions

### The main function: Publication Interface

Here is one, big, scary function that merges together page images, OCR-ed text, and bibliographical information into a simple HTML-based interface. While it looks long and scary, it is actually quite simple --- most of the code is simple fine/replace operations that are populating a template with relevant information.

In a nutshell, the function:

- takes a citation key and the path to a relevant `.bib` file as its arguments
- loads:
  - OCRed data
  - bibliographical information
  - an HTML template
- loops through the pages of OCRed data and inserts relevant information into relevant slots
- saves each page into its place

```python
# generate interface for the publication
def generatePublicationInterface(citeKey, pathToBibFile):
    print("="*80)
    print(citeKey)

    jsonFile = pathToBibFile.replace(".bib", ".json")
    with open(jsonFile) as jsonData:
        ocred = json.load(jsonData)
        pNums = ocred.keys()

        pageDic = functions.generatePageLinks(pNums)

        # load page template
        with open(settings["template_page"], "r", encoding="utf8") as ft:
            template = ft.read()

        # load individual bib record
        bibFile = pathToBibFile
        bibDic = functions.loadBib(bibFile)
        bibForHTML = functions.prettifyBib(bibDic[citeKey]["complete"])

        orderedPages = list(pageDic.keys())

        for o in range(0, len(orderedPages)):
            #print(o)
            k = orderedPages[o]
            v = pageDic[orderedPages[o]]

            pageTemp = template
            pageTemp = pageTemp.replace("@PAGELINKS@", v)
            pageTemp = pageTemp.replace("@PATHTOFILE@", "")
            pageTemp = pageTemp.replace("@CITATIONKEY@", citeKey)

            if k != "DETAILS":
                mainElement = '<img src="@PAGEFILE@" width="100%" alt="">'.replace("@PAGEFILE@", "%s.png" % k)
                pageTemp = pageTemp.replace("@MAINELEMENT@", mainElement)
                pageTemp = pageTemp.replace("@OCREDCONTENT@", ocred[k].replace("\n", "<br>"))
            else:
                mainElement = bibForHTML.replace("\n", "<br> ")
                mainElement = '<div class="bib">%s</div>' % mainElement
                mainElement += '\n<img src="wordcloud.jpg" width="100%" alt="wordcloud">'
                pageTemp = pageTemp.replace("@MAINELEMENT@", mainElement)
                pageTemp = pageTemp.replace("@OCREDCONTENT@", "")

            # @NEXTPAGEHTML@ and @PREVIOUSPAGEHTML@
            if k == "DETAILS":
                nextPage = "0001.html"
                prevPage = ""
            elif k == "0001":
                nextPage = "0002.html"
                prevPage = "DETAILS.html"
            elif o == len(orderedPages)-1:
                nextPage = ""
                prevPage = orderedPages[o-1] + ".html"
            else:
                nextPage = orderedPages[o+1] + ".html"
                prevPage = orderedPages[o-1] + ".html"

            pageTemp = pageTemp.replace("@NEXTPAGEHTML@", nextPage)
            pageTemp = pageTemp.replace("@PREVIOUSPAGEHTML@", prevPage)

            pagePath = os.path.join(pathToBibFile.replace(citeKey+".bib", ""), "pages", "%s.html" % k)
            with open(pagePath, "w", encoding="utf8") as f9:
                f9.write(pageTemp)
```

### Additional functions

As we discussed before, sometimes it makes sense to move some of the code into separate functions. These are such functions that I removed from the *main* code so that it is more readable (although a few more functions can be also extracted from the main code).

*TOC Links*: The following function generate links to all pages in a given publication so that it is easier to navigate and move around; each page in a publication recieves its own list of links where the current page is colored with red.

``` python
def generatePageLinks(pNumList):
    listMod = ["DETAILS"]
    listMod.extend(pNumList)

    toc = []
    for l in listMod:
        toc.append('<a href="%s.html">%s</a>' % (l, l))
    toc = " ".join(toc)

    pageDic = {}
    for l in listMod:
        pageDic[l] = toc.replace('>%s<' % l, ' style="color: red;">%s<' % l)

    return(pageDic)
```

*HTML-Friendly BIB:* The following function simply makes a bib record look more readable, more HTML friendly. It removes excessive curly brackets; and some field that are not needed for display (you can modify it to adjust the way your records look).

```python
def prettifyBib(bibText):
    bibText = bibText.replace("{{", "").replace("}}", "")
    bibText = re.sub(r"\n\s+file = [^\n]+", "", bibText)
    bibText = re.sub(r"\n\s+abstract = [^\n]+", "", bibText)
    return(bibText)
  ```

### Extra function

It always makes sense to write functions that do operations that we need frequently. For example, the fucntion below generates a dictionary of citation keys and paths to specific types of files. For instance, we can quickly create a dictionary of `.bib` files and then use this dictionary to process all `.bib` files.

```python
def dicOfRelevantFiles(pathToMemex, extension):
    dic = {}
    for subdir, dirs, files in os.walk(pathToMemex):
        for file in files:
            # process publication tf data
            if file.endswith(extension):
                key = file.replace(extension, "")
                value = os.path.join(subdir, file)
                dic[key] = value
    return(dic)
```

*Additional value:* in the first two steps we used bibTex bibliography exported from Zotero to process our publications and OCR relevant PDF files. Technically, right after we generated the structure (created all relevant folders, copied PDFs, and generated individual `.bib` files), we no longer need to rely on our big bibliography file, but rather use the very structure of our memex. The advantage of such an approach is that we can merge multiple memexes and then run only operations that are necessary to connect new publications with old ones.

### Code Reuse

We have already discussed functions as a useful mechanism for code reuse; additionally, use can place often-used functions into a separate file (let's call it `functions.py`) and import it with `import functions` in our other script. (**IMPORTANT:** `functions.py` **must** be in the same folder as you other script that imports it).


### Missing functions: The Index Page and the Contents Page

Since we are essentially creating a *local website*, we need a starting page---*index.html*---which will serve as the main entry point for our memex. Additionally, we would need a page where all the publications of our memex will be listed so that we could go and read whatever we are interested in. Your main task is to write such a function. In general, the main function avobe is your guide, although these two pages are significantly simpler. For the index page you need to "join" the *index page template* with the *index page content* (both are prepared and available in `./_misc/` folder). The content page is a bit trickier --- you need to generate a list of publications---with links---and "join" it with the *index page template*.

Your content page may look something like this:

![](./images/sample_contents.png)

For information for each publication, you should use (at least): author or editor, year, title. You can use the following snippet as an example, but you are more than welcome to come us with your own representation.

```html
<ul>
<li><a href="a/ab/AbbottThat2019/pages/DETAILS.html">[AbbottThat2019]</a> Abbott, Nicholas (2019) - <i>“In That One the Ālif Is Missing”: Eunuchs and the Politics of Masculinity in Early Colonial North India</i></li>
<li><a href="a/ab/AbdullahMandaean2018/pages/DETAILS.html">[AbdullahMandaean2018]</a> Abdullah, Thabit A. J. (2018) - <i>The Mandaean Community and Ottoman-British Rivalry in Late 19th-Century Iraq: The Curious Case of Shaykh Ṣaḥan</i></li>
<li><a href="a/ab/AbdurasulovMaking2020/pages/DETAILS.html">[AbdurasulovMaking2020]</a> Abdurasulov, Ulfatbek (2020) - <i>Making Sense of Central Asia in Pre-Petrine Russia</i></li>
<li><a href="a/ac/Ackerman-LiebermanContractual2011/pages/DETAILS.html">[Ackerman-LiebermanContractual2011]</a> Ackerman-Lieberman, Phillip I. (2011) - <i>Contractual Partnerships in the Geniza and the Relationship between Islamic Law and Practice</i></li>
</ul>
```

To make it a little bit easier, the actual template for each publication may look as follows:

```html
<li><a href="@PATHTOPUBL@/pages/DETAILS.html">[@CITEKEY@]</a> @AUTHOR@ (@DATE@) - <i>@TITLE@</i></li>

```

## Homework{#HWL09}

- the task is described above.
- additionally, take the solution scripts from the previous lesson and annotate every line of code; submit your annotations together with the main assignment; if you have any suggestions for improvements, please share them (this will count as extra points :).
- upload your results to your memex github repository
  - place annotated scripts into `_misc` subfolder

## Homework Solution{#HWL09S}

Check the script `3_Interface1.py` in the `memex_sandbox` repository (<https://github.com/maximromanov/memex_sandbox>).

## Additional materials: HTML, CSS; TFIDF

To get a better idea of HTML and CSS, take a look at the following tutorials at *Programming Historian*:  [@TurkelUnderstanding2012a] and [@TurkelCreating2012]; see also [starting with HTML + CSS](https://www.w3.org/Style/Examples/011/firstcss.en.html).

Next time we will discuss *keyword extraction*. It will be beneficial for all if you do some readings on TF-IDF. Please, read Chapter 1 in Ramsay's *Reading Machines* [@RamsayReading2011], which is digitally available via [Uni Wien Library](https://usearch.univie.ac.at/primo-explore/fulldisplay?docid=TN_cdi_proquest_miscellaneous_2131816455&context=PC&vid=UWI&lang=en_US&search_scope=UWI_UBBestand&adaptor=primo_central_multiple_fe&tab=default_tab&query=any,contains,ramsay%20reading%20machines&mode=basic); and take a quick look at Lavin's *Analyzing Documents with TF-IDF* on *Programming Historian* [@LavinAnalyzing2019].

<!--chapter:end:02-Lesson09.Rmd-->

# Lesson 10

## Building Memex - Step 4

One of the most interesting aspects behind the idea of memex is connections among its texts: the network of algorithmically generated connections that would suggest one what else s/he should read or might be interested to read, based on what s/he is reading right now. There are plenty of different methods that are actively used in digital humanities to identify different kinds of similarities among texts in a *corpus*, a given body of texts. These methods may be used to identify duplicate or near-duplicate texts (*document distance*), to identify texts that cover similar themes and topics [*topic modeling*: for example, @BleiProbabilistic2012], to identify texts that belong to the same forms and genres, to identify texts that might have been written by the same individual [*stylometric analysis*: for example, @EderStylometry2016], etc. What is common among all these methods is that they mathematically manipulate numeric abstractions of texts: first, texts are reduced to frequency lists of different kinds; then they are mathematically compared with each other in order to identify different types of similarities. What is different among all these methods is how the numeric abstraction is generated, what features are selected for comparison, and what kind of formulas/algorithms are used to generate similarity measures.

For our purpose, which would be finding thematically similar texts and sections of texts, we would benefit the most from using a combination of the *tf-idf* (*term-frequency - inverse document frequency*) method and a *document distance* algorithm. Chapter 1 of Ramsay's *Reading Machines* [@RamsayReading2011], which is digitally available via [Uni Wien Library](https://usearch.univie.ac.at/primo-explore/fulldisplay?docid=TN_cdi_proquest_miscellaneous_2131816455&context=PC&vid=UWI&lang=en_US&search_scope=UWI_UBBestand&adaptor=primo_central_multiple_fe&tab=default_tab&query=any,contains,ramsay%20reading%20machines&mode=basic), gives a nice general overview of the approach. Matthew Lavin's Lavin's *Analyzing Documents with TF-IDF* on *Programming Historian* [@LavinAnalyzing2019] provides a detailed technical description of the entire approach. Please, make sure to read these.

## Some explanations: TF-IDF, Document Distance

To quickly sum up, the steps will be as follows:

1. [**TF-IDF**]
  - We calculate *term frequencies* of all terms in all documents (i.e., our OCR results). Term frequency is relative frequency, i.e. absolute frequency of a term divided by the overall number of terms in a document. 
  - We then calculate *inverse document frequency*, computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.
  - We then calculate *tf-idf* as a product of *tf* and *idf* values (sometimes additional steps are included to normalize results).
2. [**Filtering**]
  - before document distance is calculated, some filtering might be desirable. Filtering is useful and sometimes necessary for a number of reasons. For example, if we are looking for similarities between texts, we may want to exclude terms that occur only in a very small number of texts (additionally, considering that we work with imperfect OCR results, most terms with *df*=1 are likely to be incorrectly recognized words). Additionally, it also makes sense to filter out terms that occur in too many documents (those terms that occur in *all* documents would carry no relevant value). An important practical benefit of such filtering is that we will reduce the amount of data that will be used for calculations, and therefore less resources will be required for calculations and they will be done faster.
  - Additionally, filtering can be done by using language-specific lists of stopwords (for example, there is no need to include such English words as "a", "the", "and", "or" into calculations). You can find lists of stopwords for specific languages [online](https://github.com/Alir3z4/stop-words/tree/bd8cc1434faeb3449735ed570a4a392ab5d35291). Many relevant python packages will have lists of stopwords already included. Curating your own lists of stopwords is usually the best practice.
  - Some other procedures can be used to improve results. For example stemming or, better, lemmatization will help to reduce morphological complexity of texts, which is particularly important for highly inflective and agglutinative languages. (however, stemming will also render results less readable). There are two caveats to consider before using these methods: first of all, both methods are language specific --- in other words, for example, if you accidentaly apply English lemmatization to German (or the other way around), results will most likely be horrible; second, OCR results are not perfect: again, OCRing German text using `eng` as a language parameter in Tesseract may yield decent results, but all characters with umlauts are likely to be replaced with corresponding English characters; the results of applying lemmatization of stemming to such text will generate more errors. That said, one should still experiment with these methods. 
2. [**Document Distance Calculations**]
  - last step: we apply some distance measure calcucations to each and every pair of documents/publications (which are now represented as numeric vectors).

*Practical Example*. Let's take a look at a very simple example of comparing the following simple sentences using our approach:

1. President Obama returned from his trip to Europe.
2. President Bush returned from his trip to China.
3. The president returned from his trip.
4. And now about something completely different.

As we discussed, we need to convert each text (sentence) into a frequency list. For calculations each text (sentence) is *vectorized*, i.e. frequencies of each word of each sentence are mapped on the entire vocabulary of the corpus (i.e., all analyzed texts). If any specific word is missing in a particilar text (sentence), its frequency is 0. This is shown in Table \@ref(tab:vectorizedTexts) below.

Table: (\#tab:vectorizedTexts) Vectorized texts

|                  |  1|  2|  3|  4|
|:-----------------|--:|--:|--:|--:|
|about             |  0|  0|  0|  1|
|and               |  0|  0|  0|  1|
|bush              |  0|  1|  0|  0|
|china             |  0|  1|  0|  0|
|completely        |  0|  0|  0|  1|
|different         |  0|  0|  0|  1|
|europe            |  1|  0|  0|  0|
|from              |  1|  1|  0|  0|
|his               |  1|  1|  1|  0|
|now               |  0|  0|  0|  1|
|obama             |  1|  0|  0|  0|
|president         |  1|  1|  1|  0|
|returned          |  1|  1|  1|  0|
|something         |  0|  0|  0|  1|
|the               |  0|  0|  1|  0|
|to                |  1|  1|  0|  0|
|trip              |  1|  1|  1|  0|

How would we then caclulate distances? The simplest distance to calculate will be *Manhattan* (or city-block) distance. Essentially, for each word/term, we need to substract the frequency in text 1 from the frequency in text 2.

|                  |  1|  2|Manhattan Distance 1-2|
|:-----------------|--:|--:|--:|
|about             |  0|  0|  0|
|and               |  0|  0|  0|
|bush              |  0|  1|\|-1\||
|china             |  0|  1|\|-1\||
|completely        |  0|  0|  0|
|different         |  0|  0|  0|
|europe            |  1|  0|  1|
|from              |  1|  1|  0|
|his               |  1|  1|  0|
|now               |  0|  0|  0|
|obama             |  1|  0|  1|
|president         |  1|  1|  0|
|returned          |  1|  1|  0|
|something         |  0|  0|  0|
|the               |  0|  0|  0|
|to                |  1|  1|  0|
|trip              |  1|  1|  0|

Then we simply add up absolute values (i.e., treating negative values in the last column as positive) --- and that is the distance value: Manhattan distance between sentence 1 and sentence 2 is 4. The symmetric matrix below shows Manhattan distances for all four sentences. These values show that sentences 1, 2, and 3 are quite similar, while sentence 4 is very different from 1, 2, and 3.

Table: (\#tab:matrix1) Symmetric Matrix with Distances

|       |  1|  2|  3|  4|
|------:|--:|--:|--:|--:|
|  **1**|  0|  4|  5| 14|
|  **2**|  4|  0|  5| 14|
|  **3**|  5|  5|  0| 11|
|  **4**| 14| 14| 11|  0|

If we apply filtering to the first three sentences, removing words with `df=1`, you can see that the first two sentences become identical (Manhattan distance = 0), and the sentence 3 is very close: all three talking about the *president returning from his trip*:

1. President ~~Obama~~ returned from his trip to ~~Europe~~.
2. President ~~Bush~~ returned from his trip to ~~China~~.
3. ~~The~~ president returned from his trip.

*Other distance measures*. There is a variety of different distances that are meant to be more efficient in particular circumstances. Most common distances are Euclidean, Manhattan, and cosine. By and large, cosine distance/similarity[^CosineMess] would be your most optimal option. Main reasons are: 1) both Euclidean and Manhattan tend to give more weight to the most frequent terms; 2) neither Euclidean nor Manhattan are normalized (in their original forms), i.e their values are between 0 and *infinity*, while cosine distance values are between 0 and 1; 3) since cosine distance focuses on an *angle* between the vectors of texts in multidimensional space, rather than on the actual distance, it yields better results for texts of varying lengths on same topics (for example, between monographs and articles). You can read more about differences between these distance measures in John Ladd's "Understanding and Using Common Similarity Measures for Text Analysis" at *Programming Historian* [@LaddUnderstanding2020].

[^CosineMess]: You can come across both *cosine distance* and *cosine similarity*, which may be a bit confusing. Both values are in the range between 0 and 1. However, in the case of *cosine distance*, the perfect match is 0, while in the case of *cosine similarity*, the perfect match is 1. Mathematically, we always calculate *cosine similarity* --- texts are identical, if the angle between their vectors is 0 (`cos(0) = 1`), but since in the idea of distance, the perfect match is 0 (for example, in the cases of Euclidean and Manhattan distances), similarity must be converted into distance: *cosine distance* = 1 - *cosine similarity*.

The image below offers a graphical representation of these three distances, where `A` and `B` are two different texts and `y` and `x` are words --- `y` occurs 5 times in `A` and 2 times in `B`; `x` occurs 2 times in `A` and 4 times in `B`.

![](./images/distance_viz.png)

Why Manhattan (city-block) distance is called that way?

![](./images/manhattan2.jpg)

## Practicalities

Only a mere few years ago we would have had to write our own code for each and every step described above. Luckily, we now python libraries that efficiently do all the heavy lifting for us. One such library is `sklearn`; another library which is very useful is `pandas` (make sure to install both!). Below you can find all relevant code snippets which you will need to put together to complete this part.

What do we need to generate? You will need to put together the following code snippets in order to produce two json files. The first one must contain top keywords for each publication. The second one must contain distances between publications. If you do everything as expected your results will look like the following:

File with *tf-idf* keywords:

```json
{
    "AdamsShepherds2006": {
        "adams": 0.1272164898033877,
        "administrative": 0.099074881442255,
        "animals": 0.07099486255345346,
        "bala": 0.08924612054231096,
        "barley": 0.06046074109305735,
        "corvée": 0.09158504454505918,
        "cuneiform": 0.09386636316636131,
        "dagan": 0.061182271700903075,
        "dec": 0.09320481910701102,
        "dynasty": 0.09595469820881344,
        "fattening": 0.052203662027505905,
        "flocks": 0.05101452549985564,
        "gangs": 0.057181428331652115,
        "herds": 0.09616271243655308,
        "husbandry": 0.0958209157684203,
        "iii": 0.13276591518791975,
        "labor": 0.12346932617991906,
        "lagash": 0.14169565407465887,
        "mcc": 0.13380706013477167,
        "pasturage": 0.0587244796687148,
        "population": 0.05990187305273483,
        "presently": 0.05103796476580478,
        "prosopographic": 0.052203662027505905,
        "province": 0.05866419351034984,
        "records": 0.10031873550890447,
        "robert": 0.07360610776912387,
        "scribal": 0.07822478752418302,
        "sheep": 0.15851065662360858,
        "shepherd": 0.06216523460065599,
        "shepherds": 0.33326918149698254,
        "steinkeller": 0.17951743245184246,
        "tue": 0.08198952007025857,
        "umma": 0.43749736307561765,
        "ur": 0.37598501212878616,
        "wool": 0.11190732190242117
    },
    "GarfinkleShepherds2004": {
        "babylonian": 0.07650357591327776,
        "barley": 0.09592656463776685,
        "consumptive": 0.05337901001432162,
        "credit": 0.08335590154194376,
        "creditor": 0.13067922107308225,
        "creditors": 0.07268117801701256,
        "customary": 0.10111479309463182,
        "debtor": 0.11012714465515581,
        "duration": 0.056149153215818835,
        "garfinkle": 0.12961758742186816,
        "gin": 0.08170723635705109,
        "iii": 0.1513526344445101,
        "institutional": 0.07383767826894902,
        "labor": 0.08619402867315176,
        "lending": 0.10344979020528536,
        "loan": 0.356626414161827,
        "loans": 0.5995451434314758,
        "mesopotamia": 0.05251561096144549,
        "nippur": 0.06739596933652772,
        "phrase": 0.05161099475107544,
        "ra": 0.09626067026962869,
        "rate": 0.06123888031779106,
        "repaid": 0.06660264626574453,
        "repayment": 0.1252001522028668,
        "shepherds": 0.06507845149950617,
        "si": 0.06021635652206201,
        "silver": 0.08912141278136604,
        "steinkeller": 0.0744916541514434,
        "steven": 0.05335019010356139,
        "sé": 0.07000447780472778,
        "ur": 0.42847893151061767,
        "witnesses": 0.06663117140384797
    },
}
```

File with distances (more correctly: cosine similarities):

```json
{
    "AbdullahMandaean2018": {
        "AlonSheikh2016": 0.2915990327301636,
        "MinawiRhetoric2015": 0.277281021038338,
        "PetriatCaravan2019": 0.27332888217367635,
        "RiedlerCommunal2018": 0.25970501516946726,
        "SaracogluReview2013": 0.2882066212119597
    },
    "AdamsShepherds2006": {
        "GarfinkleReview2005": 0.3542751363967858,
        "GarfinkleShepherds2004": 0.30286604455405086,
        "ReidRunaways2015": 0.2812514791059143,
        "WidellReflections2005": 0.27154646436723745,
        "ZettlerReconstructing2003": 0.32977887642906156
    }
}
```

*Note*: in both files higher values mean more relevance and importance; in the second example (`AdamsShepherds2006`) all algorithmically identified articles are dealing with ancient Mesopotamian history, and even more specifically, with the Third Dynasty of Ur [See, @AdamsShepherds2006; @GarfinkleReview2005; @GarfinkleShepherds2004; @ReidRunaways2015; @WidellReflections2005; @ZettlerReconstructing2003]. Keep in mind that *tf-idf* keywords are heavily filtered in the example.

## Code Snippets{#CSL10}

The following code snippets are fully working and functional. All you need to do is to put them together properly.

### Aggregating publications into a corpus

The code snippet below processes all `.json` files from our memex (remember that we saved our OCR results using this extension; it might be better to use some unique extension though, something like `.OCRED`, so that we clearly keep different types of files apart). The code below 1) generates a dictionary with citekeys as keys and paths to `.json` files as values; 2) it then loops through citekeys and, 3) with each iteration of the loop, it loads each OCRed results and updates two lists: one with citekeys and another with texts. The result is all our data sorted into two lists, where items with the same index position are the citation key and the text of the same publication. We will use these lists to generate a table, similar to Table \@ref(tab:vectorizedTexts) above. Look carefully at the code. What else does this code do?

``` python
ocrFiles = functions.dicOfRelevantFiles(pathToMemex, ".json")
citeKeys = list(ocrFiles.keys())

docList   = []
docIdList = []

for citeKey in citeKeys:
    docData = json.load(open(ocrFiles[citeKey]))
    # IF YOU ARE ON WINDOWS, THE LINE SHOULD BE:
    # docData = json.load(open(ocrFiles[citeKey], "r", encoding="utf8"))
    
    docId = citeKey
    doc   = " ".join(docData.values())

    doc   = re.sub(r'(\w)-\n(\w)', r'\1\2', doc)
    doc   = re.sub('\W+', ' ', doc)
    doc   = re.sub('\d+', ' ', doc)
    doc   = re.sub(' +', ' ', doc)

    docList.append(doc)
    docIdList.append(docId)
```


### Getting *tf-idf* Values and Distance Matrix with `sklearn`

Now that we have our texts prepared, we can use the `sklearn` library to do all the tricky transformations (for detailed documentation on this library see [official user guide](https://scikit-learn.org/stable/user_guide.html)); we will also need library `pandas`. We need to make sure that both librares are installed and proper modules are loaded in the following manner:

```python
import pandas as pd
from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer)
from sklearn.metrics.pairwise import cosine_similarity
```

The `sklearn` library condenses rather elaborate code into just a few lines. The following lines of code convert our data into a format similar to what you can see in Table \@ref(tab:vectorizedTexts) above. The only difference is that instead of frequencies we will have *tf-idf* values --- these results are saved into the variable `vectorized` (as [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix)). The last line of code generates cosine distances --- these results are saved into the variable `cosineMatrix` (a symmetric matrix, similar to what you can see in Table \@ref(tab:matrix1) above) 

```python
vectorizer = CountVectorizer(ngram_range=(1,1), min_df=5, max_df=0.5)
countVectorized = vectorizer.fit_transform(docList)
tfidfTransformer = TfidfTransformer(smooth_idf=True, use_idf=True)
vectorized = tfidfTransformer.fit_transform(countVectorized) # https://en.wikipedia.org/wiki/Sparse_matrix
cosineMatrix = cosine_similarity(vectorized)
```

Perhaps the most important line to understand in the snippet above is the very first one where we create our own *vectorizer*: here we can adjust parameters to get different results. (More on parameters [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)).

- `ngram_range` can be defined as follows (To remind you of what ngram is, see Figure \@ref(fig:ngram)):
  - `(1, 1)` only unigrams (single tokens) will be considered;
  - `(1, 2)` means that both unigrams and bigrams will be considered;
  - `(2, 2)` means only bigrams.

![(\#fig:ngram) What is ngram? [Source: StackOverflow](https://stackoverflow.com/questions/18193253/)](./images/ngram.png)

- `min_df` and `max_df` are parameters to filter out terms with *document frequency* below (`max_df`) or above (`min_df`) certain threshold. (Using *float* between 0 and 1 will filter out specific percentage.)
  - `min_df` is defined as 5, meaning that all words that occur in less than 5 documents will be excluded.
  - `max_df` is defined as 0.5, meaning that words that occur in more than half of all documents will be excluded as well.
  - **Note:** It makes sense to experiment with these parameters (defining them in the `settings.yml` will make it easier to experiment).

### Converting Results

`sklearn` generates needed results with just a few lines of code, which runs very fast (even with thousands documents). In order to make calculations fast, `sklearn` (as well as other similar libraries) use matrices (both [symmetric and sparse](https://en.wikipedia.org/wiki/Sparse_matrix)), which are not exactly human readable. For example, my memex includes 1,140 documents, so the size of the distance matrix will be 1,140 x 1,140. The size of the *tf-idf* matrix will be 1,140 x 381,742! If we apply filtering (as in the code above), the *tf-idf* matrix shrinks quite significantly --- down to 1,140 x 56,073 --- but this is still not exactly usable. Most generated results are not really interesting.

Matrices and dataframes (or tables) are rectangular and, for this reason, are quit difficult to filter. So, we need to convert this data in a more manageable format, filter out values outside of a specific range, and then save into some easy-to-use format that we can incorporate into our memex. The following lines of code show how to do the conversion into a dictionary. Your task will be to write code that loops through the dictionary and filters out irrelevant results.

With `sklearn` we have created two matrices: the first one --- `vectorized` --- is a *sparse* matrix of *tf-idf* values; and the second one --- `cosineMatrix` --- is a *symmetric* matrix of cosine distances. These two types of matrices have different format and have to be processed slightly differently. (You can think of a *sparse* matrix as a sort-of compressed matrix, which stores only non-zero values; again, check [this description](https://en.wikipedia.org/wiki/Sparse_matrix)).

**Converting a sparse matrix into a dataframe, and then into a dictionary** (the optional line prints out the shape of your dataframe, i.e. the number of columns and rows, which will give you a good idea about the size of your data).

``` python
tfidfTable = pd.DataFrame(vectorized.toarray(), index=docIdList, columns=vectorizer.get_feature_names())
print("tfidfTable Shape: ", tfidfTable.shape) # optional
tfidfTable = tfidfTable.transpose()
tfidfTableDic = tfidfTable.to_dict()
```

**Converting a symmetric matrix into a dataframe, and then into a dictionary** (the optional line prints out the shape of your dataframe, i.e. the number of columns and rows, which will give you a good idea about the size of your data).

``` python
cosineTable = pd.DataFrame(cosineMatrix)
print("cosineTable Shape: ", cosineTable.shape) # optional
cosineTable.columns = docIdList
cosineTable.index = docIdList
cosineTableDic = cosineTable.to_dict()
```

Now you have dictionaries of both datasets! You already know how to save dictionaries into JSON files, but you need to filter these dictionaries first. **_Task_: write the code that filters these dictionaries**. Both dictionaries have the same structure, so you should be able to use the same function on both of them. I would recommend that with *tf-idf* terms you keep only words with value at least 0.05. For distance measures --- include items with value at least 0.25 (keep in mind that this data will also include matches of each publications against itself -- with value ~1 -- so you would want to filter out such matches). In both cases you may have to experiment with the parameters to get usable results.

**Important:** do not use `.json` extension, because it will conflict with other parts of our code, as we assume that all files with extension `.json` store OCR results.

<!--
### Processing and Saving Results

The following function can be helpful to filter out irrelevant matches. But keep in mind that this function will work only with a dictionary in a specific format --- like the ones shown in the examples above! You can always adjust this function, of course.

```python
def filterTfidfDictionary(dictionary, threshold, lessOrMore):
    dictionaryFilt = {}

    for item1, citeKeyDist in dictionary.items():
        dictionaryFilt[item1] = {}
        for item2, value in citeKeyDist.items():
            if lessOrMore == "less":
                if value <= threshold:
                    if item1 != item2:
                        dictionaryFilt[item1][item2] = value
            elif lessOrMore == "more":
                if value >= threshold:
                    if item1 != item2:
                        dictionaryFilt[item1][item2] = value
            else:
                sys.exit("`lessOrMore` parameter must be `less` or `more`")

        if dictionaryFilt[item1] == {}:
            dictionaryFilt.pop(item1)
    
    return(dictionaryFilt)
```
-->

## Homework{#HWL10}

- the main task is to generate two json files with the following:
  - the first one must include main keywords for each publications (identified with the *tf-idf* approach).
  - the second one must include distances between all publications in your memex (you can/should filter out irrelevant matches --- this will make the file with results smaller and more manageable).
- additionally, take the solution scripts from the previous lesson and annotate every line of code; submit your annotations together with the main assignment; if you have any suggestions for improvements, please share them (this will count as extra points :).
- upload your results to your memex github repository
  - place annotated scripts into `_misc` subfolder


## Homework Solution{#HWL10S}

*the working scripts have been added to the main repository on github*


<!--chapter:end:02-Lesson10.Rmd-->

# Lesson 11

## Building Memex - Step 5

- Searching our publications
- Extra: visualizong *tf-idf* terms with wordclouds
- Extra+: a function for loading stopwords

## Running searches

As we discussed today, we are likely to lose some information because of imperfect OCR results, although it is hard to tell to what extent that affects connections that we generate using *tf-idf* values (a way to test this would be to compare connections generated for text files of high quality and artificially generated OCR results for these very text files). In any case, connections that we generated are not a substitute for searches, rather they complement each other increasing the overall value of our memex.

What would we want? The general idea is that we want to search all our OCRed publications, present results in a convenient manner that would allow us to go directly to specific publications. In practical terms that would mean that we want to write a function that:

1. takes a search string as one of its arguments (other arguments are up to you); we can and should use regular expressions here;
1. loads data on all files with OCR results;
1. loops through these files (since we saved OCR results as dictionaries of page numbers and page contents, we can loop through all the pages and check if they contain what we are looking for);
1. collects pages with matches into a dictionary of search results;
  - we can use the same dictionary-of-dictionaries structure that we have already used before;
  - the dictionary will have citeKeys as its `keys`, and the dictionary of matches for its `values`;
  - the dictionary of matches will have page numbers as its `keys` and search matches as its `values`; for search matches we may also want additional information, such as number of matches on a page; a link to the original (html) page within our memex (although this can also be generated later, when we update the interface).
  - for the main dictionary with the results we may also want to save additional information for the entire search, like the date and time (timestamp) of our search and the exact string we have been using for searching.

Overall the results should look like the example below (I have manually shortened `result` values for better view):

``` json
{
    "000000009::::GrazierHollyweird2015": {
        "0253": {
            "matches": 2,
            "pathToPage": "g/gr/GrazierHollyweird2015/pages/0253.html",
            "result": "...being to describe the collection of all potential
            universes—the most com-<br>mon of which is <span class='searchResult'>
            multiverse</span>.<br><br>There are many of us thinking of one version
            of parallel universe theory or an-<br>other. If it’s all a lot of
            nonsense, then it’s a lot of ... If there are parallel universes,
            would we ever be able to detect them? Travel<br>to them? For some
            <span class='searchResult'>multiverse</span> models, the surprising answer is..."
        },
        "0260": {
            "matches": 4,
            "pathToPage": "g/gr/GrazierHollyweird2015/pages/0260.html",
            "result": "...The quantum <span class='searchResult'>multiverse</span>
            is an outcome of the Many Worlds interpretation<br>of quantum mechanics,
            proposed by Hugh Everett in 1957. The many-worlds<br>hypothesis is an
            attempt to solve a very deep problem in quantum mechanics,<br>the
            Schrédinger’s Cat problem. Recall that for the Schrédinger’s Cat gedanken<br>
            experiment, imagine a cat sealed in a box, with a 50-50 chance of living
            or<br>dying in a given time period T. After time T elapses, what can
            we say about<br>the cat? Nothing. ...<br>"
        },
    },
    "000000009::::GribbinErwin2012": {
        "0294": {
            "matches": 2,
            "pathToPage": "g/gr/GribbinErwin2012/pages/0294.html",
            "result": "... The <span class='searchResult'>Multiverse</span> idea says
            that there always were<br>256 universes, identical to one another up to
            the point where<br>the computation is run, and that the identical
            experimenters<br>in each of those universes each decide to carry
            out the same<br>experiment—hardly surprising, since they are identical..."
        }
    },
    "searchString": "multi\\W*verse",
    "timestamp": "2021-01-12 15:32:11"
}
```

After we have search results collected and saved, we would, of course, want to make them easily available. We would want to be able to preview those results (to see the match in context) and to go to the original page. Your page may look something like on the screenshot below, where 1) results are grouped by publications and ranked by the number of matches per publication; 2) clicking on the citeKey of a publication opens the preview with matches and you can click on a link to get to the original HTML page:


![](./images/searchResultsPreview.png)

To simplify the task of generating such an html page with results, you can use [this html code](https://github.com/maximromanov/memex_sandbox/blob/main/_misc/multiWverse.html) as your guide and template.

Remember that you can always adjust the code to fit your own goals. As we briefly discussed, you can add a function that would run some *normalization* (or, rather, *de-normalization*) of your search strings to catch words that might have been recognized with errors during the OCR process. For example, if you are searching for `Schrödinger` (and not his cat :), you need to modify your search string in such a way that we can find this word with its most common misspelling (if you looked carefully at the example above, you would have noticed that the name of this physicist was misspelled as `Schrédinger`). The following automatic modification will help to create a regular expression that will catch the most common misspelled forms (`Schrédinger, Schrodinger, Schriidinger`, etc.). You can easily write a function that will apply such modifications to all problematic letters.

``` python
import re

string = "Schrödinger"
stringModified = string.replace("ö", "\w{,2}")
```

## Wordclouds as visual summaries

Wordsclouds can be a rather nice way to graphically represent keywords of publications. They can be problematic as more often than not they are created to be a pretty picture rather than an informative visualization (we can discuss this issue later). Be that as it may, wordclouds can add a nice touch to our project. Let's create them.

The following code will generate a wordcloud if you provide the function with the path to save file and a dictionary of *tf-idf* terms:

``` python
from wordcloud import WordCloud
import matplotlib.pyplot as plt

AndrewsTree2013 = {
        "academic": 0.05813626462255791, "acyclic": 0.06250123247317078,
        "andrews": 0.12638860902474044, "artificial": 0.07179606130684399,
        "coincidental": 0.107968929904822, "collatex": 0.05992322690922588,
        "collation": 0.06303029408091644, "computational": 0.05722784996783764,
        "computing": 0.10647437138201694, "conflation": 0.06207493202469115,
        "copied": 0.11909333470623461, "deletion": 0.1139260485820703,
        "deum": 0.08733065096290674, "dsh": 0.12062758085160386,
        "empirical": 0.06256016440693514, "exemplar": 0.06413026390502427,
        "february": 0.06598308140156786, "fig": 0.06658631708313895,
        "figure": 0.057484475246214396, "finnish": 0.06718916017623287,
        "genealogical": 0.14922848147612744, "grammatical": 0.06639003369358097,
        "graph": 0.3064885690820798, "heikkila": 0.06250123247317078,
        "howe": 0.0502983742283863, "lachmannian": 0.05357248497700353,
        "legend": 0.05576734763473662, "lexical": 0.07504108167413616,
        "library": 0.052654345617120235, "linguistic": 0.08295373338961148,
        "literary": 0.05453901405048022, "macé": 0.09416507085735495,
        "manuscript": 0.056407679242811995, "medieval": 0.0722276960100237,
        "methods": 0.06617794961055612, "model": 0.06712481163842804,
        "oup": 0.11328056728336285, "phylogenetic": 0.0771729868291211,
        "quae": 0.06207493202469115, "readings": 0.14004936139939653,
        "relationships": 0.05294605101492676, "reverted": 0.08384604576952619,
        "roos": 0.057864729221843304, "root": 0.050044699046501814,
        "sermo": 0.05136276592219361, "spelling": 0.10943421974916011,
        "spencer": 0.058047262455825824, "stemma": 0.42250289612460745,
        "stemmata": 0.0813887983497219, "stemmatic": 0.09093028877718234,
        "stemmatology": 0.11984645381845176, "table": 0.0650730361130049,
        "traditions": 0.06135920913100084, "transmission": 0.07196157976331963,
        "transposition": 0.09182278547380966, "tree": 0.06790095059625004,
        "user": 0.08246083727188196, "variant": 0.2562367460915465,
        "variants": 0.1432698250058143, "variation": 0.2935924450487041,
        "vb": 0.11035543471056206, "vertices": 0.050436368099886865,
        "vienna": 0.07506704856975273, "witness": 0.11471933221048437,
        "witnesses": 0.09101116903971758
    }

savePath = "AndrewsTree2013.jpg"

def createWordCloud(savePath, tfIdfDic):
    wc = WordCloud(width=1000, height=600, background_color="white", random_state=2,
                   relative_scaling=0.5, colormap="gray") 
    wc.generate_from_frequencies(tfIdfDic)
    # plotting
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    #plt.show() # this line will show the plot
    plt.savefig(savePath, dpi=200, bbox_inches='tight')

createWordCloud(savePath, AndrewsTree2013)
```

The result will look something like this:

![](./images/AndrewsTree2013.png)

*Hint*: you can always use the file with *tf-idf* values that you generated before, although you might want to change `max_df` and `min_df` parameters for wordclouds (for example, `min_df=1` and `max_df=0.75`). 

One more thing: experiment with how your wordclouds look. You can find lot of different parameters in [the official documentation of the library](https://amueller.github.io/word_cloud/) --- you can change colors, fonts, shapes, sizes, etc.

## Stopwords for *tf-idf*

I have added a function for loading lists of stopwords. You can find it in `functions.py`.

## Homework{#HWL11}

- described above;
- additionally, take the solution scripts from the previous lesson and annotate every line of code; submit your annotations together with the main assignment; if you have any suggestions for improvements, please share them (this will count as extra points :).
- upload your results to your memex github repository
  - place annotated scripts into `_misc` subfolder


## Homework Solution{#HWL11S}

*the working scripts have been added to the main repository on github*


<!--chapter:end:02-Lesson11.Rmd-->

# Syllabus{-}

* Course: 070172-1 UE Methodological course - Introduction to DH: Tools & Techniques (2020W) `Memex Edition`
* Instructor: Dr. Maxim Romanov, [maxim.romanov@univie.ac.at](maxim.romanov@univie.ac.at)
* Language of instruction: English
* Office hours: Tu 14:00-15:00 (on Zoom; please, contact beforehand!)
* Office: Department of History,  Maria-Theresien-Straße 9, 1090 Wien, Room 1.10

## Course Details{-}

* u:find Link: <https://ufind.univie.ac.at/en/course.html?lv=070172&semester=2020W>
* Meeting time: Tu 09:00-10:30
* Meeting place: Seminarraum Geschichte 3 Hauptgebäude, 2.Stock, Stiege 9; due to COVID, all meetings will be held online

## Aims, Contents and Method of the Course {-}

Back in 1945, Vannevar Bush, a Director of the US Office of Scientific Research and Development, proposed a device, which he called memex:

> *Consider a future device ... in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory. ... The owner of the memex, let us say, is interested in the origin and properties of the bow and arrow. Specifically he is studying why the short Turkish bow was apparently superior to the English long bow in the skirmishes of the Crusades. He has dozens of possibly pertinent books and articles in his memex. First he runs through an encyclopedia, finds an interesting but sketchy article, leaves it projected. Next, in a history, he finds another pertinent item, and ties the two together. Thus he goes, building a trail of many items. Occasionally he inserts a comment of his own, either linking it into the main trail or joining it by a side trail to a particular item. When it becomes evident that the elastic properties of available materials had a great deal to do with the bow, he branches off on a side trail which takes him through textbooks on elasticity and tables of physical constants. He inserts a page of longhand analysis of his own. Thus he builds a trail of his interest through the maze of materials available to him. And his trails do not fade. Several years later, his talk with a friend turns to the queer ways in which a people resist innovations, even of vital interest. He has an example, in the fact that the outraged Europeans still failed to adopt the Turkish bow. In fact he has a trail on it. A touch brings up the code book. Tapping a few keys projects the head of the trail. A lever runs through it at will, stopping at interesting items, going off on side excursions. It is an interesting trail, pertinent to the discussion.* ... — [The Atlantic, July 1945](<https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/>); YouTube: <https://www.youtube.com/watch?v=c539cK58ees>.

The memex machine is often thought of as a precursor of the Internet. Be it as it may, the idea of a personal knowledge device is still of great relevance and of great importance to scholars and scientists whose job is to construct such trails on a regular basis. Needless to say that *historians* will benefit greatly from having such a machine at their disposal. The course will introduce you to basic, intermediate, and some advanced computational techniques, which will allow you to build and maintain your own digital memex machine.

No prior programming experience is expected (we will be learning Python). Each class session will consist in large part of practical hands-on exercises led by the instructor. Laptops are required for the course. We will accommodate whatever operating system you use (Windows, Mac, or Linux), but it must be a laptop rather than a tablet.

## Course Evaluation  {-}

Course evaluation will be a combination of in-class participation (30%), weekly homework assignments (50%), and the final project (20%).

## Class Participation {-}

Attendance is required; regular participation is the key to completing the course; all students must come with their laptops; homework assignments must be submitted on time (some can be completed later as a part of the final project, but this must be discussed with the instructor whenever the issue arises); the final project must be submitted on time.

## Homework Assignments {-}

* Homework assignments are to be submitted by the beginning of the next class;
* These must be emailed to the instructor as attachments;
* In the subject of your email, please, use the following format: `CourseID-LessonID-HW-Lastname-matriculationNumber`, for example, if I were to submit homework for the first lesson, my subject header would look like: `070112-L01-HW-Romanov-12435687`.
* DH is a collaborative field, so you are most welcome to work on your homework assignments in groups, however: you must still submit it. That is, if a groups of three works on one assignment, there must be three separate submissions emailed from each member’s email.

## Final Project {-}

The final project is your own memex machine, which can help you with your studies and your research. Your final project must include all working scripts that will allow you in the future to continuously expand your memex machine by adding new readings into the mix. You are most welcome to work on this final project in groups, but everybody is required to produce their own working machine.

## Study materials {-}

**MAIN TEXTBOOK**

- **Zelle, John M. *Python Programming: An Introduction to Computer Science*.** Third edition. Portland, Oregon: Franklin, Beedle & Associates Inc, 2017. (*access via Moodle*); [@ZellePython2017]
	- We will focus primarily on learning how to work with `python`, which is one of the most popular programming languages used in digital humanities. We will use several resources and the emphasis will be on you studying on your own: partially, this is because of time constraints, but more importantly, you will need to acquire a skill of learning on your own. No worries, I will provide necessary help whenever needed.
	- This textbook will be our main resource. It is well written and will help you to wrap your heads around important computer science concepts; this reading is crucial and without it many interactive tutorials out there will not be particularly helpful. Each chapter has assignments and self-test multiple choice sections;
	- Supplementary materials are available at [the publisher's website](<https://fbeedle.com/our-books/23-python-programming-an-introduction-to-computer-science-3rd-ed-9781590282755.html>), where you can download example code and end-of-chapter solutions; additionaly, you can find videos with complimentary instructions
	- **Additional:**
		- Paul Vierthaler's “Hacking the Humanities Tutorials” (Python+): <https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17>
		- <https://www.codecademy.com/learn/learn-python> :: you can use this free interactive Python course; it, however, uses Python 2.x, while the main textbook focuses on Python 3.x; the course is still a good supplementary practice.  

**ADDITIONAL MATERIALS**

* <https://www.codecademy.com>
	* `Codecademy` has a series of free course that you are encouraged to use for specific skills and technologies:
		* <https://www.codecademy.com/learn/learn-how-to-code>
		* <https://www.codecademy.com/learn/learn-python>
		* <https://www.codecademy.com/learn/learn-html>
		* <https://www.codecademy.com/learn/introduction-to-regular-expressions> 
		* <https://www.codecademy.com/learn/learn-css>

* <https://programminghistorian.org/lessons/>
	* “Programming Historian” offers a number of tutorials for aspiring digital humanists. These will be assigned to you as reference materials. You also are encouraged to explore those tutorials that are not included into the course.

## Software, Tools, & Technologies {-}

The following is the list of software, applications and packages that we will be using in the course. Make sure to have them installed by the class when we are supposed to use them.

* Zotero, <https://www.zotero.org/>
* [Mac] Terminal / [Windows] Powershell (both are already on your machines)
* Python <https://www.python.org/>, install the latest 3.x version
* git and <https://github.com/>, version control system
* pandoc (<https://pandoc.org/>), markdown, bibTex (bibliographical format for LaTeX)
* Regular expressions; (Sublime Text, <https://www.sublimetext.com/> is a text editor which supports regular expressions)
* Wget (<https://www.gnu.org/software/wget/>), a free software package for retrieving files
* Understanding formats: [TEI] XML, csv/tsv, json, yml, etc.
* Creating: HTML, css, tiny snippets of Javascript

## Schedule  {-}

**Location**: Seminarraum Geschichte 3 Hauptgebäude, 2.Stock, Stiege 9; due to COVID, all meetings will be held online via video-conferencing

- **Tuesday**	06.10.	09:00 - 10:30
- **Tuesday**	13.10.	09:00 - 10:30
- **Tuesday**	20.10.	09:00 - 10:30
- **Tuesday**	27.10.	09:00 - 10:30
- **Tuesday**	03.11.	09:00 - 10:30
- **Tuesday**	10.11.	09:00 - 10:30
- **Tuesday**	17.11.	09:00 - 10:30
- **Tuesday**	24.11.	09:00 - 10:30
- **Tuesday**	01.12.	09:00 - 10:30
- **Tuesday**	15.12.	09:00 - 10:30
- **Tuesday**	12.01.	09:00 - 10:30
- **Tuesday**	19.01.	09:00 - 10:30
- **Tuesday**	26.01.	09:00 - 10:30

## Lesson Topics {-}

- **=== CORE TOOLS & METHODS ===**
- **[ #01 ]** Introduction & Roadmap; Managing Bibliography with Zotero
- **[ #02 ]** Getting to Know the Command Line; Getting Started with Python
- **[ #03 ]** Version Control and Collaboration 
- **[ #04 ]** Sustainable [Academic] Writing
- **[ #05 ]** Constructing Robust Searches / *Optional*: Basics of Webscraping
- **[ #06 ]** Understanding Structured Data and Major Formats 
- **=== BUILDING MEMEX ===**
- **[ #07 ]** Parsing and Manipulating Bibliographic Data 
- **[ #08 ]** Processing PDFs: OCR
- **[ #09 ]** View and Display: Simple HTML-based Interface 
- **[ #10 ]** Summarizing Textual Data: Keyword Extraction
- **[ #11 ]** Finding Connections: Similarity Measures
- **[ #12 ]** Processing Everything Together: Batch Processing and re-Processing
- **[ #13 ]** Improving the Overall Memex Design: What Else Can We Add?

<!--
- **=== CORE TOOLS & METHODS ===**
- **[ #01 ]** Introduction & Roadmap (Core tools, Memex and *Zettelkasten*; Tool #1: Zotero; Getting started with Python)
- **[ #02 ]** “Off with the Interface!” Getting to know the `command line`
- **[ #03 ]** Version Control and Collaboration: `git` and GitHub.com 
- **[ #04 ]** Sustainable Academic Writing: with `pandoc`, `markdown`, `BibTex` (via Zotero)
- **[ #05 ]** Constructing Robust Searches with `regular expressions` / *Optional*: `wget` and webscraping
- **[ #06 ]** Understanding Structured Data and Major Formats: XML/HTML, JSON, CSV, etc. 
- **=== BUILDING MEMEX ===**
- **[ #07 ]** Parsing `bibTeX` Data: Collecting library (via Zotero); Managing and exporting bibliographical data (`bibLaTeX`; `BetterBibTeX`); Transforming bibliographic data 
- **[ #08 ]** Processing a PDF: Saving Images and OCR Results
- **[ #09 ]** A Simple Way to View/Display Data: creating HTML template, CSS 
- **[ #10 ]** Summarizing Textual Data: extracting keywords with `tf-idf`
- **[ #11 ]** Finding Connections: applying similarity measures
- **[ #12 ]** Processing Everything Together: batch processing and re-processing
- **[ #13 ]** Improving the Overall Memex Design: What else can we add?
-->

Note: one of the classes might be canceled; this will be announced separately. Lesson materials will be appearing on the website shortly before each class. Lessons will be accessible via the *Lessons* link on the left panel.

<!--chapter:end:98-syllabus.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:99-references.Rmd-->

