--- 
title: "Digital Humanities in Middle Eastern Studies"
author: "Maxim G. Romanov"
date: "`r Sys.Date()`"
description: "This is a collection of relevant materials for the class '57-525 S: Digital Humanities in Middle East Studies --- Introduction to Algorithmic Analysis' (WS2021), offered at the University of Hamburg (Afrika-Asien Institut) and taught by Maxim G. Romanov"
url: 'https\://maximromanov.github.io/dh_in_mes/'
github-repo: "maximromanov/dh_in_mes"
apple-touch-icon: "touch-icon.png"
apple-touch-icon-size: 120
favicon: "favicon.ico"
documentclass: book
link-citations: yes
bibliography:
- book.bib
- packages.bib
biblio-style: apalike
---

# Syllabus{-}

## Course Details{-}

* Course: 57-525 S: Digital Humanities in Middle East Studies --- `Introduction to Algorithmic Analysis` (WS2021)
* Language of instruction: English
* Meeting time: Mo 12:00-14:00
* Meeting place: due to COVID, all meetings will be held online via Zoom
* Meeting link: *shared via Slack*; other details are available via STiNE
* Office hours: Mo 14:00-15:00 (on Zoom); if you have any questions, please, post them on Slack
* Instructor: Dr. Maxim Romanov, [maxim.romanov@uni-hamburg.de](maxim.romanov@uni-hamburg.de)
* Course: 070172-1 UE Methodological course - Introduction to DH: Tools & Techniques (2020W) `Memex Edition`
* Instructor: Dr. Maxim Romanov, [maxim.romanov@univie.ac.at](maxim.romanov@univie.ac.at)


## Aims, contents and method of the course

The course is a practical introduction to a series of digital tools and techniques that are relevant for analytical work both inside and outside of academia. The course will cover such topics as sustainable academic/analytical writing, organization of research workflow, data collection and structuring, as well as basics of text analysis, mapping, and social network analysis. In the course of these units, you will start working with Python, one of the most prominent programming languages used now by humanists and data scientists alike (no prior programming experience is required, but beneficial). The assessment will be based on your in-class participation, timely submission of homework assignments, and the final project, where you will be encouraged to work with data from your disciplinary domain.

Personal computers are required both for in-class work and for your homework (running full versions of either Windows, MacOS, or Linux; unfortunately, neither tablets nor Chrome-based laptops are suitable for this course).

## Course Evaluation 

Course evaluation will be a combination of in-class participation (30%), weekly homework assignments (50%), and the final project (20%). 

## Class Participation

Each class session will consist in large part of practical hands-on exercises led by the instructor. BRING YOUR LAPTOP! We will accommodate whatever operating system you use (Windows, Mac, Linux), but it should be a laptop rather than a tablet. Don’t forget that asking for help counts as participation!

## Homework

Just as in research and real life, collaboration is a very good way to learn and is therefore encouraged. If you need help with any assignment, you are welcome to ask a fellow student. If you do work together on homework assignments, then when you submit it please include a brief note (just a sentence or two) to indicate who did what.

**NB:** On submitting homework, see below.

## Final Project

Final project will be discussed later. You will have an option to build on what we will be doing in class, but you are most encouraged to pick a topic of your own. The best option will be to work on something relevant to your field of study, your term paper or your thesis.

## Study materials:

Most study materials will be distributed by the instructor.
* Zelle, John M. 2016. *Python Programming: An Introduction to Computer Science*. 3rd edition. Portland, Oregon: FRANKLIN BEEDLE & ASSOC.
* “Programming Historian” offers a number of tutorials for aspiring digital humanists. These will be assigned to you as reference materials. You also are encouraged to explore those tutorials that are not included into the course. <https://programminghistorian.org/lessons/>
* Paul Vierthaler's “Hacking the Humanities Tutorials” (Python), <https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17>

## Software, Tools, & Technologies:

The following is the list of software, applications and packages that we will be using in the course. Make sure to have them installed by the class when we are supposed to use them.

* Zotero, <https://www.zotero.org/>;
* MS Word or Apache OpenOffice (you most likely already have one of these)
* [Mac] Terminal / [Windows] Powershell (both are already on your machines)
* Python <https://www.python.org/>
* git and <https://github.com/>, version control system 
* pandoc (<https://pandoc.org/>), markdown, bibTeX (bibliographical format for LaTeX)
* QGIS, a Free and Open Source Geographic Information System (<https://qgis.org/en/site/>)
* Regular expressions, EditPad Pro/Sublime Text
* Wget (<https://www.gnu.org/software/wget/>), a free software package for retrieving files
* [TEI] XML, csv/tsv, json, yml, etc.
* Gephi (<https://gephi.org/>)

## Submitting Homework:

* Homework assignments are to be submitted by the beginning of the next class;
* For the first few classes you must email them to the instructor (as attachments)
* Later, you will be publishing your homework assignments on your github pages and sending an email to the instructor informing that you have completed your homework and providing a relevant github link.
	*  In the subject of your email, please, use the following format: `CourseID-LessonID-HW-Lastname-matriculationNumber`, for example, if I were to submit homework for the first lesson, my subject header would look like: `070112-L01-HW-Romanov-12435687`.
* DH is a collaborative field, so you are most welcome to work on your homework assignments in groups, however, you must still submit it. That is, if a groups of three works on one assignment, there must be three separate submissions: either emailed from each member’s email and published at each member’s github page.  

## Schedule

**Location**: Online

* 00 - Mo, 11. Okt. 2021 - 12:00-14:00
* 01 - Mo, 18. Okt. 2021 - 12:00-14:00
* 02 - Mo, 25. Okt. 2021 - 12:00-14:00
* 03 - Mo, 01. Nov. 2021 - 12:00-14:00
* 04 - Mo, 08. Nov. 2021 - 12:00-14:00
* 05 - Mo, 15. Nov. 2021 - 12:00-14:00
* 06 - Mo, 22. Nov. 2021 - 12:00-14:00
* 07 - Mo, 29. Nov. 2021 - 12:00-14:00
* 08 - Mo, 06. Dez. 2021 - 12:00-14:00
* 09 - Mo, 13. Dez. 2021 - 12:00-14:00
* 10 - Mo, 03. Jan. 2022 - 12:00-14:00
* 11 - Mo, 10. Jan. 2022 - 12:00-14:00
* 12 - Mo, 17. Jan. 2022 - 12:00-14:00
* 13 - Mo, 24. Jan. 2022 - 12:00-14:00

## Lesson Topics (subject to modification)

- **[ `#01` ]** Citation Management and Academic Writing I - with Zotero and MS Word or Open Office
- **[ `#02` ]** “Off with the Interface!” Getting to know the command line
- **[ `#03` ]** Version Control and Collaboration: Github.com
- **[ `#04` ]** Citation Management and Academic Writing II - with Pandoc, markdown, Zotero/BibTex
- **[ `#05` ]** Constructing robust searches with Regular expressions
- **[ `#06` ]** Webscraping with Wget, preparing URLs with Python and other tools // “The Dispatch”
<!-- - ~~**[ #07 ]** Text Markup [TEI XML], and how to remove it... - with Python scripts [anaconda]~~ -->
- **[ `#07` ]** Understanding Structured Data
- **[ `#08` ]** Converting data into different formats // “The Dispatch”
<!-- - ~~**[ #09 ]** Georeferencing with QGIS~~ -->
- **[ `#09` ]** Extracting tagged data for analysis // “The Dispatch”
<!-- - ~~**[ #10 ]** Text to Map (1/2) - with Python and QGIS~~ -->
- **[ `#10` ]** Graphing chronological data  // “The Dispatch”
<!-- - ~~**[ #11 ]** Text to Map (2/2) - with Python and QGIS~~ -->
- **[ `#11` ]** Mapping data // “The Dispatch”
- **[ `#12` ]** Topic modeling & TF-IDF // “The Dispatch”
- **[ `#13` ]** Social Network Analysis (with Gephi); modeling network data // “The Dispatch”


<!--chapter:end:index.Rmd-->

# Lesson 01: Academic Writing I

## Bibliography Managers

Bibliography managers make your life easier when it comes to collectin, organizing and maintaining bibliographical references and your library of electronic publications (most commonly as PDFs). Additionally, they are an indispensable writing tool as they take care of formatting (and reformatting) references and bibliographies in any writing project that you might undertake. There are plenty of different programs out there with their advantages and disadvantages (for example, Mendeley, RefWorks, Citavi, Endnote, Papers, Zotero, and quite a few more). We will use Zotero---it is being developed by scholars for scholars; it is free and open source; it does pretty much everything you might possibly need from a program of this kind.

## Zotero

### Getting Started

- Zotero can be installed from here: <https://www.zotero.org/download/>; the page will offer you a version suitable for your operating system, but you should also see the links to versions for specific systems (Mac OS, Windows, Linux).
- During installation Zotero should automatically integrate into your browser (like Chrome or Firefox) and into your word processor (MS Word, LibreOffice, GoogleDocs are supported). It is possible that you may have to do that manually.
  - Zotero Connector for Chrome can be installed from the same page (<https://www.zotero.org/download/>)
  - detailed explanations on how to use word processor plugins can be found [here](https://www.zotero.org/support/word_processor_integration); you can use Zotero with [MS Word](https://www.zotero.org/support/word_processor_plugin_usage), [LibreOffice](https://www.zotero.org/support/libreoffice_writer_plugin_usage) and [Google Docs](https://www.zotero.org/support/google_docs); in case you cannot get your plugin activated, check the [Troubleshooting Section](https://www.zotero.org/support/word_processor_plugin_troubleshooting).

    
### Main Functionality

You need to be able to do the following tasks with your Zotero in order to take full advantage of its functionality.

*Online Tutorials*: If you prefer video tutorials, you can check a series of tutorials prepared by the [McGill Library](https://www.youtube.com/playlist?list=PL4asXgsr6ek5H5mM9GlA1d-YCb9KvP3Ja) (there are also plenty other tutorials on YouTube :); if you prefer to read, you can check a series of tutorials prepared by the [UC Berkley Library](https://guides.lib.berkeley.edu/zotero).

1. **Adding bibliographical records (and PDFs)**
  - *Using Zotero Connector*: the easiest way to add a reference is from a browser with Zotero connector. This can be done practically from any library or journal database (e.g., Uni Wien Library, Worldcat.org, JSTOR); simply click the connector button while you are on a page with a publication that you want to add to your Zotero database. PDF may be automatically downloaded, if available; keep in mind that in places like JSTOR you need to agree to terms before this function will work; what you need to do is to download one PDF manually from a JSTOR page, where you will be asked to agree to terms of their services;
  - *Drag-and-dropping PDFs into Zotero*; this however works only when Zotero can parse relevant bibliographical information from a PDF; This might be a good way to start if you already have lots of PDFs that you want to add to Zotero.
  - *Using Unique Identifiers*: you can use ISBN or DOI numbers.
  - *Using Import*: you can import bibliographical data from another application or from bibliographical files (formats, like RIS, which you can download from most libraries as well).
  - *Manually*: you can manually add and fill in a record as well.
2. **Write-and-cite**
  - *Detailed Instructions*: [MS Word](https://www.zotero.org/support/word_processor_plugin_usage), [LibreOffice](https://www.zotero.org/support/libreoffice_writer_plugin_usage) and [Google Docs](https://www.zotero.org/support/google_docs); you can also check the video tutorial.
  - *Add a citation*
  - *Customize a citation* (by adding prefixes, suffixes, page range for a specific reference, etc.).
  - *Change citation style*.
    - For example, change from *Chicago Manual of Style* to *Universität Wien - Institut für Geschichte* (Yes, there is this specific citation style for Zotero: <https://www.zotero.org/styles?q=id%3Auniversitat-wien-institut-fur-geschichte>); in order to do that you need to download the IfG style and install it into Zotero.
    - You can find lots of different citation styles here: <https://www.zotero.org/styles>; to add a new style to Zotero:
      - download the style you want.
      - Open Zotero. Go to Preferences (under Zotero, Edit, or Tools --- depending on your system).
      - Click the "Cite" button.
      - Click the "Styles" tab.
      - Click the `+` button at the bottom right.
      - Select the style file you saved in the first step.
  - *Generate and update bibliography* in your paper.
  - **NB:** If you use Zotero plugin for adding your citations, they remain connected to Zotero and can be automatically reformatted; you can also drag-and-drop any bibliographical record into any text editor---the reference will be formatted according to the currently selelected style, but it will not be connected to Zotero and cannot be reformatted automatically later.
3. **General Maintenance and Organization**
  - Zotero can [automatically] rename PDFs using metadata, although the default function is not very robust (see, *Zotfile* plugin below).
  - You can create “collections” and drag-and-drop publications relevant to a specific topic or project you are working on.

### Additional Functionality: Plug-Ins

There is a variety of third-party plugins that you can add to Zotero for additional functionality. The list of plugins can be found at <https://www.zotero.org/support/plugins>. To install a plugin, you need to download its `.xpi` file to your computer. Then, in Zotero, click “Tools → Add-Ons”, then drag the `.xpi` for the plugin onto the Add-Ons window that opens. Two plugins will be of particular interest to us: `Zotfile` and `BetterBibTeX`.

### Zotfile

Zotfile (<http://zotfile.com/>) is a Zotero plugin to manage your attachments: automatically rename, move, and attach PDFs (or other files) to Zotero items, sync PDFs from your Zotero library to your (mobile) PDF reader (e.g. an iPad, Android tablet, etc.) and extract annotations from PDF files.

This plugin is particularly helpful for organizing PDFs on your hard drive. By default, Zotero saves PDFs in a computationally safe, but humanely incomprehensible manner: each PDF, even if it is renamed from bibliographical metadata and is human readable, it is still placed into a folder whose name is a random sequence of characters. Zotfile allows you to organize PDFs in a more human-friendly manner. The first screenshot below shows Zotero default mode, while the second one shows Zotfile mode: essentially, Zotfile creates a folder for each author and PDFs of all publications by that author get placed in that folder. You can sync this folder with Dropbox or other cloud service and access it from your tablet or phone.

![Zotero default organization.](./images/zotero_default.png)

![Zotfile organization.](./images/zotero_zotfile.png)

### Better BibTeX for Zotero

For a moment this will not be an immediately useful plug-in, but it is the most important one for our Memex project. This plugin exports bibliographical data into a `bibTeX` format, which is very easy to process with python scripts (it also generates *citation keys* which can be used for citation in markdown, which we will cover later). The two screenshots below show how the same record looks in Zotero preview and in the `bibTeX` format.

![A Record in Zotero.](./images/rec_zotero.png)

![The Same Record in `BibTeX` Format.](./images/rec_bibtex.png)

## Homework{#HWL01}

- collect 30-50 bibliographic records into your Zotero (ideally with PDFs); the number may seem like a lot, but you will see that you can do that it will take only about 30 mins on JSTOR; those of you who are already using Zotero must already have more than 50 records in your databases. 
- clearly, you should be collecting items that are relevant to your fields of study and your research; organize them into folders, if that is necessary;
- **create Bibliography and email it to me** (this is one-click operation; try to figure on your own how to do this; asking on *Slack* counts);
- make sure that you are comfortable with the main functionality of Zotero; that you have the discussed plugins installed; to get comfortable with the main functionality, you should practice each listed procedure at least a couple of times.
- **in preparation for the next class**, please, watch the following  two short videos from Dr. Paul Vierthaler's *Hacking the Humanities* series:
  - [Episode 1: Introduction to the Hacking the Humanities Tutorial Series](https://www.youtube.com/watch?v=fhsH4ua9zP8&list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17&index=1) and install Python via Anaconda; you can also install Python directly from <https://www.python.org/>, but Anaconda distribution might make your life easier, especially if you are on Windows.
  - [Episode 2: The Command Prompt](https://www.youtube.com/watch?v=q5b19xMb97I&list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17&index=2&t=206s).

**Submitting homework:**

- Homework assignment must be submitted by the beginning of the next class;
- Email your homework to the instructor as attachments.
		* In the subject of your email, please, add the following: `CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber`, where `CCXXXXX` is the numeric code of the course; `LXX` is the lesson for which the homework is being submitted; `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.

<!--chapter:end:02-Lesson01.Rmd-->

# Lesson 02: Command Line

## Command Line

The knowledge of “command line” opens a whole new world of opportunities, as the number of interface-less programs and applications is significantly larger; command line also offers a more robust and direct controls over a computer. The main goal is to learn the basics of this indispensable tool.   

We can use `Terminal` on Mac (installed), `Powershell` on MS Windows (should be installed), although other command line tools will work as well. 

Before we proceed, however, let's discuss a few concepts:

* What a *filesystem* is
* How to run a program from the command line
* What it means to run a program
* How the computer knows what program to run
* How to refer to a file from the command line

### The filesystem

Every disk contains a filesystem and information about where disk data is stored and how it may be accessed by a user or application. A filesystem typically manages operations, such as storage management, file naming, directories/folders, metadata, access rules and privileges. Commonly used file systems include File Allocation Table 32 (FAT 32), New Technology File System (NTFS) and Hierarchical File System (HFS).

All the files and programs on your computer are organized into folders; all these folders are in some other folders all the way down to your hard drive, which we call the **root** of your filesystem. Every hard drive, USB drive, DVD, and CD-ROM has its own filesystem.

You normally look at the contents of your filesystem via the Finder (on Mac) or the Explorer (on Windows). Open a window there now.

The Finder / Explorer window opens in some folder, which might be different depending on what computer operating system you’re using. But you’ll usually have a navigation bar to the left, that will let you go to different places. You see *folders*, also known as *directories*, and you might see *files* too.

One thing that computer OSes like to hide from you is the fact that you have a home directory, where all your personal files and folders should live. This makes it easier for multiple users to use a single computer. You can find your home directory like this:

* On **Mac**, select `Go` > `Home` in the menu.

![](./images/L02/mac_finder.png)

* On **Windows**, click on `Local Drive (C:)`, then click on `Users`, then click on your login name.

![](./images/L02/windows_explorer.png)

You’ll see that your home directory has several folders in it already, that were created automatically for you when you first made a user account.

Now how can you tell where you are, with respect to the root of your drive?

* On Mac, select `View` > `Show Path Bar` in the menu.

![](./images/L02/mac_path.png)

On Windows, look:

![](./images/L02/windows_path.png) 

The Finder / Explorer will also show you where in your computer’s filesystem you are. This is called the path—it shows you the path you have to take from the root of your filesystem to the folder you are in.

Now if you are on Windows, click on that bar and you’ll see something surprising.

![](./images/L02/windows_realpath.png) 

This is your real path. The `C:\` is how Windows refers to the root of your filesystem. Also note that, even if your OS is not in English, the path may very well be!

### Getting started with the command line

Now that you have a hint of what is going on behind the scenes on your computer, let’s dive into the command line. Here is how you get there:

* On Mac, look for a program called `Terminal.app`

![](./images/L02/mac_terminal.png) 

* On Windows, look for a program called `Powershell`

![](./images/L02/windows_powershell.png) 

By default, these shells open in your home directory. On Windows this is easy to see, but on Mac it is less clear—that is, until you know that this `~` thing is an alias for your home directory.

### Components of the command line

The command line consists of a *prompt* where you type your commands, the *commands* and *arguments* that you type, and the *output* that results from those commands.

The *prompt* is the thing that looks like (where `user` is your username):

```
MacBook-Pro:~ user$
```

or

```
PS C:\Users\user>
```

You will never need to type the prompt. That means that, if you are noting down what we do in class for future reference, you should not copy this part!

The prompt actually gives you a little bit of information.

* On Mac, it has the name of the computer, followed by a `:`, followed by the directory where you are, followed by your username, with `$` at the end.
* On Windows, it has `PS` for `PowerShell`, followed by the name of the drive (`C` for most of you), followed by a `:`, followed by the full path to where you are, with `>` at the end.

When you type a command, nothing happens until you press the `Return/Enter` key. Some commands have output (more text that appears after you press `Return/Enter`) and others don’t. You cannot run another command until the prompt is given again.

**NOTE**: From this point on, you will be running the commands that are run here!

Let’s first make sure we are in our home directory by typing `cd ~`. For most of you this should change nothing, but now you know your first shell command. The `cd` stands for `change directory`, and what follows is the directory you want to go to.

```
cd ~
```

Now let’s have a look around. The command to show what is in any particular directory is called ls, which stands for list. Try running it.

```
ls
```

If you are on Windows, what you get will look more like this:

```
PS C:\Users\user> ls
```

You should then see something like:

```
Verzeichnis: C:\Users\user


Mode                LastWriteTime     Length Name
----                -------------     ------ ----
d----        23.02.2016     21:18            .oracle_jre_usage
d-r--        23.02.2016     20:40            Contacts
d-r--        23.02.2016     20:40            Desktop
d-r--        23.02.2016     21:11            Documents
d-r--        23.02.2016     21:16            Downloads
d----        23.02.2016     21:24            exist
d-r--        23.02.2016     20:40            Favorites
d-r--        23.02.2016     20:40            Links
d-r--        23.02.2016     20:40            Music
d-r--        23.02.2016     20:40            Pictures
d-r--        23.02.2016     20:40            Saved Games
d-r--        23.02.2016     20:40            Searches
d-r--        23.02.2016     20:40            Videos


PS C:\Users\user>
```

Now go into your documents folder and look around.

```
cd Documents
ls
```

How does this compare to what you see in the Finder / Explorer window, if you click on the Documents folder?

Another important command, which tells you where you are at any given time, is `pwd`. This means print working directory. Try it now and see what you get.

```
pwd
```

If ever you get lost on the command line, `pwd` will always help you find your way.

### File paths and path notations

By now you will have noticed that I've mentioned the **path** a few times, and that it seems to have something to do with this thing that **pwd** prints out. (And, most annoyingly, that it looks different on `Mac` and `Windows`) The bit of text that you get from `pwd` is what is called path notation, and it is very important that you learn it if you want to do anything with your own digital data. Here are some rules:

* The `/` (or `\\` on Windows) separates folder names. So `Desktop/Video` means “the thing called Video inside the Desktop folder”.
* The `/` all by itself refers to the base of your hard drive (usually `Macintosh HD` or `C:\`.)
* The `~` refers to your home folder.
* These things can be combined; `~/Documents` means “the Documents folder in my home folder.”
* The `.` means “the current working directory”, i.e. what you would get if you ran the command `pwd`.
* The `..` means “one directory back”—if `pwd` gives you `/Users/user`, then `..` means `/Users`.
* If the path does not start with a `.` or a `/` or a `~`, then it will be assumed to start with a `./`, that is, “start from the current working directory.”

Let’s wander around a bit. But, first, let's download [a zip file with some materials for this class](./files/command_line_practice.zip). Unzip it somewhere and go to that folder in your `Terminal` or `Powershell`.

```
cd /path/to/the/folder/tnt_practice_materials
pwd
```

```
cd ./cd 02_CommandLine/
pwd
```

```
ls
```

Try the following if you are on Mac
```
ls -lh
```

```
cd ..
pwd
```

**NB:** you can use `TAB` to autocomplete the path: type `ls` to see what folders are in `Documents`, then go to any one of them by typing `cd` (space) and then the first two letters > after that use `TAB` and the name will be complete automatically. 

```
cd 03[TAB]
pwd
```

```
cd ../01[TAB]
pwd
```

```
ls McCarty_Modeling.pdf
cd ..
```

### Command line arguments

So far we have learned three commands: `cd`, `ls`, and `pwd`. These are useful for navigation, but we can run a lot more commands once we learn them, and have a need for them!

What are we doing, exactly?

* First word is the `command`
* All other words are the `arguments`
* Words **must be** separated by `spaces`

`cd` is a command that expects an argument: the name of the directory you want to go to. But what if the name has a space in it?

**NB:** You may think of most commands as sentences with subject, predicate, and object (or multiple objects).

```
cd ./01_Zotero_Word/Green Eggs and Ham
```

What happened there?

Well, we have a folder called **Green Eggs and Ham** in our example, and we tried to go there. But since the command line works with arguments, and since arguments are separated by space, the machine interpreted this as if we were saying “Change to the `./01_Zotero_Word/Green` folder, and then `Eggs`, `and`, `Ham`, whatever that means.” And it gave us an error, because we don’t have a folder called `Green` in our example.

You can get around this. How you get around it depends on whether you’re on Windows or not. One way to get around it that should work both places is like this:

On Windows:
```
cd './01_Zotero_Word/Green Eggs and Ham'
```

On Mac (you need to *escape* spaces by adding a `backslash` in front of them):
```
cd ./01_Zotero_Word/Green\ Eggs\ and\ Ham/
```

**NB:** The easiest solution is to use `TAB` for autocomplete!

### More commands

With command line you can do everything that you became accustomed to be doing in a graphical interface of your favorite file manager. For example, you can `copy`, `move`, and `delete` files and folders.

You can use:

* `mv` to move files
* `rm` (on Windows also: `del`) to remove/delete files
* `cp` to copy files

In all cases you need to state which files you want to `mv`, `rm`, or `cp`. In some cases you also need to point where you want to `mv` or `cp` your files.

**NB:** Syntax on `Mac` and `Windows` will vary slightly, but if you keep using `[TAB]` for autocompletion, there will be no different in the process of typing the command, so let’s try to do it this way. 

To start, let’s go to the root directory of our course materials. From there, let’s do the following:

```
cd 01[TAB]
ls
cp Mc[TAB] Green[TAB]
cd Green[TAB]
ls
```

**NB:** when you hit `[TAB]` after `Mc` you are not going to get the full autocomplete, because there are two files that start with `McCarty_Modeling`—one is `pdf` and another—`txt`. You will need to type one more letter `p` and then hit `[TAB]` again to get the file name that you need. Thus, the command can be transcribed as: `M[TAB]p[TAB]`

Now let’s `rm` (*delete*) the `McCarty_Modeling.pdf` from this folder, then go to the folder where we copied it, and then `mv` (*move*) it back to where it was in the first place.

```
rm M[TAB]p[TAB]
ls
cd G[TAB]
mv Mc[TAB] ../
cd ..
ls
```

Tada! The `McCarty_Modeling.pdf` should now be back where it was.

If you want to learn about new commands, try to `google`. Googling things like this is a very big part of being a DH scholar! You will most likely find your answers on <https://stackoverflow.com/>, which will become your most frequented resource, if you embark on the DH path. 

## Homework{#HWL02}

**Command line**

* Watch again a short video on `Command Prompt` in Dr. Vierthaler's *Hacking the Humanities* series: [Episode 2: The Command Prompt](https://www.youtube.com/watch?v=q5b19xMb97I&list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17&index=2&t=206s).
* Work through the following materials on command line which is relevant to your operating system.
  * Ted Dawson, "Introduction to the Windows Command Line with PowerShell," The Programming Historian 5 (2016), <https://programminghistorian.org/en/lessons/intro-to-powershell>.
  * Ian Milligan and James Baker, "Introduction to the Bash Command Line," The Programming Historian 3 (2014), <https://programminghistorian.org/en/lessons/intro-to-bash>.

**Python**

* Work through Chapter I of Zelle's book; read the entire chapter; retype and run all code snippets as described in the book; work through the chapter summary and exercises; complete all programming exercises;
  * For submission: email me the results of "Programming Exercises". In your submission there should be text files or python script files for exercises 1 (results of print function), 3, 4, 5, 7. Each python script should be working, i.e. you should be able to run it and get relevant results. You are welcome to discuss any of these assignments on Slack.
* Work through the following videos from Dr. Vierthaler's *Hacking the Humanities* series:
  * [Episode 3: The Very Basics of Python](https://www.youtube.com/watch?v=4oSiCLq3AWo)
  * [Episode 4: Strings](https://www.youtube.com/watch?v=NtJiVvs96zY)
  * [Episode 5: Integers, Floats, and Math in Python](https://www.youtube.com/watch?v=Y_OmmuNA_NE)
  * **NB:** The best way to work through these tutorials is to repeat all steps after the instructor. You can find the scripts at <https://github.com/vierth/humanitiesTutorial>.

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Email your homework to the instructor as attachments.
	* In the subject of your email, please, add the following: `CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber`, where `CCXXXXX` is the numeric code of the course; `LXX` is the lesson for which the homework is being submitted; `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.

<!--chapter:end:02-Lesson02.Rmd-->

# Lesson 03: Version Control

## Version Control and Collaboration

Version control systems are extremely helpful for the development of DH projects, which are often lengthy and complex and require organic collaboration. `Git` and `GitHub` are currently the most popular tools of this kind. In fact, it is difficult to imagine a DH project that would not rely on the use of `git` and `GitHub`. 

Before we begin, make sure to:

* Create a github account at <https://github.com/>, if you do not have one yet.
* Download and install `git` software:
	* for Windows:
		* you can download it from <https://git-scm.com/download/win>. Please, choose **64-bit Git for Windows Setup**.
			* you can also install a portable version of `git` which does not require installation <https://git-scm.com/download/win>. For this, choose **64-bit Git for Windows Portable**. Simply download and unzip (*Suggestion*: move that unzipped folder to the folder where you keep all class-related files and materials). In the folder, run `git-bash.exe` (for a more Unix-like command line) or `git-cmd.exe` (for Windows command line).
	* for Mac: try to run `git --version` from Terminal. If `git` is not installed, you will be prompted to install `Xcode Command Line Tools` which comes with `git` among other things. This is the easiest way.
	* **Note:** there are also interface tools for *github*. We will not be working with them in the class, but you are welcome to test them on your own at home. See, <https://desktop.github.com>. The main reason for this is because interface tools will be different for different operating systems, while the command line usage will be exactly the same across all platforms.

In class we will cover the following:

* Basic `git` functionality;
* Starting a `github`-based website;
* Basics of `markdown`;

## Setting-up `git`

 - `git config --global user.name "YourName"`
 - `git config --global user.email "YourEmail"`

## General `git` workflow

* *In `Terminal` (on Mac) or `Git-Bash` (on Windows)*
	1. create a repository under your account online at <https://github.com>.
	2. Alternatively, you can also `fork` somebody else's repository.^[**NB:** this is done on <https://github.com>); forking means creating your own copy of some one's repository at that specific moment in time]
	3. `clone` (**NB:** this is done on <https://github.com>!)
	4. *work*
	5. `add`
	6. `commit`
	7. `push` / `pull`
	8. send `pull request` (**NB:** this is done on <https://github.com>)

**Note:** Steps 2 and 8 are relevant only when you work on a project (*repository*) that is owned by somebody else. If you work on a *repository* that you created under your account, you only need steps 1, 3-7. Below is a visual representation of this cycle.

```{r echo=FALSE}

DiagrammeR::grViz("
digraph research_cycle {

  # a 'graph' statement
  graph [overlap = false, fontsize = 10, rankdir = TB]

  # several 'node' statements
  node [shape = box,
        fontname = Baskerville,
        fontsize = 10,
        style = rounded,
        penwidth = 0.1]
  github;
  create;
  fork;

  # several 'node' statements
  work [shape=box, regular=1, style=rounded, fillcolor=grey50, label = 'working...']

  # several 'node' statements
  node [shape = box,
        fontname = Courier,
        fontsize = 10,
        style = filled,
        penwidth = 0.1]  
  clone [label = 'git clone <repository_link>'];
  add [label = 'git add .'];
  commit [label = 'git commit -m “message”'];
  push [label = 'git push origin master'];
  pull [label = 'git pull origin master'];
  
  subgraph cluster1 {
  github->create; github->fork;
    color = gray;
    style = rounded;
    label = 'github.com';
    fontcolor = gray;
  }
  
  subgraph cluster2 {
  clone->work;
  work->add; add->commit;
  commit->push;
  push->github;
  github->pull; pull->work;
    color = black;
    style = rounded;
    label = 'work cycle';
    fontcolor = gray;
    fillcolor = gray;
  }
  
  create->clone; fork->clone;

}",height = 600)
```


## Main `git` Commands

* `git clone <link>`
	- clones/downloads a repository on you machine
* `git status`
	- shows the current status of the repository (new, changed, deleted)
* `git add .`
	- adds all new files and modified files to the repository
* `git commit -m "message"`
	- saves all files in their current state into the repository, and created a milestone
* `git push origin master`
	- uploads changes to <https://github.com>
	- `origin` is a specific repository you are pushing your changes to; it is automatically set up, when you clone a repository on your computer.
	- `master` is the *branch* you are pushing to the repository; `master` is the default name of the main branch in a git repository. To check the names of your branches, you can type `git branch`.
	- **NB:** sometimes you may get an error, which in most cases means that you need to `pull` first
* `git pull origin master`
	- downloads changes from <https://github.com>
* `git log`
	- shows the history of `commit`s; here you can choose where you want to roll back, in case of troubles.

## Some useful command line commands to remember

* `pwd`
	- shows you where you are on a drive (gives you path)
* `ls` / `dir` [on Windows]
	- shows everything in the your current location/folder
* `cd <name of the folder>`
	- takes you to that folder
* `cd ..` 
	- takes you one level up in the tree structure of your computer

## Practice

- Under your GitHub account, create repository `HW070172`;
- clone it to your computer (use command line: `git clone LinkToYourRepository`);
- Now, in the repository:
  - let's edit `README.md` (create it, if you have not yet); add some text into this file
  - create subfolders for Lessons, like `L01`, `L02`, `L03`, etc.
  - copy/paste your homework files in respective subfolders.
  - Now, do the `add`-`commit`-`push` routine to upload the files to your repository
- Now, online:
  - check if your files are there
  - let's do some edits to the `README.md` file (markdown basics / *github flavor*)
- `pull` / `push`

## Homework{#HWL03}

**Git and GitHub**

* Watch a video on `Git & GitHub` in Dr. Vierthaler's *Hacking the Humanities* series: [Supplement 1: A quick Git and Github Tutorial](https://www.youtube.com/watch?v=YetC-gxgIVY). This will help you to go over the new material and pick up a few more useful `git` *&* `gitHub` tricks.
* There is an interface for github that you can also use, but I strongly recommend to use command line; interfaces change, but commandline commands remain the same!
  - Daniel van Strien. 2016. "An Introduction to Version Control Using GitHub Desktop," The Programming Historian 5, [https://programminghistorian.org/](https://programminghistorian.org/en/lessons/getting-started-with-github-desktop).
* Please, also read (for `markdown`): Simpkin, Sarah. 2015. “Getting Started with Markdown.” Programming Historian, November. [https://programminghistorian.org/](https://programminghistorian.org/lessons/getting-started-with-markdown).
  * More on github-flavored `markdown`: <https://guides.github.com/features/mastering-markdown/>.
  * On `markdown` for academic writing, see <https://pandoc.org/MANUAL.html>.
  * A cheat-sheet & interactive tutorial for your practice: <https://commonmark.org/help/>.

*Extra*: you can build and host a website on `github.com`; your website will have the name: `YourUserName.github.io` --- you can create a repository with that name and build your website there using Jekyll and GitHub Pages. Any other repository may also be converted into a part of your website, which will be accessible at `YourUserName.github.io/YourRepository/`

* Visconti, Amanda. 2016. “Building a Static Website with Jekyll and GitHub Pages.” Programming Historian, April. [https://programminghistorian.org/](https://programminghistorian.org/lessons/building-static-sites-with-jekyll-github-pages).


**Python**

- Work through Chapter II of Zelle's book; read the entire chapter; retype and run all code snippets as described in the book; work through the chapter summary and exercises; complete all programming exercises;
- Watch [Dr. Vierthaler's videos](https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17):
  - Episode 04: Strings;
  - Episode 05: Integers, Floats, and Math in Python;
  - Episode 06: Lists

**Submitting homework**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfoler in your `HW070172` repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)
	* In the subject of your email, please, add the following: `CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber`, where `CCXXXXX` is the numeric code of the course; `LXX` is the lesson for which the homework is being submitted; `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.

<!--chapter:end:02-Lesson03.Rmd-->

# Lesson 04: Sustainable Academic Writing

## `markdown`, `pandoc`, the `*TeX` family, and `obsidian`

Introduction to sustainable academic writing that avoids any proprietary software solutions and formats. Before the class, make sure to install the following.

* `pandoc` (follow instructions on [https://pandoc.org/installing.html](https://pandoc.org/installing.html))
* `LaTeX` engine (install from here: [https://miktex.org/](https://miktex.org/))
	* `LaTeX` for Mac: MikTeX seems to be very finicky with Macs. The following solution proves to be more manageable and stable (you need to run this command in Terminal): `brew install librsvg python homebrew/cask/basictex` (from Pandoc page); after that missing packages might have to be installed manually, but that is relatively easy --- the sustem will prompt you to install them, and that needs to be done only once; alternatively, one can install MacTeX (this one is quite large, about 4Gb).
* `markdown`
* Obsidian (<https://obsidian.md/>)


```{r echo=FALSE}

DiagrammeR::grViz("
digraph writing_cycle {

  # a 'graph' statement
  graph [overlap = false, fontsize = 10, rankdir = TB]

  # several 'node' statements
  node [shape = box,
        fontname = Baskerville,
        fontsize = 10,
        style = rounded,
        penwidth = 0.1]
  bibtex [label = 'bibTex bibliography']
  markdown [shape=box, style=filled, fillcolor=grey50, fontcolor=white, fontname='courier-bold', label = 'textfile in markdown'] ;
  zotero[label = 'Zotero']
  pandoc[label='pandoc engine']
  style[label='citation style']
  latex[label='TeX engine']
  html[label='HTML']
  doc[label='MS Word']
  pdf[label='PDF']
  
  
  zotero->bibtex; bibtex->markdown; bibtex->pandoc;
  markdown->pandoc; style->pandoc;
  pandoc->html; pandoc->doc; pandoc->latex->pdf;

}",height = 600)
```


## Class Notes

*Files*: Download the following archive file: [sustainable_writing.zip](./files/sustainable_writing.zip). Make sure to unzip it! It contains the following files:

* `biblio.bib`—a bibliography file;
* `cms-fullnote.csl`—a citation style;
* `main.md`—the main text file (its contents are also shown below);

**NB:** remember that all files must be in the same folder; it makes sense to put folders into a subfolder (not to overcrowd your main folder), but then do not forget to change the path in your `image` code.

(The text of the `main.md` file is also given at the very end of the lesson.)

## `pandoc` *Commands*

(*a quick demo*)

**NB:** On Windows, you may see a pop-up Windows from `MikTex` asking to download a missing package for `LaTeX`. This means that some package is missing and you need to download it (or several of them). Uncheck a birdie to install all necessary packages at once. After that everything should work.

First try to convert to `docx` or `html`. These two formats do not require `LaTeX`.

```
pandoc -f markdown -t docx -o main.docx --filter pandoc-citeproc main.md
pandoc -f markdown -t html -o main.html --filter pandoc-citeproc main.md
pandoc -f markdown -t epub -o main.epub --filter pandoc-citeproc main.md
pandoc -f markdown -t latex -o main.pdf --filter pandoc-citeproc main.md
```

**NB:** it may so happen that your version of `pandoc` will complain about `--filter pandoc-citeproc`. If that happens, your commands should look like the following:

```
pandoc -f markdown -t docx -o main.docx --citeproc main.md
pandoc -f markdown -t html -o main.html --citeproc main.md
pandoc -f markdown -t epub -o main.epub --citeproc main.md
pandoc -f markdown -t latex -o main.pdf --citeproc main.md
```


**Comment:**

* `-f` means “convert `from` a specific format”.
* `-t` means “convert `to` a specific format”.

Thus, the whole command (say, the first one) reads as follows: `pandoc converts from (-f) markdown to (-t) latex, then outputs (-o) main.pdf, to which a ‘filter’ that processes citations (--filter pandoc-citeproc) is applied; and the file to which this all is applied is main.md`.

## Analytical Writing with markdown

You can find a number of blogposts online about how different scholars are using markdown to write their academic works. For example, Scott Selisker, an Associate professor at the Department of English at the U Arizona, shares his experience of writing his using a popular text editor Atom (<https://atom.io/>), which allows one to integrate writing in markdown with the helpfulness of Zotero, as well as offers quite a few other nice features. (You can find how to set everything up in Scott Selisker's blog post: <http://u.arizona.edu/~selisker/post/workflow/>.)

## Zettelkasten with Obsidian

One of the most useful pieces of software for the purpose of analytical writing is Obsidian (<https://obsidian.md/>), which is available for all major operating systems. The developers of Obsidian call is "a powerful knowledge base on top of a local folder of plain text markdown files". The design of the program gives you the complete control over your data as there are no proprietary formats which may make your data inaccessible and no complicated technologies that hide your data in difficult-to-access formats (like, MySQL, for example).

The main power of Obsidian is that it allows you to connect your notes into a network of information, which makes it nearly perfect for complex analytical writing projects where you want to have an option to *branch* your writing into multiple directions and then reassemble your main argument by including and excluding parts of these branches. This method of academic writing has been most famously formalized by the German sociologist [Niklas Luhmann](https://en.wikipedia.org/wiki/Niklas_Luhmann), who came up with a paper-based slip-box system (*Zettelkasten*) which he used to produce some 50 books and 550 articles [@SchmidtNiklas2016, p. 289].  

There are lots of materials written about the *Zettelkasten* system---many of them, for obvious reasons, are in German. One of the best expositions of the method is Sönke Ahrens's *How to Take Smart Notes: One Simple Technique to Boost Writing, Learning and Thinking – for Students, Academics and Nonfiction Book Writers*, which is an English version of his *Das Zettelkasten-Prinzip: Erfolgreich wissenschaftlich Schreiben und Studieren mit effektiven Notizen*. [See: @AhrensHow2017; for more: @LuhmannKommunikation1982; @SchmidtNachlass2014; @SchmidtNiklas2018; @SchmidtNiklas2016]. There are several videos on YouTube, where Sönke Ahrens discusses his book in great details (for example, [this](https://www.youtube.com/watch?v=kXnR7qX3BDc) and [this](https://www.youtube.com/watch?v=nPOI4f7yCag)).

*How to* with/in Obsidian (*a quick demo*):

- using markdown;
- creating notes;
- linking notes; creating branches; creating a network;
- crafting the final narrative/argument;

## Reference Materials:

* Simpkin, Sarah. 2015. “Getting Started with Markdown.” Programming Historian, November. [https://programminghistorian.org/lessons/getting-started-with-markdown](https://programminghistorian.org/lessons/getting-started-with-markdown).
* Tenen, Dennis, and Grant Wythoff. 2014. “Sustainable Authorship in Plain Text Using Pandoc and Markdown.” Programming Historian, March. [https://programminghistorian.org/lessons/sustainable-authorship-in-plain-text-using-pandoc-and-markdown](https://programminghistorian.org/lessons/sustainable-authorship-in-plain-text-using-pandoc-and-markdown).

## Homework{#HWL04}

* Convert a plain text paper into markdown and convert it with Pandoc into a PDF, MS Word, and HTML documents.
	* Plain text file for the task: [McCarty_Modeling.txt](./files/McCarty_Modeling.txt)
	* Use this PDF file as a guide for your formatting: [McCarty_Modeling.pdf](./files/McCarty_Modeling.pdf) 
	* Convert only the first 7 pages. You can skip up to 1/3 of bibliographical records, if you cannot find them online.
	* Alternatively, you can use any of your own papers that you have already written: 5 pages, 10 footnotes, 5 bibliography items. I would say this is preferable, since it will give you a better idea of how these tools work together.

**Python**

- Work through Chapters 3 and 5 of Zelle's book; read chapters carefully; work through the chapter summaries and exercises; complete the following programming exercises: 1-8 in Chapter 3; 1-7 in Chapter 5;
- Watch [Dr. Vierthaler's videos](https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17):
	- Episode 07: Booleans (and Boolean Operators)
	- Episode 08: Loops (and file objects)

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfoler in your `HW070172` repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)
	* In the subject of your email, please, add the following: `CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber`, where `CCXXXXX` is the numeric code of the course; `LXX` is the lesson for which the homework is being submitted; `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.
	
## Appendix: *TEXT for your `main.md` file.*

```
---
title: |
    *From*: "Modeling: A Study in Words and Meanings" by Willard McCarty
subtitle: 
author: 
date: \today
bibliography: biblio.bib
csl: cms-fullnote.csl
---

>> Out on site, you were never parted from your plans.
They were your Bible. They got dog-eared, yellowed,
smeared with mud, peppered with little holes from
where you had unrolled them on the ground. But
although so sacred, the plans were only the start.
Once you got out there on the site everything was
different. No matter how carefully done, the plans
could not foresee the *variables*. It was always
interesting, this moment when you saw for the first
time the actual site rather than the idealised
drawings of it.

>> Kate Grenville, *The Idea of Perfection*
(Sydney: Picador, 1999): 62–3


# Introduction

The question of modeling arises naturally for
humanities computing from the prior question
of what its practitioners across the disciplines
have in common. What are they all doing with
their computers that we might find in their
diverse activities indications of a coherent or
cohesible practice? How do we make the best,
most productive sense of what we observe? There
are, of course, many answers: practice varies 
from person to person, from project to project, 
and ways of construing it perhaps vary even more. 
In this chapter I argue for modeling as a model 
of such a practice. I have three confluent goals: 
to identify humanities computing with an intellectual 
ground shared by the older disciplines, so that we 
may say how and to what extent our field is of as 
well as *in* the humanities, how it draws from and 
adds to them; at the same time to reflect experience 
with computers "in the wild"; and to aim at the
most challenging problems, and so the most
intellectually rewarding future now imaginable.

My primary concern here is, as Confucius almost 
said, that we use *the correct word* for the 
activity we share lest our practice go awry 
for want of understanding (*Analects 13.3*). 
Several words are on offer. By what might be 
called a moral philology I examine them, arguing 
for the most popular of these, "modeling." The 
nominal form, "model", is of course very useful 
and even more popular, but for reasons I will 
adduce, its primary virtue is that properly 
defined it defaults to the present participle, 
its semantic lemma. Before getting to the 
philology I discuss modeling in the light of 
the available literature and then consider 
the strong and learned complaints about the 
term.

# Background

Let me begin with provisional definitions[^1]. 
By "modeling" I mean *the heuristic process of 
constructing and manipulating models*, a "model" 
I take to be either *a representation of something 
for purposes of study*, or *a design for realizing 
something new*. These two senses follow Clifford 
Geertz's analytic distinction between a denotative 
"model *of*" such as a grammar describing the 
features of a language, and an exemplary "model 
*for*" such as an architectural plan
[@geertz_interpretation_2017, 93][^2]. In both 
cases, as the literature consistently emphasizes, 
a model is by nature a simplified and therefore 
fictional or idealized representation, often taking 
quite a rough-and-ready form: hence the term 
"tinker toy" model from physics, accurately suggesting 
play, relative crudity, and heuristic purpose 
[@cartwright_how_1984, 158]. By nature modeling 
defines a ternary relationship in which it mediates 
epistemologically, between modeler and modeled, 
researcher and data or theory and the world 
[@morgan_models_1999]. Since modeling is 
fundamentally relational, the same object may 
in different contexts play either role: thus, 
e.g., the grammar may function prescriptively, 
as a model for correct usage, the architectural 
plan descriptively, as a model of an existing style. 
The distinction also reaches its vanishing point in 
the convergent purposes of modeling: the model of 
exists to tell us that we do not know, the model 
for to give us what we do not yet have. Models 
*realize*.

[^1]: My definitions reflect the great majority of 
the literature explicitly on modeling in the history 
and philosophy of the natural sciences, especially 
of physics. The literature tends to be concerned 
with the role of modeling more in formal scientific 
theory than in experiment. The close relationship 
between modeling and experimenting means that the 
rise of a robust philosophy of experiment since 
the 1980s is directly relevant to our topic; see 
[@hacking_stability_1988]. Quite helpful in 
rethinking the basic issues for the humanities 
are the writings from the disciplines other 
than physics, e.g., [@clarke_models_2015] on 
archaeology; on the social sciences, the essays 
by de Callatay, Mironesco, Burch, and Gardin 
in [@franck_explanatory_2011]. For interdisciplinary 
studies see Shanin (1972) and [@morgan_models_1999], 
esp. "Models as Mediating Instruments" (pp. 10–37).

[^2]: Cf. Goodman's distinction between 
"denotative" and "exemplary" models, respectively 
(1976: 172–3); H. J. Groenewold's "more or less 
poor substitute" and "more or less exemplary ideal" 
(1960: 98). Similar distinctions are quite common 
in the literature.

# Bibliography
```

<!--chapter:end:02-Lesson04.Rmd-->

# Lesson 05: Robust Searches

## Constructing Robust Searches with Regular Expressions

## `regular expressions`

In this lesson we will learn about `regular expressions`, an important semi-language for constructing complex searches. Any text editor that supports `regular expressions` will work fine for this lesson, but let's all use [Sublime Text](https://www.sublimetext.com/) (both Mac and Windows).

The practical materials for this lesson can be downloaded from here: <https://github.com/maximromanov/re_tutorial> (simply download the zip file of the repository.)

<!--
Let's use the following practicum files (`Right Click > Save File as ...`) for the in-class practice: 1) [version for training](https://raw.githubusercontent.com/maximromanov/re_tutorial/master/re_practucum_text_western.txt); 2) [version with answers](https://raw.githubusercontent.com/maximromanov/re_tutorial/master/re_practucum_text_western_answers.txt).
-->

Open the practicum file in `Sublime Text`.

### What are `regular expressions`?**

- very small language for describing textual patterns
- not a programming language, yet a part of each one
- incredibly powerful tool for find/replace operations
- old (1950s-60s)
- “arcane art”
- ubiquitous

![**Source**: <https://xkcd.com/208/>](https://imgs.xkcd.com/comics/regular_expressions.png)

### What would we use `regular Expressions` for?

**to search**:

  * all spelling variations of the same word:
    - Österreich, Osterreich or Oesterreich..
  * words of specific morphological patterns:
    - [*search*], [*search*]er, [*search*]ed, [*search*]ing, [*search*]es: all derivatives from the same root/word 
  * entities that may be referred to differently: 
    - references to Vienna in different languages? (Wien, Vienna, Вена, فيينا, etc.) 
    - references to Austria? (Vienna, Graz, Linz, Salzburg, Innsbruck, etc.)
  * references to concepts:
    - references to education in biographies: "s/he graduated from", "s/he studied", etc. 

**to search and replace**:

  * reformat “dirty”/inconsistent data (OCR output, for example)

**to tag**:

  * make texts navigable and more readable
  * tag information relevant to your research 

**and many other uses…**

### The Basics 

A *regular expression* (can be shortened as *regex* or *regexp*) is a sequence of symbols and characters expressing a string or pattern to be searched for within a longer piece of text. In this sequence there are characters that match themselves (most characters) and there are characters that activate special functionality (*special characters*). For example: 

- `Vienna` is a regular expression that matches “Vienna”;
- “`Vienna`” is a pattern;

**Question:** if the pattern `at` matches strings with “a” followed by “t”, which of the following strings will it match?[^Q1]

[]() |     |      |       |     |        |
-----|-----|------|-------|-----|--------|
at   | hat | that | atlas | aft | Athens |

[^Q1]: Matches are highlighted: **at**, h**at**, th**at**, **at**las, aft, Athens.

### Characters *&* Special Characters

- most characters match themselves.
- matching is case sensitive.
- special characters: `()^${}[]\|.+?*`. 
- to match a special character in your text, you need to “escape it”, i.e. precede it with “\” in your pattern: 
	– `Osterreich [sic]` **does not* match “Osterreich [sic]”.
	– `Osterreich \[sic\]` matches “Osterreich [sic]”.
	

### Character Classes: `[]` 

- characters within `[]` are choices for a single-character match; think of this as a type of **either or**.
- the order within `[]` is unimportant.
	- `x[01]` matches “x0” and “x1”.
	- `[10][23]` matches “02”, “03”, “12” and “13”.
- initial `^` negates the class: 
	– `[^45]` matches any character except 4 or 5.

**Question:** if the pattern `[ch]at` matches strings with “c” or “h” followed by “a”, and then by “t”, which of the following strings will this regular expression match?[^Q2]:

[]() |    |      |     |     |      |
-----|----|------|-----|-----|-------
that | at | chat | cat | fat | phat |

[^Q2]: Matches are highlighted: t**hat**, at, c**hat**, **cat**, fat, p**hat**.

### Ranges (within classes) 

- Ranges define sets of characters within a class. 
	– `[1-9]` matches any number in the range from 1 to 9 (i.e., any non-zero digit)
	– `[a-zA-Z]` matches any letter of the English alphabet (ranges for specific languages will vary)
	– `[12][0-9]` matches numbers between 10 and 29 (i.e., the first digit is either 1 or 2; the second one---any digit) 

**Ranges shortcuts**

Shortcut | Name | Equivalent Class  
:---------|:------|:----------------
`\d` | digit       | `[0-9]`  
`\D` | not digit   | `[^0-9]`  
`\w` | word        | `[a-zA-Z0-9_]` (*actually more*) 
`\W` | not word    | `[^a-zA-Z0-9_]` (*actually more*)
`\s` | space       | `[\t\n\r\f\v ]`  
`\S` | not space   | `[^\t\n\r\f\v ]`  
`.`  | everything  | `[^\n]` (depends on mode)  

**Question:** if the pattern `/\d\d\d[- ]\d\d\d\d/` matches strings with a group of three digits, followed by a space or a dash, and then---by another group of four digits, which of the following strings will this regular expression match?[^Q3]:


[]() |    |      |     |     |      |
-----|----|------|-----|-----|-------
501-1234 | 234 1252 | 652.2648 | 713-342-7452 | PE6-5000 | 653-6464x256 |

[^Q3]: Matches are highlighted: **501-1234**, **234 1252**, 652.2648, 713-**342-7452**, PE6-5000, **653-6464**x256.

### Repeaters

- these special characters indicate that the preceding element of the pattern can be repeated in a particular manner: 
	- `runs?` matches “runs” or “run”
	- `1\d*` matches any number beginning with “1”

repeater | count 
:--------|:------
`?`        | zero or one
`+`        | one or more
`*`        | zero or more
`{n}`      | exactly *n* times
`{n,m}`    | between *n* and *m* times
`{,m}`     | no more than *m* times
`{n,}`     | no less than *n* times

**Question:** We have several patterns, which strings will they match?[^Q4]

Patterns  |           |               |           |           |       
:---------|:----------|:--------------|:----------|:----------|:----------
A) `ar?t` | B) `ar*t` |  C) `a[fr]?t` | D) `ar+t` | E) `a.*t` | F) `a.+t`

Strings |          |             |
:-------|:---------|:------------|:----------
1) “at” | 2) “art” | 3) “arrrrt” | 4) “aft” 

[^Q4]: `ar?t` matches “at” and “art” but not “arrrt”; `a[fr]?t` matches “at”, “art”, and “aft”; `ar*t` matches “at”, “art”, and “arrrrt”; `ar+t` matches “art” and “arrrt” but not “at”; `a.*t` matches anything with an ‘a’ eventually followed by a ‘t’.

### Lab: *Intro*  (in the *practicum file*).

![](./images/regex01.png)

### Anchoring

- anchors match *between* characters.
- anchors are used to assert that the characters you’re matching must appear in a certain place.
- for example, `\bat\b` matches “**at** work” but not “b**at**ch”.

Anchor | matches...
:------|:----------
`^`    | the beginning of a line or a string
`$`    | the end of a line of a string
`\b`   | *word boundary*
`\B`   | *not word boundary*

### Alternation: “|” (*pipe*)

- in `regex`, “|” means “or”
  - on the US keyboard layout, this character is in the vicinity of “Enter” and “Right Shift”.
- you can put a full expression to the left of the *pipe* and another full expression to the right, so that either one could match:
  - `seek|seeks|sought` matches “seek”, “seeks”, or “sought”. 
  - `seeks?|sought` matches “seek”, “seeks”, or “sought”. 

### Grouping 

- everything within `( … )` is grouped into a single element for the purposes of repetition or alternation:
  - the expression `(la)+` matches “la”, “lala”, “lalalala” (but not “all”). 
  - `schema(ta)?` matches “schema” and “schemata” but not “schematic”. 
- grouping example: what regular expression would match “eat”, “eats”, “ate” and “eaten”? 
  - `eat(s|en)?|ate`
  - **NB**: we can make it more precise by adding word boundary anchors to exclude what we do not need, like, for example, words “sate” and “eating”: `\b(eat(s|en)?|ate)\b`.
  

### Lab: Part I (in the *practicum file*).

![](./images/regex02.png)

### Replacement

- `regular expressions` are most often used for search/replace operations
- in *text editors*:
  - *Search Window*: search pattern
  - *Replace Window*: replacement pattern
  
### Capture

- during searches, `( … )` groups capture patterns for use in replacement.
- special variables `\1`, `\2`, `\3`, etc. contain the capture (in some text editors: `$1`, `$2`, `$3`).
- if we apply `(\d\d\d)-(\d\d\d\d)` to “123-4567”:
  – `\1` (or, `$1`) captures “123”
  – `\2` (or, `$2`) captures “4567”

### Capture *&* Reformat

- How to convert “Schwarzenegger, Arnold” to “Arnold Schwarzenegger”?
  - Search: `(\w+), (\w+)`
  - Replace (a): `\2 \1`
  - Replace (b): `$2 $1`
- **NB:** (!) Before hitting “Replace”, make sure that your match does not catch what you do NOT want to change

### Lab: Part II (in the *practicum file*).

![](./images/regex02.png)

- Finding toponyms (placenames):
  - *very simple:* Construct regular expressions that find references to *all* Austrian cities.[^QII1]
  - *a bit tricky*: Construct regular expression that finds only cities from 1) Lower Austria; 2) Salzburg.[^QII2]

[^QII1]: *Solution*: Simply connect all toponyms from the list with a pipe symbol “|”
[^QII2]: *Solution 1*: `\b([\w ]+) \(Lower Austria\)` --- for Lower Austria; `\b([\w ]+) \(Salzburg\)` --- for Salzburg; *Solution 2 (cooler):* `\b([\w ]+)(?=( \(Lower Austria\)))` --- for Lower Austria; `\b([\w ]+)(?=( \(Salzburg\)))` --- for Salzburg.

### *To keep in mind*

- `regular expressions` are “greedy,” i.e. they tend to catch more than you may need. Always test!
- test before applying! (In text editors Ctrl+Z (Win), Cmd+Z (Mac) can help to revert changes)
- check the language/application-specific documentation: some common shortcuts are not universal (for example, some languages/applications use `\1` to refer to groups, while others use `$1` for the same purpose).

## Class materials

* Presentation with all the slides:
	- [PDF](https://github.com/maximromanov/re_tutorial/blob/master/RegularExpressions_Western.pdf?raw=true) (Windows PowerPoint Format)

## Digital materials

* Online references:
	- <http://www.regular-expressions.info/>
	- <http://ruby.bastardsbook.com/chapters/regexes/>
* Interactive tutorial: <http://regexone.com/>
* Cheat Sheets:
	- <http://krijnhoetmer.nl/stuff/regex/cheat-sheet/>
	- <http://www.rexegg.com/regex-quickstart.html>


## Reference Materials

* Goyvaerts, J. and Levithan, S. (2012). *Regular Expressions Cookbook*. Second edition. Beijing: O’Reilly. [Amazon Link](http://www.amazon.com/Regular-Expressions-Cookbook-Jan-Goyvaerts/dp/1449319432/).
* Friedl, J. E. F. (2006). *Mastering Regular Expressions*. 3rd ed. Sebastapol, CA: O’Reilly. [Amazon Link](http://www.amazon.com/Mastering-Regular-Expressions-Jeffrey-Friedl/dp/0596528124/)

(I will share PDFs of these books via Slack; I strongly recommend to flip through the first book just to get an idea of what kind of things one can do with regular expressions.)

## Homework{#HWL05}

- Finish the practicum; push your answers to your github repository.

**Python**

- Work through Chapters 6 and 7 of Zelle's book; read chapters carefully; work through the chapter summaries and exercises; complete the following programming exercises: 1-8 in Chapter 6; 1-9 in Chapter 7;
- Watch [Dr. Vierthaler's videos](https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17):
	- Episode 09: Dictionaries
	- Episode 10: Putting it Together (Analyses)
	- Episode 11: Errors (reading and handling)
- **Note:** the sequences are somewhat different in Zelle's textbook and Vierthaler's videos. I would recommend you to always check Vierthaler's videos and also check videos which cover topics that you read about in Zelle's book.

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfoler in your `HW070172` repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)
	* In the subject of your email, please, add the following: `CCXXXXX-LXX-HW-YourLastName-YourMatriculationNumber`, where `CCXXXXX` is the numeric code of the course; `LXX` is the lesson for which the homework is being submitted; `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.

<!--chapter:end:02-Lesson05.Rmd-->

# Lesson 06: Webscraping

## Getting to Know `WGET`

Here we will discuss webscraping --- the main process of efficiently collecting large volumes of information from the Internet.

## Software

* `wget` (<https://www.gnu.org/software/wget/>), a free software package for retrieving files using HTTP, HTTPS, FTP and FTPS the most widely-used Internet protocols. It is a non-interactive command line tool, so it may easily be called from scripts, cron jobs, terminals without X-Windows support, etc.

* **NB**: on installing `wget`:
	* On Windows (the easiest): download from <https://eternallybored.org/misc/wget/> > choose the latest 64-bit ZIP file (`EXE` will most likely be blocked by your browser as a potentially dangerous file).
		* Unzip the file and copy `wget.exe` to the folder where you are planning to scrape data; *NB:* the easiest approach on Windows is to move/copy this file into relevant folders. 
	* On Mac (and, possibly, Linux): `brew install wget`

## Class

* practical examples of working with `wget`
* single link download
* batch download
	* web-page analysis
	* extraction of links with `regular expressions`
	* modification of links with `regular expressions`

## Sample commands

```
wget link
wget -i file_with_links.txt
wget -i file_with_links.txt -P ./folderYouWantToSaveTo/ -nc 
```

<!--
1. The first command will simply download a file specified in the link;
2. The second command will go over all links given in the `file_with_links.txt` --- in this file links must be stored one per line;
3. The third command also redirects downloaded results to a subfolder (`-p ./folderYouWantToSaveTo/`, where `-p` is an argument that specifies that `./folderYouWantToSaveTo/` is a folder for saved results; `-nc` is another useful argument, which tells `wget` not to download file if is already exists in the folder --- this command allows you to stop and restart downloading from where you stopped);
-->

Where:

* `-P` is a folder parameter, which instructs `wget` where you want to store downloaded files (*optional*).
* `-nc` is a *no-clobber* parameter, which instructs `wget` to skips files, if they already exist (*optional*)

Link examples:

```
https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_01.txt
https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_02.txt
https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_03.txt
```

**NB:** there are many other parameters with which you can adjust `wget` to your needs.

### Issues with `WGET` on Windows

As we have run into a number of issues with trying to run `WGET` on Windows, here are some steps that will help to run it smoothly:

- first of all, `WGET` does not seem to play well with `Powershell`; it does work without any problems via `Command Prompt` (if your Windows “speaks” German, it is called *Eingabeaufforderung*; but if you search for `Command Prompt`, you should still be able to find it.). It also works without any issues via `Git-Bash`. The following steps will make it easier to use with both `Command Prompt` and `Git-Bash`.
- download the `wget.exe` file (from here: <https://eternallybored.org/misc/wget/>);
- copy/paste it into the `C:\Windows\System32` folder (`C:\Windows\System32` is a part of the so-called `PATH` --- a series of paths which all Windows command line tools *check*);
- now, all the commands should be working as expected;

**NB:** There is an alternative to `WGET` on Windows `Powershell`. Here is a detailed tutorial: https://adamtheautomator.com/powershell-download-file/.

### Issues with `WGET` on Mac

It may so happen that `WGET` will not work as intended with the homework assignment (specifically, you may not be able to download all the issues of *Dispatch*). From what I understand this is specifically a Mac issue. Last year the MacOS changed their main command line program from `bash` to `zsh`, and in some cases `WGET` may not work as intended under `zsh`. The solution is, luckily, rather simple: we just need to install `bash` and run `WGET` from `bash`.

- to install `bash`, run `brew install bash`;
- after that you can start `bash` by running `bash` on the command line; the prompt (the beginning of the command line where you type in your commands) should change into something like: `bash-5.1$`
- now you can go to a folder where you want to save your downloaded results and run `WGET`;
- **NB:** you will need `bash` only for this step; after you restart the `Terminal`, you will be back to the default `zsh` (you can also run `exit` command to quit `bash` and return to `zsh`).

## A sidenote on issues in general

Keep in mind that one of the most important things that you need to learn in this course is that there are multiple solutions to most of the problems and tasks that you may face and the most common way to solve your problem is to break is down into smaller tasks (remember, there was a detailed discussion of this in Zelle’s book), and then look for efficient solutions to each step. No matter how advanced you are, “googling” will be the major way of finding suitable solutions.

## Practicing Scraping

The following sections give you some examples of links that you are most likely to encounter in real life. Your task is to figure out how to prepare lists of links (URLs) for downloading with `WGET`. Your first step will be to look under the hood of the current page, which you can do right clicking on the page and selecting something that looks like “View page source” (in Chrome or Edge) or “Show Page Source” (in Safari; you will need to enable “Show Develop menu in menu bar” in Preferences > Advanced). Now, looking at the HTML code of the page you can find the actual URLs, which you can then extract from the HTML code with regular expressions (one thing you can do is to `copy/paste` the entire code of the page into a text editor that supports regular expressions---like *Sublime Text*). 

## Practice 1: very easy

* [Article 01](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_01.txt)
* [Article 02](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_02.txt)
* [Article 03](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_03.txt)
* [Article 04](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_04.txt)
* [Article 05](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_05.txt)
* [Article 06](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_06.txt)
* [Article 07](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_07.txt)
* [Article 08](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_08.txt)
* [Article 09](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_09.txt)
* [Article 10](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_10.txt)
* [Article 11](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_11.txt)
* [Article 12](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_12.txt)
* [Article 13](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_13.txt)
* [Article 14](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_14.txt)
* [Article 15](https://maximromanov.github.io/dh_in_mes/files/articles/1860-11-12_article_15.txt)

<!--
* [Article 01](./files/articles/1860-11-12_article_01.txt)
* [Article 02](./files/articles/1860-11-12_article_02.txt)
* [Article 03](./files/articles/1860-11-12_article_03.txt)
* [Article 04](./files/articles/1860-11-12_article_04.txt)
* [Article 05](./files/articles/1860-11-12_article_05.txt)
* [Article 06](./files/articles/1860-11-12_article_06.txt)
* [Article 07](./files/articles/1860-11-12_article_07.txt)
* [Article 08](./files/articles/1860-11-12_article_08.txt)
* [Article 09](./files/articles/1860-11-12_article_09.txt)
* [Article 10](./files/articles/1860-11-12_article_10.txt)
* [Article 11](./files/articles/1860-11-12_article_11.txt)
* [Article 12](./files/articles/1860-11-12_article_12.txt)
* [Article 13](./files/articles/1860-11-12_article_13.txt)
* [Article 14](./files/articles/1860-11-12_article_14.txt)
* [Article 15](./files/articles/1860-11-12_article_15.txt)
-->

## Practice 2: easy-ish

* [Article 16](./files/articles/1860-11-12_article_16.txt)
* [Article 17](./files/articles/1860-11-12_article_17.txt)
* [Article 18](./files/articles/1860-11-12_article_18.txt)
* [Article 19](./files/articles/1860-11-12_article_19.txt)
* [Article 20](./files/articles/1860-11-12_article_20.txt)
* [Article 21](./files/articles/1860-11-12_article_21.txt)
* [Article 22](./files/articles/1860-11-12_article_22.txt)
* [Article 23](./files/articles/1860-11-12_article_23.txt)
* [Article 24](./files/articles/1860-11-12_article_24.txt)
* [Article 25](./files/articles/1860-11-12_article_25.txt)
* [Article 26](./files/articles/1860-11-12_article_26.txt)
* [Article 27](./files/articles/1860-11-12_article_27.txt)
* [Article 28](./files/articles/1860-11-12_article_28.txt)
* [Article 29](./files/articles/1860-11-12_article_29.txt)
* [Article 30](./files/articles/1860-11-12_article_30.txt)
* [Article 31](./files/articles/1860-11-12_article_31.txt)
* [Article 32](./files/articles/1860-11-12_article_32.txt)
* [Article 33](./files/articles/1860-11-12_article_33.txt)
* [Article 34](./files/articles/1860-11-12_article_34.txt)
* [Article 35](./files/articles/1860-11-12_article_35.txt)
* [Article 36](./files/articles/1860-11-12_article_36.txt)
* [Article 37](./files/articles/1860-11-12_article_37.txt)
* [Article 38](./files/articles/1860-11-12_article_38.txt)
* [Article 39](./files/articles/1860-11-12_article_39.txt)

## Practice 3 (aka *Homework*): a tiny-bit tricky

* download issues of “Richmond Times Dispatch” (Years 1860-1865, only!), which are available at: <http://www.perseus.tufts.edu/hopper/collection?collection=Perseus:collection:RichTimes>)

## Reference Materials{#RML06}

* Milligan, Ian. 2012. “Automated Downloading with Wget.” Programming Historian, June. <https://programminghistorian.org/lessons/automated-downloading-with-wget>.
* Kurschinski, Kellen. 2013. “Applied Archival Downloading with Wget.” Programming Historian, September. <https://programminghistorian.org/lessons/applied-archival-downloading-with-wget>.
* Baxter, Richard. 2019. “How to download your website using WGET for Windows.” <https://builtvisible.com/download-your-website-with-wget/>.
* Alternatively, this operation can be done with a Python script: Turkel, William J., and Adam Crymble. 2012. “Downloading Web Pages with Python.” Programming Historian, July. <https://programminghistorian.org/lessons/working-with-web-pages>.

## Homework{#HWL06}

1. Scraping the “Dispatch”: download issues of “Richmond Times Dispatch” (Years 1860-1865, only!), which are available at: <http://www.perseus.tufts.edu/hopper/collection?collection=Perseus:collection:RichTimes>)
2. In a separate markdown file, describe your steps of how you completed this task (to be uloaded with the rest of your homework).

**Python**

- Work through Chapters 8 and 11 of Zelle's book; read chapters carefully; work through the chapter summaries and exercises; complete the following programming exercises: 1-8 in Chapter 8 and 1-11 in Chapter 11;
- Watch [Dr. Vierthaler's videos](https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17):
	- Episode 12: Functions
	- Episode 13: Libraries and NLTK
	- Episode 14: Regular Expressions
- **Note:** the sequences are somewhat different in Zelle's textbook and Vierthaler's videos. I would recommend you to always check Vierthaler's videos and also check videos which cover topics that you read about in Zelle's book.

## Submitting homework{#SHL06}

* Homework assignment must be submitted by the beginning of the next class;
* Email your homework to the instructor.
	* if your homework is to create a file, email it as an attachment
	* if your homework is a blogpost on your website, email the link to your website and to the blogpost with your homework.
	*  In the subject of your email, please, add the following: `070112-LXX-HW-YourLastName-YourMatriculationNumber`, where `LXX` is the lesson for which the homework is submitted, `YourLastName` is your last name, and `YourMatriculationNumber` is your matriculation number.

<!--chapter:end:02-Lesson06.Rmd-->

# Lesson 07: Understanding Structured Data

## Major Data Formats

Doing Digital Humanities practically always means working with structured data of some kind. In most general terms, structured data means some explicit annotation or classification that the machine can understand, and therefore --- effectively use. When we see the word “Berlin”, we are likely to automatically assume that this is the name of the capital of Germany The machine cannot know that, unless there is something else in the data that allows it to figure it out (here, an XML tag): `<settlement country="Germany" type="capital city">Berlin</settlement>` --- from this *annotation* (and its attributes) the machine can be instructed to interpret the string `Berlin` as a *settlement* of the type *capital city* in the country of *Germany*. It is important to understand most common data formats in order to be able to create and generate them as well as to convert between different formats.

When we decide which format we want to work with, we need to consider the following: the ease of working with a given format (manual editing); suitability for specific analytic software; human-friendliness and readability; open vs. proprietary. In general, it does not make any sense to engage in *the format wars* (i.e., one format is better than another); one should rather develop an understanding that almost every format has its use and value in specific contexts or for specific tasks. What we also want is not to stick to a specific format and try to do everything with it and only it, but rather to be able to write scripts with which we can generate data in suitable formats or convert our data from one format into another.

Let's take a look at the same example in some of the most common formats.

### XML (*Extensible Markup Language*)

``` xml
<note>
  <to>Tove</to>
  <from>Jani</from>
  <heading>Reminder</heading>
  <body>Don’t forget me this weekend!</body>
</note>
```

### CSV/TSV (*Comma-Separated Values*/ *Tab-Separated Values*)

```
to,from,heading,body
Tove,Jani,Reminder,Don’t forget me this weekend!
```

``` cs
to,from,heading,body
"Tove","Jani","Reminder","Don’t forget me this weekend!"
```

### JSON (*JavaScript Object Notation*)

``` json
{
  "to": "Tove",
  "from": "Jani",
  "heading": "Reminder",
  "body": "Don’t forget me this weekend!"
}
```

### YML or YAML (*Yet Another Markup Language* > *YAML Ain't Markup Language*)

``` yml
to: Tove
from: Jani
heading: Reminder
body: Don’t forget me this weekend
```

### `BibTeX`: *most common bibliographic format*

We have already used this format in our lesson on sustainable writing. If you take a closer look at the record below, you may see that this format contains lots of valuable information. Most of this we will need for our project.

```bibtex
@incollection{LuhmannKommunikation1982,
  title = {Kommunikation mit Zettelkiisten},
  booktitle = {Öffentliche Meinung und sozialer Wandel: Für Elisabeth Noelle-Neumann = Public opinion and social change},
  author = {Luhmann, Niklas},
  editor = {Baier, Horst and Noelle-Neumann, Elisabeth},
  date = {1982},
  pages = {222--228},
  publisher = {{westdt. Verl}},
  location = {{Opladen}},
  annotation = {OCLC: 256417947},
  file = {Absolute/Path/To/PDF/Luhmann 1982 - Kommunikation mit Zettelkiisten.pdf},
  isbn = {978-3-531-11533-7},
  langid = {german}
}
```

## Larger Examples

<!-- **NB** data example from [here](https://gist.github.com/Miserlou/c5cd8364bf9b2420bb29). -->

In most cases, if you do data analysis, you will need formats that allow you to store multiple items. So, let's take a look at some most commonly used options. (**NB**: In some cases, you may still want to opt for a format that stores a single item per file; this may be the case when single items are rather large and it may make sense to keep them as separate files, especially if you need to work more closely with each item --- ready closely, annotate, edit, etc.)

### `CSV` / `TSV`

```
city,growth_from_2000_to_2013,latitude,longitude,population,rank,state
New York,4.8%,40.7127837,-74.0059413,8405837,1,New York
Los Angeles,4.8%,34.0522342,-118.2436849,3884307,2,California
Chicago,-6.1%,41.8781136,-87.6297982,2718782,3,Illinois
```

`TSV` is a better option than a `CSV`, since `TAB` characters are very unlikely to appear in values.

Neither `TSV` not `CSV` are good for preserving *new line characters* (`\n`)---or, in other words, text split into multiple lines. As a workaround, one can convert `\n` into some unlikely-to-occur character combination (for example, `;;;`), which would allow to restore `\n` later , if necessary.

### `JSON`

``` json
[
    {
        "city": "New York", 
        "growth_from_2000_to_2013": "4.8%", 
        "latitude": 40.7127837, 
        "longitude": -74.0059413, 
        "population": "8405837", 
        "rank": "1", 
        "state": "New York"
    }, 
    {
        "city": "Los Angeles", 
        "growth_from_2000_to_2013": "4.8%", 
        "latitude": 34.0522342, 
        "longitude": -118.2436849, 
        "population": "3884307", 
        "rank": "2", 
        "state": "California"
    }, 
    {
        "city": "Chicago", 
        "growth_from_2000_to_2013": "-6.1%", 
        "latitude": 41.8781136, 
        "longitude": -87.6297982, 
        "population": "2718782", 
        "rank": "3", 
        "state": "Illinois"
    }
]
```


### `YML`/`YAML`

YAML is often used only for a single set of parameters.

``` yml
city: New York 
growth_from_2000_to_2013: 4.8% 
latitude: 40.7127837 
longitude: -74.0059413
population: 8405837 
rank: 1 
state: New York
```

But it can also be used for storage of serialized data. It has advantages of both JSON and CSV: the overall simplicity of the format (no tricky syntax) is similar to that of CSV/TSV, but it is more readable than CSV/TSV in any text editor, and is more difficult to break—again, due to the simplicity of the format.

``` yml
New York:
  growth_from_2000_to_2013: 4.8% 
  latitude: 40.7127837 
  longitude: -74.0059413 
  population: 8405837 
  rank: 1 
  state: New York 
Los Angeles:
  growth_from_2000_to_2013: 4.8% 
  latitude: 34.0522342 
  longitude: -118.2436849 
  population: 3884307 
  rank: 2 
  state: California
Chicago:
  growth_from_2000_to_2013: -6.1% 
  latitude: 41.8781136 
  longitude: -87.6297982 
  population: 2718782 
  rank: 3 
  state: Illinois
```

YAML files can be read with Python into `dictionaries` like so:

```py
import yaml
dictionary = yaml.load(open(pathToFile))
```

You will most likely need to install `yaml` library; it is also quite easy to write a script that would read such serialized data. (`yaml` module may not work on later versions of `python`.)

### Installing libraries for `python`

In general, it should be as easy as running the following command in your command line tool:

```
pip install --upgrade libraryName
```

- `pip` is the standard package installer for `python`; if you are running version 3.xx of `python`, it may be `pip3` instead of `pip`. If you have Anaconda installed, you can also use Anaconda interface to install packages;
- `install` is the command to install a package that you need;
- `--upgrade` is an optional argument that you would need only when you upgrade already installed package;
- `libraryName` is the name of the library that you want to install.

This should work just fine, but sometimes it does not---usually when you have multiple versions of `python` installed and they may start conflicting with each other (another good reason to handle your `python` installations via Anaconda). There is, luckily, a workaround that seems to do the trick. You can modify your command in the following manner:

```
python -m pip install --upgrade libraryName
```

- `python` here is whatever alias you are using for running `python`. If you are on Mac, `python` is installed with the original MacOS setup and `python` command remains reserved for the erlier versions of `python` (usually, 2.x). If you installde the latest version of `python`, it will be some 3.x version and the default command to run it on your Mac will be `python3`, so the full command will look: `python3 -m pip install --upgrade libraryName`)

## In-Class Practice (and homework)

Let's try convert this [csv file with geographical data on the medieval Islamic world](./files/settlements.csv) into all the above discussed formats.

This is a TSV file with the following structure:

```
settlement_id	languages	names_ara_common	names_ara_common_other	names_eng_search	names_eng_translit	names_eng_translit_other	region_URI	source	top_type	coordinates
QAHIRA_312E300N_S	['ara', 'eng']	القاهرة	القاهرة	Qahira, Cairo	al-Qāhiraŧ	al-Madīnaŧ al-Qāhiraŧ	Misr_RE	maximromanov	metropoles	[31.2357, 30.0444]
IRBIL_440E361N_S	['ara', 'eng']	إربيل	إربيل	Irbil, Erbil	Irbīl	Irbīl	Aqur_RE	maximromanov	towns	[44.009085, 36.191231]
DANIYA_001E388N_S	['ara', 'eng']	دانية	دانية	Daniya, Dénia	Dāniyaŧ	Dāniyaŧ	Andalus_RE	maximromanov	towns	[0.105056, 38.838799]
WASHQA_003W421N_R	['ara', 'eng']	وشقة	وشقة	Washqa	Wašqaŧ	Wašqaŧ	Andalus_RE	cornuData	regions	[-0.35371, 42.16109]
WASHQA_003W421N_S	['ara', 'eng']	وشقة	وشقة	Washqa	Wašqaŧ	Wašqaŧ	Andalus_RE	cornuData	towns	[-0.35371, 42.16109]
BALANSIYYA_004W394N_R	['ara', 'eng']	بلنسية	بلنسية	Balansiyya	Balansiyyaŧ	Balansiyyaŧ	Andalus_RE	cornuData	regions	[-0.41486, 39.43516]
BALANSIYYA_004W394N_S	['ara', 'eng']	بلنسية	بلنسية	Balansiyya	Balansiyyaŧ	Balansiyyaŧ	Andalus_RE	cornuData	towns	[-0.41486, 39.43516]
SHAQR_004W391N_S	['ara', 'eng']	الشقر	الشقر	al-Shaqr	al-Šaqr	al-Šaqr	Andalus_RE	cornuData	villages	[-0.43734, 39.16483]
QANT_004W383N_S	['ara', 'eng']	قانت	قانت	Qant	Qānt	Qānt	Andalus_RE	cornuData	towns	[-0.47061, 38.34618]
```

You need to convert it into:

- XML (here, you will need to come up with an format for your XML; use the very first example given in the lesson as your template);
- CSV/TSV;
- JSON;
- YML;

Suggestions:

- start with some pseudo code: what are the steps into which you can break this operation?
- using a `dictionary` may help a lot;
- for your solutions, you are welcome to look for "easy" ways to convert these files (like online converters that convert from one format into another) --- add those into your solution. But you also need to write scripts that convert from one format into another. 

## Homework{#HWL07}

- Finish the conversion task;
  - *Hint*: loading the TSV file into a `dictionary` may be a good step to start with;
  - upload your results together with scripts to your homework github repository;
  - send me an email with a link to these files

**Python**

- Make sure to finish the last assignment from Zelle's book: work through Chapters 8 and 11 of Zelle's book; read chapters carefully; work through the chapter summaries and exercises; complete the following programming exercises: 1-8 in Chapter 8 and 1-11 in Chapter 11;
- And watch [Dr. Vierthaler's videos](https://www.youtube.com/playlist?list=PL6kqrM2i6BPIpEF5yHPNkYhjHm-FYWh17), if you have not done that already:
	- Episode 12: Functions
	- Episode 13: Libraries and NLTK
	- Episode 14: Regular Expressions
- **Note:** the sequences are somewhat different in Zelle's textbook and Vierthaler's videos. I would recommend you to always check Vierthaler's videos and also check videos which cover topics that you read about in Zelle's book.

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfolder in your repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)


## Solution{#SolutionL07}

Below is the solution to the homework: three functions that convert a TSV file into XML, YML, and JSON. Your solutions may be different, but they are considered correct as long as your results are what they must be.

```{r engine='python', highlight=TRUE, eval=FALSE}

import csv
import json

# delim should be either "\t" for TSV  or "," for CSV
def converter_tsv_to_json(file, delim):
    with open(file) as f1:
        reader = csv.DictReader(f1, delimiter=delim)
        settlements = {}
        for row in reader:
            settlements[row["settlement_id"]] = row
    with open(file.replace(".csv", ".json"), "w") as f9:
        json.dump(settlements, f9, indent=4, ensure_ascii=False)

def converter_tsv_to_yml(file):
    with open(file, "r", encoding="utf8") as f1:
        data = f1.read().strip().split("\n")
        header = data[0].split("\t")
        allData = []
        for d in data[1:]:
            temp = d.split("\t")
            tempVar = [temp[0]+":"]
            for i in range(0, len(header)):
                item = "\t%s: %s" % (header[i], temp[i])
                tempVar.append(item)
            tempVarFinal = "\n".join(tempVar)
            allData.append(tempVarFinal)
    ReallyFinalData = "\n\n".join(allData)
    with open(file.replace(".csv", ".yml"), "w", encoding="utf8") as f9:
        f9.write(ReallyFinalData)

def converter_tsv_to_xml(file):
    with open(file) as f1:
        reader = csv.DictReader(f1, delimiter="\t")
        data = []
        for row in reader:
            temp = []
            for k, v in row.items():
                temp.append("<%s>%s</%s>" % (k, v, k))
            tempComplete = "<settlement>\n\t%s\n</settlement>" % "\n\t".join(
                temp)
            data.append(tempComplete)
    ReallyFinalData = "\n\n".join(data)
    with open(file.replace(".csv", ".xml"), "w", encoding="utf8") as f9:
        f9.write(ReallyFinalData)

converter_tsv_to_json("settlements.csv", "\t")
converter_tsv_to_yml("settlements.csv")
converter_tsv_to_xml("settlements.csv")
```

<!--chapter:end:02-Lesson07.Rmd-->

# Lesson 08: Converting the Dispatch

## Original XML files analysis

- analyze structure and identify main structural elements;
- extract main structural units (articles);
- extract and generate additional metadata elements:
  - date;
  - article ID;
  - header/title;
  - texts.

## Convert to a cleaner format

- what format would be best for this kind of data? (no single correct answer; any answer must be substantiated);
- possible formats:
  - a simple XML;
  - JSON;
  - YML;
  - CSV / TSV;
  - other formats.

## In-Class Practice (and homework)

Let's start working on the conversion of our initial data into other formats. (*Suggestion*: start with some pseudo code: what are the steps into which you can break this operation?)

## Homework{#HWL08}

- Finish the conversion task;
- Annotate your script (i.e., add comment to every line of code describing what is happenning there);

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfolder in your repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)


## Solution{#SolutionL08}

Below is the solution to the homework: all issues of the Dispatch (stored in `./Dispatch/`) are converted into `YML` and saved into a different folder (`./Dispatch_Processed/`).

```{r engine='python', highlight=TRUE, eval=FALSE}

import re
import os

source = "./Dispatch/"
target = "./Dispatch_Processed/"  # needs to be created beforehand!

lof = os.listdir(source)
counter = 0  # general counter to keep track of the progress

for f in lof:
    if f.startswith("dltext"):  # fileName test
        newF = f.split(":")[-1] + ".yml"  # in fact, yml-like

        issueVar = []
        with open(source + f, "r", encoding="utf8") as f1:
            text = f1.read()
            date = re.search(r'<date value="([\d-]+)"', text).group(1)
            split = re.split("<div3 ", text)

            for s in split[1:]:
                s = "<div3 " + s  # a step to restore the integrity of each item

                try:
                    unitType = re.search(r'type="([^\"]+)"', s).group(1)
                except:
                    unitType = "noType"

                try:
                    header = re.search(r'<head.*</head>', s).group(0)
                    header = re.sub("<[^<]+>", "", header)

                except:
                    header = "NO HEADER"

                text = s
                text = re.sub("<[^<]+>", " ", text)
                text = re.sub(" +\n|\n +", "\n", text)
                text = text.strip()
                text = re.sub("\n+", ";;; ", text)
                text = re.sub(" +", " ", text)
                text = re.sub(r" ([\.,:;!])", r"\1", text)

                itemID = "ID: " + date + "_" + unitType + "_%03d" % c

                if len(re.sub("\W+", "", text)) != 0:
                    dateVar = "DATE: " + date
                    unitType = "TYPE: " + unitType
                    header = "HEADER: " + header
                    # @§@ is used to replace ":", because in YML : is used
                    # as a divider between the key and value
                    text = "TEXT: " + text.replace(":", "@§@") + "\n\n"
                    var = "\n".join([itemID, dateVar, unitType, header, text])

                    issueVar.append(var)

        issueNew = "".join(issueVar)
        with open(target + newF, "w", encoding="utf8") as f9:
            f9.write(issueNew)

        counter += 1
        if counter % 100 == 0:
            print(counter)


```

<!--chapter:end:02-Lesson08.Rmd-->

# Lesson 09: Extracting Tagged Data for Analysis

## Concept of *tidy data*

* Each variable is in its own column
* Each observation is in its own row
* Each value is in its own cell

(**NB:** Additionally, data must be normalized, i.e. values in the same columns must be in the same format: if length, all in inches or centimeters; if weight, all in pounds or kilos; etc. It does not matter what units are used; the important part is that the same units are used throughout.)

![*Source:* Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. Sebastopol, CA: O’Reilly UK Ltd. <https://r4ds.had.co.nz/>; for a Chapter on **tidy data**, see: <https://r4ds.had.co.nz/tidy-data.html>.](https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png)

## Extracting tagged entities

*Why?* We can analyze tagged entities as they feature across time and space in the coverage of the Dispatch (again, for all intents and purposes, we can use this newspaper as an equivalent of a chronicle, or even broader, as that of a chronological corpus.)

*How?* Different types of entities are already tagged in the text and we can use this tagging to make abstractions of each article. These abstractions is what we will then use in our initial simple analysis. In order to extract tagged data, we first need to understand what we can extract. This can be done by creating a frequency list of all XML tags used in the issues of the Dispatch. These results will help us to understand what kind of data we can use for analysis. The following script counts all the tags and saves results into a TSV format organized from the most frequent to the least frequent.

```{r engine='python', highlight=TRUE, eval=FALSE}

import re
import os

source = "./Dispatch/"
target = "./Dispatch_Processed/"  # needs to be created beforehand!

lof = os.listdir(source)

resDic = {}

for f in lof:
    if f.startswith("dltext"):  # fileName test
        newF = f.split(":")[-1] + ".yml"  # in fact, yml-like

        # collect and count all XML tags

        issueVar = []
        c = 0  # technical counter
        with open(source + f, "r", encoding="utf8") as f1:
            text = f1.read()

            for i in re.findall(r"(<\w+)", text):
                # print(i)

                if i in resDic:
                    resDic[i] += 1
                else:
                    resDic[i] = 1

final = []
for k, v in resDic.items():
    value = "%010d\t%s" % (v, k)
    final.append(value)
    # input(value)

sortedResults = sorted(final, reverse=True)
finalResults = "\n".join(sortedResults)
with open("tag_results.csv", "w", encoding="utf8") as f9:
    f9.write(finalResults)

print("Done!")

```

The results will look in the following manner (with some ommissions to save space):

```
0000907398	<milestone
0000649103	<p
0000552453	<persName
0000526235	<surname
0000446296	<head
0000402419	<placeName
0000370390	<num
0000353521	<rs
0000342807	<div3
0000325316	<foreName
0000197514	<roleName
0000170166	<measure
0000164988	<orgName
0000106719	<hi
...
0000017332	<div1
0000015210	<cit
0000013816	<opener
...
0000002571	<table
0000002445	<sic
0000002278	<language
...
0000001349	<text
0000001349	<teiHeader
0000001349	<taxonomy
...
0000000016	<dateRange
0000000013	<div6
0000000001	<div7
```

These results suggest that the following tags can be useful, as they have high frequencies and carry meaningful data:

- `<persName>` (552,453);
- `<placeName>` (402,419);
- `<orgName>` (164,988);

(`<rs>` is another frequent tag with potentially valuable information (353,521). These tagged entities, however, lack additional metadata, which will make their analysis more complicated, so we will skip it. On the `<rs>` tag see </https://tei-c.org/release/doc/tei-p5-doc/en/html/ref-rs.html/>.)

Next step will be to understand the structure of these selected tags and process them accordingly, aggregating results into a tidy format. We can build on the script from the previous lesson, to which we will need to add some modifications. This is your homework :)

Our tidy results should look similar to what you see below:

```{r echo=FALSE, results="asis"}
library(knitr)
data<-read.table(file = './files/entities.csv', sep = '\t', header = TRUE)
kable(data)
```



The structure of this data is as follows:

- `articleID` is the ID of each article;
- `date` is the date when the article was published (the same date effectively refers to the same issue);
- `itemType`, `itemUnified`, `itemId`: these following three columns represent each tagged entity. In most cases each entity can be described with a single or multiple variables. A single variable can be used when it represents some kind of unique identifier from some external database/databank. such unique identifiers can be used to collect additional information on our entities from these external databases/databanks. For example, for `<placeName>` entities it could be enough to use the `key=` attribute which contains unique identifiers (for example, `tgn,2111971`) from the Getty Thesaurus of Geographical Names (<http://tgndownloads.getty.edu/default.aspx>). Unfortunately, such data is not available more often than it is. I decided to use the following three elements (ideally trying to preserve (1) the type of an entity (`itemType`), (2) how it appears in the text (*skipped*), (3) its unified/normalized form (`itemUnified`, not available for all types); and (4) its unique identifier (`itemId`, also not available for all types)). 

## Homework{#HWL09}

- Finish the assigned task;
- Annotate your script (i.e., add comment to every line of code describing what is happenning there);

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfolder in your repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)


## Solution{#SolutionL09}

Below is the solution to the homework: all issues of the Dispatch (stored in `./Dispatch/`) are converted into `YML` and saved into a different folder (`./Dispatch_Processed/`). This is essentially the script from the previous lesson, to which additional lines of code have been added (after `# NEW PART`).

```{r engine='python', highlight=TRUE, eval=FALSE}
import re
import os

source = "./Dispatch/"
target = "./Dispatch_Processed/"  # needs to be created beforehand!

lof = os.listdir(source)
counter = 0  # general counter to keep track of the progress

entities = []  # we will collect all extracted data here

for f in lof:
    if f.startswith("dltext"):  # fileName test
        newF = f.split(":")[-1] + ".yml"  # in fact, yml-like

        issueVar = []
        c = 0  # technical counter
        with open(source + f, "r", encoding="utf8") as f1:
            text = f1.read()
            date = re.search(r'<date value="([\d-]+)"', text).group(1)
            split = re.split("<div3 ", text)

            for s in split[1:]:
                c += 1
                s = "<div3 " + s  # a step to restore the integrity of items

                try:
                    unitType = re.search(r'type="([^\"]+)"', s).group(1)
                except:
                    unitType = "noType"

                try:
                    header = re.search(r'<head.*</head>', s).group(0)
                    header = re.sub("<[^<]+>", "", header)

                except:
                    header = "NO HEADER"

                text = s
                text = re.sub("<[^<]+>", " ", text)
                text = re.sub(" +\n|\n +", "\n", text)
                text = text.strip()
                text = re.sub("\n+", ";;; ", text)
                text = re.sub(" +", " ", text)
                text = re.sub(r" ([\.,:;!])", r"\1", text)

                itemID = date + "_" + unitType + "_%03d" % c

                if len(re.sub("\W+", "", text)) != 0:
                    itemIdvar = "ID: " + itemID
                    dateVar = "DATE: " + date
                    unitType = "TYPE: " + unitType
                    header = "HEADER: " + header
                    # @§@ is used to replace ":", because in YML : is used as a divider between the key and value
                    text = "TEXT: " + text.replace(":", "@§@") + "\n\n"
                    var = "\n".join(
                        [itemIdvar, dateVar, unitType, header, text])

                    issueVar.append(var)

                    # NOW, WE CAN ADD SOME CODE TO PROCESS EACH ITEM AND COLLECT ALL INTO OUR TIDY DATA FORMAT
                    # STRUCTURE: itemID, dateVar, EXTRACTED_ITEM (type, unified_form, id)
                    # ADDING TO: entities (list)

                    for i in re.findall(r"(<\w+[^>]+>)", s):
                        if "persName" in i and "authname" in i and "n=" in i:
                            # input(i)
                            itemType = "persName"
                            itemUnified = re.search(r'n="([^"]+)"', i).group(1)
                            itemId = re.search(
                                r'authname="([^"]+)"', i).group(1)

                            tempVar = "\t".join(
                                [itemID, date, itemType, itemUnified, itemId])
                            entities.append(tempVar)

                        elif "placeName" in i and "authname" in i and "reg=" in i:
                            # input(i)
                            itemType = "placeName"
                            itemUnified = re.search(
                                r'reg="([^"]*)"', i).group(1)
                            itemId = re.search(
                                r'authname="([^"]+)"', i).group(1)

                            tempVar = "\t".join(
                                [itemID, date, itemType, itemUnified, itemId])
                            entities.append(tempVar)

                        elif "orgName" in i and "type" in i and "n=" in i:
                            # print(i)
                            itemType = "orgName"
                            itemUnified = re.search(r'n="([^"]+)"', i).group(1)
                            itemId = re.search(
                                r'type="([^"]+)"', i).group(1)

                            tempVar = "\t".join(
                                [itemID, date, itemType, itemUnified, itemId])
                            entities.append(tempVar)

                        else:
                            pass

        issueNew = "".join(issueVar)
        with open(target + newF, "w", encoding="utf8") as f9:
            f9.write(issueNew)

        # count processed issues and print progress counter at every 100
        counter += 1  # counter = counter + 1
        # if counter is divisible by 100 (i.e., no remainder), then print it
        if counter % 100 == 0:
            print(counter)

header = "\t".join(["articleID", "date", "itemType", "itemUnified", "itemID"])
entitiesFinal = header + "\n" + "\n".join(entities).lower()
with open("entities.csv", "w", encoding="utf8") as f9:
    f9.write(entitiesFinal)

print("Done!")
```

<!--chapter:end:02-Lesson09.Rmd-->

# Lesson 10: Graphing Chronological Distribution of Tagged Entities

## Abstractions of Newspaper Articles

In the previous lesson we created *abstractions* of our newspaper articles. Namely, we reduced the text in natural languages to mentions of places, persons, and organizations. Together, they give us an idea of the main actors (persons and organizations), i.e. who is involved in some developments described in each article, and main locations (placenames) of where these developments took place.

```{yml}
ID: 1864-04-28_article_002
DATE: 1864-04-28
TYPE: article
HEADER: Yankee rule in Plymouth.
TEXT: Yankee rule in Plymouth.<br> The following orders are copies of hand-bills
  posted in the town of Plymouth.<br> It will be seen that Brig. Gen. Wessels is
  a model after Lincoln 's own heart, and undertook to "run the churches" and
  the schools besides.<br> As we find the names of the General and the Provost
  Marshal, and the A. A., G.'s on the register of the Libby Hotel, in this city,
  it is more than likely that the children "between eight and fourteen" in
  Plymouth are having a cheerful vacation, and that Col. Moffitt will refrain
  for the present from the disagreeable duty of reporting the derelict heads of
  families who don't enforce their attendance.<br> This is the school order<br>
  Notice.<br> The inhabitants of Plymouth are hereby notified that a Free school,
  for white children, will be spend under competent teachers,<br> On Monday, 18th
  inst,<br> in the Episcopal Church.<br> The attention of parents and guardians
  is called in this important subject; and it is expected that all children
  between eight and fourteen years of age will attend the school.<br> Those over
  fourteen may attend if they wish.<br> Lieut. Col. Moffitt, Provost Marshal,
  will institute careful inquiries, and report such families as neglect to avail
  themselves of the advantages thus offered. By command of Brig. Gen. H. W.
  Wessels, Andrew Stewart, Assistant Adjutant General. Plymouth, N. C., April
  14th, 1864.<br> And this is the order for running the churches<br> Notice<br>
  Until further orders church call will be sounded at the Provost Guard on
  Sundays, at fifteen minutes before 11 A. M., and at 2.15 P. M. the call to be
  repeated promptly by the drums of the several regiments and detachments.<br>
  The annoyance caused by entering and leaving the churches during the
  performance of Divine service, and by the practice of spitting on the floor
  is excessive, and it is hoped that these evils will be corrected without the
  necessity of individual reproof. By order of Brig. Gen. H. W. Wessels, D. F.
  Beegle, Lieut, A. D. C. & A. A. A. G. Plymouth, N. C, April 11th, 1864.
```

Thus, the article above (`1864-04-28_article_002`) can be abstracted to the following data (here, based exclusively on entities already tagged in the initial XML documents):

```{r echo=FALSE, results="asis"}
library(knitr)
data<-read.table(file = './files/entities.csv', sep = '\t', header = TRUE)
kable(data)
```

Such abstractions can never fully replace the *close reading* of text, but they open up possibilities of *distant reading* of the entire corpus of our newspaper. For example, we can measure the frequencies of mentions of specific entities over time in order to assess their prominence and importance across the entire period as well as to identify specific moments in time, when they (people, places, organizations/institutions) soared to some level of importance. Additionally, we can also assess how traditional reading can be reinforces or informed by distant reading. Let's now take a look at a couple of simple examples. (After which we will take a look at the code and you will get an assignment to modify it.)

*Example 1:* General Sherman and the burning of Atlanta

> William Tecumseh Sherman was a Union General serving under the command of Ulysses Grant during the Civil War. He is most known for his campaign through Georgia and the Carolinas in 1864 where he followed a scorched earth policy including the capture and burning of Atlanta. During the Grant Presidency, Sherman became Commanding General of the Army, formulating the military response to the conflict with Indian tribes in the west.

> [*Source*: Lloyd Sealy Library --- Key Personolities of the Civil War](https://guides.lib.jjay.cuny.edu/c.php?g=288398&p=4496619)

The two graphs below show mentions of both `sherman` and `atlanta` in the Dispatch issues: there we can see/show how the prominence of William Sherman went up in the course of his campaign in 1864, while the spike in mentions of Atlanta must reflect the capture and burning of the city. 

![](./files/graphs/plot1_data_sherman.png)
![](./files/graphs/plot1_data_atlanta.png)

*Example 2.* The Battle of Shiloh (April 6–7, 1862)

![](./files/plot_shiloh_shiloh.png)

> The Battle of **Shiloh** (also known as the Battle of Pittsburg Landing) was an early battle in the Western Theater of the American Civil War, **fought April 6–7, 1862**, in southwestern Tennessee. The Union Army of the Tennessee (Major General Ulysses S. **Grant**) had moved via the Tennessee River deep into Tennessee and was encamped principally at Pittsburg Landing on the west bank of the Tennessee River, where the Confederate Army of Mississippi (General Albert Sidney **Johnston**, P. G. T. **Beauregard** second-in-command) launched a surprise attack on Grant's army from its base in Corinth, Mississippi. *Johnston was mortally wounded during the fighting*; Beauregard took command of the army and decided against pressing the attack late in the evening. Overnight, Grant was reinforced by one of his divisions stationed farther north and was joined by three divisions from the Army of the Ohio (Maj. Gen. Don Carlos **Buell**). The Union forces began an unexpected counterattack the next morning which reversed the Confederate gains of the previous day. On April 6, the first day of the battle, the Confederates struck with the intention of driving the Union defenders away from the river and into the swamps of Owl Creek to the west. Johnston hoped to defeat Grant's army before the anticipated arrival of Buell and the Army of the Ohio. The Confederate battle lines became confused during the fighting, and Grant's men instead fell back to the northeast, in the direction of Pittsburg Landing. A Union position on a slightly sunken road, nicknamed the "Hornet's Nest" and defended by the divisions of Brig. Gens. Benjamin **Prentiss** and William H. L. **Wallace**, provided time for the remainder of the Union line to stabilize under the protection of numerous artillery batteries. *Wallace was mortally wounded when the position collapsed*, while several regiments from the two divisions were eventually surrounded and surrendered. Johnston was shot in the leg and bled to death while leading an attack. Beauregard acknowledged how tired the army was from the day's exertions, and decided against assaulting the final Union position that night. Tired but unfought and well-organized men from Buell's army and a division of Grant's army arrived in the evening of April 6 and helped turn the tide the next morning, when the Union commanders launched a counterattack along the entire line. Confederate forces were forced to retreat, ending their hopes of blocking the Union advance into northern Mississippi. Though victorious, the Union army had suffered heavier casualties than the Confederates, and Grant was heavily criticized in the media for being taken by surprise. The Battle of Shiloh was *the bloodiest engagement of the Civil War up to that point*, with nearly twice as many casualties as the previous major battles of the war combined.

> *Source:* <https://en.wikipedia.org/wiki/Battle_of_Shiloh>.

Now, let's take a look at the graphs for the names highlighted in bold above:

![](./files/graphs/plot1_shiloh_beauregard.png)
![](./files/graphs/plot1_shiloh_buell.png)
![](./files/graphs/plot1_shiloh_johnston.png)
![](./files/graphs/plot1_shiloh_prentiss.png)
![](./files/graphs/plot1_shiloh_wallace.png)
![](./files/graphs/plot1_data_grant.png)

The battle of Shiloh is considered one of the bloodiest engagements of the Civil War. If we look at the mentions of *deserter(s)*, *killed*, and *wounded*, we discover the most significant spike of all these terms soon after the battle.

![](./files/graphs/plot_losses_at_war.png)

<!--
Such graphs can be used to monitor discussions of different topic in chronological perspective.
# interesting examples:
# deserters, killed,
# donelson (The Battle of Fort Donelson took place in early February of 1862),
# manassas (place of the Second Bull Run, fought in August 28–30, 1862),
# shiloh (Battle of Shiloh took place in April of 1862) <https://en.wikipedia.org/wiki/Battle_of_Shiloh>
-->


## Code: Line Graph

```{r engine='python', highlight=TRUE, eval=FALSE}

import re
import pandas
import matplotlib.pyplot as plt

template = """> Plotting data: %s"""

# load entities data
df = pandas.read_csv("entities.csv", sep="\t", header=0)
print(df)

def searchDispatchData(searchTerm, fileName="fromTagged"):
    print(template % (searchTerm))

    # processing our data
    df["month"] = [re.sub("-\d\d$", "-01", str(i)) for i in df["date"]]
    df["month"] = pandas.to_datetime(df["month"], format="%Y-%m-%d")

    # create zeros : two columns "month" and "searchTerm" (where all values are zeros)
    dfZeros = df[["month"]]
    dfZeros = dfZeros.reset_index()
    dfZeros[searchTerm] = 0
    dfZeros = dfZeros.drop_duplicates()

    # create a new table only with values that we want
    dfTemp = df[df.itemUnified.str.contains(searchTerm, na=False)]
    dfTemp = dfTemp[["month", "itemType"]]
    dfTemp = dfTemp.groupby(["month"]).count()
    dfTemp = dfTemp.reset_index()
    dfTemp[searchTerm] = dfTemp["itemType"]
    dfTemp = dfTemp[["month", searchTerm]]

    # merge with dfZeros (reason: we need explicit 0 values for dates when our search term is not found
    # otherwise the graph will be misleading as the line on the graph will be connecting only dates with
    # frequencies more than zero)
    dfTemp = dfTemp.append(dfZeros, ignore_index=True)
    dfTemp = dfTemp.groupby(["month"]).sum()
    dfTemp = dfTemp.sort_values(by="month")
    dfTemp = dfTemp.reset_index()

    # plotting the results
    plt.rcParams["figure.figsize"] = (20, 9)
    dfTemp.plot(x='month', y=searchTerm, legend=True, color='blue')

    plt.ylabel("absolute frequencies")
    plt.xlabel("dates (issues aggregated into months)")
    plt.title("entities with \"%s\" in them" % (searchTerm))
    plt.gca().yaxis.grid(linestyle=':')
    
    # the following line will simply open a pop-up with the graph
    # plt.show()

    # the following line will save the graph into a file
    fileNameToSave = "plot1_%s_%s.png" % (fileName, searchTerm)
    plt.savefig(fileNameToSave, dpi=300, bbox_inches="tight")
    plt.close("all")

searchDispatchData("shiloh", "shiloh")

```

## Missing Zeros Problem

Compare the following two graphs. The first one is "with zeros" and the second one "without zeros". 

![](./files/graphs/plot1_shiloh_shiloh.png)
![](./files/graphs/plot1_shiloh_shiloh_zeroErrorExample.png)
In the second graph we simply "collected" all mentions of Shiloh and plotted them with a line graph. The problem is that we have no explicit data on dates when Shiloh is not mentioned and, as a result, the graph suggests that Shiloh is mentioned quite a lot in the course of the year 1861. If we look at the first graph---where we have data for all the dates and each date when Shiloh is not mentioned has a value 0---we can see that Shiloh is mentioned only a couple of times in 1861 and then there is a huge spike of mentions in 1862, when the Battle of Shiloh took place.

The main takeaway from this is that you should always be mindful of how you are filtering your data and what strategies you use for plotting it. Line graph can make a false suggestion, when some x-values are missing. A more reliable graph in such cases would be a "lollipop" graph that is shown below --- lollipop graphs do not connect observations, and due to that they clearly show gaps in data.

![](./files/graphs/plot1_lollipop_shiloh_shiloh.png)

## Code: Lollipop Graph

```{r engine='python', highlight=TRUE, eval=FALSE}
import re
import pandas
import matplotlib.pyplot as plt

template = """> Plotting data: %s"""

# load entities data
df = pandas.read_csv("entities.csv", sep="\t", header=0)
print(df)

def searchDispatchData(searchTerm, fileName="fromTagged"):
    print(template % (searchTerm))

    # processing our data
    df["month"] = [re.sub("-\d\d$", "-01", str(i)) for i in df["date"]]
    df["month"] = pandas.to_datetime(df["month"], format="%Y-%m-%d")

    # create a new table only with values that we want
    dfTemp = df[df.itemUnified.str.contains(searchTerm, na=False)]
    dfTemp = dfTemp[["month", "itemType"]]
    dfTemp = dfTemp.groupby(["month"]).count()
    dfTemp = dfTemp.reset_index()
    dfTemp[searchTerm] = dfTemp["itemType"]
    dfTemp = dfTemp[["month", searchTerm]]

    # plotting the results - lollipop
    plt.rcParams["figure.figsize"] = (20, 9)
    plt.stem(dfTemp['month'], dfTemp[searchTerm])

    plt.ylabel("absolute frequencies")
    plt.xlabel("dates (issues aggregated into months)")
    plt.title("entities with \"%s\" in them" % (searchTerm))
    plt.gca().yaxis.grid(linestyle=':')

    # the following line will save the graph into a file
    fileNameToSave = "plot1_lollipop_%s_%s.png" % (fileName, searchTerm)
    plt.savefig(fileNameToSave, dpi=300, bbox_inches="tight")
    plt.close("all")

searchDispatchData("shiloh", "shiloh")
```

## Jupyter Notebook

Jupyter Notebook (<https://jupyter.org/>) is a web application for creating and sharing computational documents. It is a very popular way of writing analytical code in Python. It allows one to write and execute code right in your browser, to combine code with regular prose, ans, also, execute written code section by section (or, more correctly, cell by cell --- using the Jupyter lingo) --- which in some cases has a lot of advantages.

Jupyter Notebook can be installed in the manner that you are already familiar with:

`python -m pip install notebook` (or: `python3 -m pip install notebook`)

It can be then opened by running the following command in the command line tool of your choice (you should be in your working directory; make sure to restart your command line tool after installation):

`jupyter notebook`

After the Jupyter Notebook starts, you will be taken to your default browser and the first page of the Jupyter interface should show you the contents of the folder from which you started it. Something like this:

![](./files/jupyter01.png)

You can download [this already prepared notebook](./files/simple_analysis_with_pandas.ipynb), which should look like this when you open it:

![](./files/jupyter02.png)

## Homework{#HWL10}

- Rewrite the code in such a way that would allow you to graph *any kind of words or phrases* in a similar manner ("The Losses at War" graph is generated in this manner).;
- Annotate your script (i.e., add comment to every line of code describing what is happening there);

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfolder in your repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)

## Solution{#SolutionL10}

<!-- The solution will be posted here later... -->

Below is the solution to the homework.

```{r engine='python', highlight=TRUE, eval=FALSE}

import re
import os
import io
import pandas
import matplotlib.pyplot as plt

source = "./Dispatch/"
lof = os.listdir(source)

template = """
=================================================================================
= Plotting: %s (searching for: `\\b(%s)\\b`)
=================================================================================
"""


def searchDispatch(searchTermREGEX, searchTermPretty):
    print(template % (searchTermREGEX, searchTermPretty))

    counter = 0
    entities = []  # we will collect all extracted data here
    for f in lof:
        counter += 1
        if counter % 100 == 0:
            print(counter)

        if f.startswith("dltext"):  # fileName test
            newF = f.split(":")[-1] + ".yml"  # in fact, yml-like

            issueVar = []
            c = 0  # technical counter
            with open(source + f, "r", encoding="utf8") as f1:
                text = f1.read()
                date = re.search(r'<date value="([\d-]+)"', text).group(1)
                split = re.split("<div3 ", text)

                for s in split[1:]:
                    c += 1
                    s = "<div3 " + s  # a step to restore the integrity of items

                    try:
                        unitType = re.search(r'type="([^\"]+)"', s).group(1)
                    except:
                        unitType = "noType"

                    try:
                        header = re.search(r'<head.*</head>', s).group(0)
                        header = re.sub("<[^<]+>", "", header)

                    except:
                        header = "NO HEADER"

                    text = s
                    text = re.sub("<[^<]+>", " ", text)
                    text = re.sub(" +\n|\n +", "\n", text)
                    text = text.strip()
                    text = re.sub("\n+", ";;; ", text)
                    text = re.sub(" +", " ", text)
                    text = re.sub(r" ([\.,:;!])", r"\1", text)

                    itemID = date + "_" + unitType + "_%03d" % c

                    if len(re.sub("\W+", "", text)) != 0:
                        itemIdvar = "ID: " + itemID
                        dateVar = "DATE: " + date
                        unitType = "TYPE: " + unitType
                        header = "HEADER: " + header
                        # @§@ is used to replace ":", because in YML : is used as a divider between the key and value
                        text = "TEXT: " + text.replace(":", "@§@") + "\n\n"
                        var = "\n".join(
                            [itemIdvar, dateVar, unitType, header, text])

                        issueVar.append(var)

                        # NOW, WE CAN ADD SOME CODE TO PROCESS EACH ITEM AND COLLECT ALL INTO OUR TIDY DATA FORMAT
                        # STRUCTURE: itemID, dateVar, EXTRACTED_ITEM (type, unified_form, id)
                        # ADDING TO: entities (list)

                        results = re.findall(r"\b(%s)\b" %
                                             searchTermREGEX, text.lower())
                        matches = str(len(results))
                        tempVar = "\t".join([itemID, date, matches])
                        entities.append(tempVar)

    header = "\t".join(["itemID", "date", searchTermPretty])
    entitiesFinal = header + "\n" + "\n".join(entities)
    with open("search_results_%s.csv" % searchTermPretty, "w", encoding="utf8") as f9:
        f9.write(entitiesFinal)

    # reading our string of data into a pandas dataframe
    entitiesFinalStringIO = io.StringIO(entitiesFinal)
    df = pandas.read_csv(entitiesFinalStringIO, sep="\t", header=0)

    df["month"] = [re.sub("-\d\d$", "-01", str(i)) for i in df["date"]]
    df["month"] = pandas.to_datetime(df["month"], format="%Y-%m-%d")

    df = df[["month", searchTermPretty]]
    df = df.groupby(["month"]).sum()
    df = df.reset_index()

    # plot itself
    plt.rcParams["figure.figsize"] = (20, 9)
    df.plot(x='month', y=searchTermPretty, legend=True, color='red')

    plt.ylabel("absolute frequencies")
    plt.xlabel("dates (issues aggregated into months)")
    plt.title("References to \"%s\" (regex: `\\b(%s)\\b`)" %
              (searchTermPretty, searchTermREGEX))
    plt.gca().yaxis.grid(linestyle=':')

    # the following line will save the graph into a file
    plt.savefig("plot_%s.png" % searchTermPretty, dpi=300, bbox_inches="tight")
    plt.close("all")
    print("Done!")

searchDispatch("deserters?|killed|wounded", "losses_at_war")

```

<!--chapter:end:02-Lesson10.Rmd-->

# Lesson 11: Mapping Data (using tagged data)

## Preparing and Modeling Geospatial Data

In a similar manner we can also graph geographical data --- in combination with the temporary dimension. For this, however, we need to think through what kind of insight we can possibly get from geospatial data and how it should be prepared for mapping.

Let's start with a simple task: we will map the geospatial data from one of the previous lessons: [settlements from the al-Thurayya Gazetteer](./files/settlements.csv). Below is the code that produces an interactive map. (**Note**: it is possible to use `matplotlib` for mapping, but installation of needed components has become rather complicated recently, so we will proceed with a different library, `plotly`, which produced web-oriented maps.)

```{r engine='python', highlight=TRUE, eval=FALSE}

import plotly.express as px
import pandas as pd

althurayyaFile = "settlements.csv"

df = pd.read_csv(althurayyaFile, sep="\t", header=0)
print(df)

fig = px.scatter_geo(df, lon='lon', lat='lat', color="region_URI",
                     hover_name="names_eng_translit", size=df["top_size"]*df["top_size"],
                     projection="natural earth", fitbounds='locations')

fig.update_layout(
    title_text='Provinces of the classical Islamic World (IX-Xthe centuries CE)<br>(Click legend to toggle provinces)',
    showlegend=True,
    geo=dict(
        scope='world',
        landcolor='rgb(250, 250, 250)',
    )
)

fig.show()

# the following line will save the graph into an html file
fig.write_html("althurayya.html")

```

We can get the following map (for an interactive map, try [this link](./files/althurayya.html)):

![](./files/althurayya.png)
We can also map our geographical data dynamically, if we want to highlight its chronological dimension. Below is the example of the growth of US cities. [Click here for the interactive graph](./files/us_cities_pop.html).

![](./files/us_cities_pop.png)
The following code is used to produce the visualization above. Use this as a starting point for mapping data from the *Dispatch*. You can download the `us_cities_pop.csv` from [here](./files/us_cities_pop.csv). One thing to keep in mind, the chronological data must be passed to the function as `string`, not as `date` (that is why there is no conversion to the date format in the code below).

```{r engine='python', highlight=TRUE, eval=FALSE}
import plotly.express as px
import pandas as pd
import re

us_cities_data = "us_cities_pop.csv"
usCities = pd.read_csv(us_cities_data, sep=",", header=0)

# plot places
placesAll = usCities[['year', 'cityst', 'population', 'lat', 'lon']]
print(placesAll)

fig = px.scatter_geo(placesAll, lon='lon', lat='lat',
                     hover_name="cityst",
                     animation_frame="year",
                     size='population')

fig.update_layout(
    title_text='Growth of US cities (1790-2010)',
    showlegend=True,
    geo=dict(
        scope='usa',
        landcolor='rgb(235, 235, 235)',
    )
)

fig.show()
fig.write_html("us_cities_pop.html")

```


Now, let's get back to our Dispatch data. We have placenames tagged --- and we already have the data ready for processing. We can focus on a specific period of time (by filtering our data, including only dates that we are interested in). Then we need to calculate frequencies of mentions of placenames --- we can use these values to size dots on the final map. Unfortunately, a very important element is missing --- as you recall, our data does not have any coordinates. Luckily, we have unique identifiers from the Getty Thesaurus of Geographical Names. What we can do is to extract coordinates from the Thesaurus and connect them to corresponding placenames in our data.

- Getty Thesaurus is available for download here: <http://tgndownloads.getty.edu/> and here: <http://vocab.getty.edu/dataset/tgn/>
- You can download `explicit.zip` from <http://vocab.getty.edu/dataset/tgn/>
- There is a file `TGNOut_Coordinates.nt` with coordinates. You will need to write a script to extract coordinates from this file in order to use them. I suggest you convert this file into a usable CSV/TSV (the format is `N-triples`).

### Triples (`N-triples`)

What are triples? This is one of the most robust formats that can describe any kind of data. The main idea is that every triple is expressed through the structure `subject-predicate-object`. There are multiple formats for expressing triples (for example, RDF, N-Triples, etc.) ([*more details...*](https://en.wikipedia.org/wiki/Semantic_triple)).

Let's take a look at the following example from `TGNOut_Coordinates.nt`. There are three triples (`N-triples`) which together describe a specific object (which in our data would look like `tgn,1000007`).

```
<http://vocab.getty.edu/tgn/1000007-geometry> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/GeoCoordinates> .
<http://vocab.getty.edu/tgn/1000007-geometry> <http://schema.org/latitude> "-83.843"^^<http://www.w3.org/2001/XMLSchema#decimal> .
<http://vocab.getty.edu/tgn/1000007-geometry> <http://schema.org/longitude> "65.725"^^<http://www.w3.org/2001/XMLSchema#decimal> .
```

Each triple is stored on a separate line, ending with a period ("."). Elements of each triple are separated with spaces. Each triple has the same structure---`subject-predicate-object`---and can be read as follows:

- Triple 1: entity `tgn,1000007` has (expressed in a particular manner) geographical coordinates;
- Triple 2: entity `tgn,1000007` has decimal latitude `-83.843`;
- Triple 3: entity `tgn,1000007` has decimal longitude `65.725`;

You should have no problems writing a script that converts this data into a CSV with the structure: `ID`, `lon`, and `lat`. (Technically, you can also store these triples in a CSV/TSV format, but that would require additional conversion later.)

If you are not up to this challenge, you can use the CSV file that I have already prepared, to make your life a bit easier: [TGNOut_Coordinates.csv.zip](./files/TGNOut_Coordinates.csv.zip)

If everything done correctly, the final map may look like what you see below. [The complete interactive map can be found here.](./files/dispatch_1860_to_65.html)):

![](./files/dispatch_1860_to_65.png)

<!-- Here is the link to the description of how dynamic maps can be done: [Bubble Map with Animation](https://plotly.com/python/bubble-maps/). Use this example to produce something similar for the Dispatch data. -->

Last but not least, we may get interesting maps if we map places that co-occur with some specific names. An example of Gen. Sherman and Atlanta could be quite interesting (we have seen the graphs in the previous lesson), although in such cases a chronological graph of mentions of Atlanta---only in articles where Gen. Sherman is mentioned---may be a more clear option (see the preceding lesson). Please, try to code a solution that will do one or the other.

## Homework{#HWL11}

- Graph geographical data from the Dispatch. Experiment with different periods and different selections of placenames (for example, Richmond and Virginia will be the most frequently mentioned placenames---because the newspaper was published in Richmond, Virginia; this would mean that these two places will overshadow all other places.)
  - Suggestions:
    - you can create a separate data file that only has placenames (`articleID`, `date`, `placename`, `lat`, `lon`)
    - you can map places in a more complex manner. For example, map only placenames that are mentioned in the same articles as, say, General Sherman. In this case we may expect the maps to reflect his campaign through Georgia and the Carolinas (or, more precisely, the Dispatch's coverage of Gen. Sherman's campaign).
    - for interactive maps, you can use `Plotly` library: <https://plotly.com/python/bubble-maps/>.
- Finish the assigned task;
- Annotate your script (i.e., add comment to every line of code describing what is happening there);

- Additional materials:
  - Interactive maps with `Plotly`: <https://plotly.com/python/bubble-maps/>;
  - Basemap Matplotlib Toolkit: [Plotting data on a map (Example Gallery)](https://matplotlib.org/basemap/users/examples.html).
  - Python Data Science Handbook by Jake VanderPlas [Chapter 4. Visualization with Matplotlib](https://www.oreilly.com/library/view/python-data-science/9781491912126/ch04.html); [Geographic Data with Basemap](https://jakevdp.github.io/PythonDataScienceHandbook/04.13-geographic-data-with-basemap.html);
  - [How to plot data on maps in Jupyter using Matplotlib, Plotly, and Bokeh](https://www.bigendiandata.com/2017-06-27-Mapping_in_Jupyter/);
  - [Visualization: Mapping Global Earthquake Activity](http://introtopython.org/visualization_earthquakes.html).

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfolder in your repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)

## Solution{#SolutionL11}

<!--The solution for the homework will be added soon...-->

These are model solutions to the assignments to this lesson: 


```{r engine='python', highlight=TRUE, eval=FALSE}
# EXTRACT TGN DATA

import re
file = "TGNOut_Coordinates.nt"

csvDataDic = {}

with open(file) as f1:
    for line in f1:
        if "latitude" in line:
            line = line.split(" ")
            ID = re.search(r"(tgn/\d+)", line[0]).group(1).replace("/", ",")
            lat = line[2].split('"^^')[0][1:]

            if ID in csvDataDic:
                csvDataDic[ID][1] = lat
            else:
                csvDataDic[ID] = [0, 0, 0]
                csvDataDic[ID][0] = ID
                csvDataDic[ID][1] = lat

        elif "longitude" in line:
            line = line.split(" ")
            ID = re.search(r"(tgn/\d+)", line[0]).group(1).replace("/", ",")
            lon = line[2].split('"^^')[0][1:]

            if ID in csvDataDic:
                csvDataDic[ID][2] = lon
            else:
                csvDataDic[ID] = [0, 0, 0]
                csvDataDic[ID][0] = ID
                csvDataDic[ID][2] = lon
        else:
            pass

csvData = []
for k, v in csvDataDic.items():
    csvData.append("\t".join(v))

csvData = "itemId\tLAT\tLON\n" + "\n".join(csvData)
with open(file.replace(".nt", ".csv"), "w", encoding="utf8") as f9:
    f9.write(csvData)

print("Done!")
```

The following lines of code *merge* two tables in such a way that coordinates are added to all placenames---matching done on the `itemId` column. Two additional columns are added --- with latitudes and longitudes; when 

```{r engine='python', highlight=TRUE, eval=FALSE}
# MERGE TGN DATA

import pandas as pd

tgnDataFile = "TGNOut_Coordinates.csv"
dispatchDataFile = "entities.csv"

tgnData = pd.read_csv(tgnDataFile, sep="\t", header=0)
dispatchData = pd.read_csv(dispatchDataFile, sep="\t", header=0)

dispatchDataUpd = pd.merge(dispatchData, tgnData, how='left', on="itemId")

```

The final dataframe looks like what you see below. You can see that some rows have `NaN` values (`not a number`) in columns `LAT` and `LON` --- this essentially means that these rows could not be matched (because persnames and orgnames do not have TGN identifiers).

```
                     articleID        date   itemType                           itemUnified         itemID      LAT      LON
0       1864-04-28_article_001  1864-04-28  placename        gordonsville, orange, virginia    tgn,2111971  38.1333 -78.1833
1       1864-04-28_article_002  1864-04-28  placename  plymouth, washington, north carolina    tgn,2076159  35.8667 -76.7333
2       1864-04-28_article_002  1864-04-28  placename  plymouth, washington, north carolina    tgn,2076159  35.8667 -76.7333
3       1864-04-28_article_002  1864-04-28   persname         wessels,brigadier-general,,,,  wessels,h.,w.      NaN      NaN
4       1864-04-28_article_002  1864-04-28   persname                          lincoln,,,,,        lincoln      NaN      NaN
...                        ...         ...        ...                                   ...            ...      ...      ...
989392  1864-03-31_article_169  1864-03-31   persname                         hunt,,chas,,,      hunt,chas      NaN      NaN
989393  1864-03-31_article_170  1864-03-31   persname                            davis,,,,,    davis,waddy      NaN      NaN
989394  1864-03-31_article_170  1864-03-31  placename    albemarle, virginia, united states    tgn,2002137  38.0333 -78.5500
989395  1864-03-31_article_170  1864-03-31   persname                             cook,,,,,           cook      NaN      NaN
989396  1864-03-31_article_170  1864-03-31   persname                        turner,,geo,,,         turner      NaN      NaN
```

The following will generate "dynamic" maps:

```{r engine='python', highlight=TRUE, eval=FALSE}
# DYNAMIC MAPS

import plotly.express as px
import pandas as pd
import re

tgnDataFile = "TGNOut_Coordinates.csv"
dispatchDataFile = "entities.csv"

tgnData = pd.read_csv(tgnDataFile, sep="\t", header=0)
dispatchData = pd.read_csv(dispatchDataFile, sep="\t", header=0)

dispatchDataUpd = pd.merge(dispatchData, tgnData, how='left', on="itemID")

dispatchDataUpd["month"] = [re.sub("-\d\d$", "", str(i)) for i in dispatchDataUpd["date"]]
# FOR THIS MAP WE NEED DATES STORED AS STRINGS
# dispatchDataUpd["month"] = pd.to_datetime(dispatchDataUpd["month"], format="%Y-%m")

dfPlaces = dispatchDataUpd
dfPlaces = dfPlaces[dfPlaces.itemType.str.contains("placename", na=False)]

dfPersons = dispatchDataUpd
dfPersons = dfPersons[dfPersons.itemType.str.contains("persname", na=False)]

# plot places
placesAll = dfPlaces[['month', 'itemUnified', 'LAT', 'LON']]
placesAll['count'] = 1
placesAll = placesAll.groupby(['month', 'itemUnified', 'LAT', 'LON']).count()
placesAll = placesAll.reset_index()
print(placesAll)

fig = px.scatter_geo(placesAll, lon='LON', lat='LAT',
                     hover_name="itemUnified",
                     animation_frame="month", size='count')

fig.update_layout(
    title_text='Placenames in the <i>Dispatch</i> (1860-1865)',
    showlegend=True,
    geo=dict(scope='usa', landcolor='rgb(235, 235, 235)',
    )
)

#fig.show()
fig.write_html("dispatch_1860_to_65.html")
```

<!--chapter:end:02-Lesson11.Rmd-->

# Lesson 12: Topic Modeling *&* TF-IDF (automatic text analysis)

## Goals:

- Introduction to topic modeling, or how to classify texts by shared content (“topics”).

## Software

* python
* other python libraries
  - `wheel` (this package is helpful for the installation of `gensim` and `pyLDAvis`)
  - `nltk`
  - `gensim`
  - `spacy`
  - `pyLDAvis`
  - `matplotlib`
  - `numpy`
  - `pandas`
  - `plotly`
  - `pprint`
* *alternatively*: jupyter notebook (see the last part of the Lesson)

<!--
## Workbooks (jupyter notebooks)

- [for Windows](https://www.dropbox.com/s/b1jg5r3bmgforu5/TnT_L12_materials_final_Win.zip?dl=0)
- [for Mac and Linux](https://www.dropbox.com/s/zey9qtuht1tnsku/TnT_L12_materials_final.zip?dl=0)
-->

### Installing: on Windows

On Mac and Linux things are easy, just follow the commands below; for Windows things are trickier and the easiest way would be to use Anaconda <https://www.anaconda.com/distribution/#download-section>.

Please, download and install. Most packages will come with Anaconda distribution; others you can install through its interface.

**NB:** After Anaconda is installed, it is still better to install libraries from the terminal opened directly from Anaconda and using the following command `conda install -c conda-forge gensim` (the latest version is not available via Anaconda interface).
More details: <https://radimrehurek.com/gensim/install.html>


#### Installing: on Mac and Linux

Python libraries and additional data can be installed in the following manner

```bash
pip install nameOfLibrary
```

Although a better way would be (where `python3` is the exact version of `python` that you are using):

```bash
python3 -m pip install nameOfLibrary
```

Lemmatization library (although we are not going to be using it in the tutorial)

``` bash
python -m spacy download en
```

## Thinking about themes and topics

### First set of Examples

Let's take a look at the following three examples and think about what themes and topics they cover. More importantly, let's think about *how* we "assign" those themes and topics. What is out thinking process? What textual elements do we keep in mind when we argue that such and such text is about such and such topics?

**Example 1**: *March 27, 1862* --- Light Artillery

I am authorized by the Governor of Virginia to raise a Company of Light Artillery for the war. All those desirous of enlisting in this the most effective arm of the service, would do well to call at once at the office of Johnson & Guigon, Whig Building. Uniforms and subsistence furnished. A.B.GUIGON. mh 25—6t

**Example 2**: *August 17, 1864* --- Royal Marriages.

There is a story circulated in Germany, and some in Paris, that the match between the heir-apparent of the Imperial throne of Russia and the Princess Dagmar of Denmark having been definitively broken off, another is in the course of negotiation between His Imperial Highness and the Princess Helens of England.

**Example 3**: *June 22, 1863* --- News from Europe.

The steamship Scotia arrived at New York on Thursday from Europe, with foreign news to the 7th inst. The news is not important. The Confederate steamer Lord Clyde was searched by order of the British Government, but nothing contraband being found on board her she was permitted to sail. The Russians have been defeated near Grochoury by the Polish insurgents. The three Powers have sent an earnest note to Russia, asking for a representative Government, a general amnesty, and an immediate cessation of hostilities in Poland.

### Second Set of Examples

**Example 1**: *March 27, 1862* --- Light Artillery

I am authorized by the Governor of Virginia to raise a Company of Light Artillery for the war. All those desirous of enlisting in this the most effective arm of the service, would do well to call at once at the office of Johnson & Guigon, Whig Building. Uniforms and subsistence furnished. A.B.Guigon. mh 25—6t


**Example 2**: *July 17, 1861* --- Another improvement in fire-arms

Mr. T.W.Cofer, of Portsmouth, Va., has just completed an improvement in revolving fire-arms by which the process of loading is so much facilitated that the ordinary Colt or other Revolver, with this improvement attached, may be loaded and discharged with fourfold rapidity. Mr.Cofer left that place for Richmond yesterday for the purpose of securing a patent for his invention.


**Example 3**: *January 21, 186!* --- [Advertisement]

Just received and for sale low, a complete assortment of Colt's, Sharpe's, Smith *&* Wesson's, and other Revolvers. A call is solicited from those in want of anything in the Gun way. T.W.Tignor, Gun Maker, Main st., below the Market.


**Example 4**: *December 17, 1861* --- Unlawful shooting

On Saturday night, while Peter Padractti was pursuing the even tenor of his way on 11th and Cary streets, a man in a sky blue coat approached and discharged a pistol at him, and made use of most fearful expletives, which were quite enough to alarm a peaceful citizen. The strange individual, who was soon afterward arrested, gave his name as William Leftwich. When arraigned before the Mayor yesterday, he represented that he was a soldier on furlough, and was quite oblivious when he performed the deed which he now deeply deplored. The Mayor remanded him for indictment.

**Example 5**: *January 18, 1861* --- South Carolina Legislature

Charleston, S.C., January 17. The Senate report of the Military Committee, for raising four companies of artillery to meet the exigencies of the times, that demand South Carolina to be on a war footing, meets no opposers from any quarter, it being the general impression that she should have permanent military establishments for garrison purposes in the State fortifications. This establishment, the Committee recommend, to consist of a regiment of four companies, as it will form the nucleus around which volunteers and militia can rally, and will, besides, be a peace establishment, or furnish South Carolina 's quota in the army of the Southern Confederacy. The Senate went into secret session on the proposition to lay a submarine telegraph between Charleston, Morris' Island, Forts Moultrie, Johnson, Castle Pinckney, *&*c.

**Example 6**: *May 29, 1861* --- For the defence of Virginia

Twenty five men are wanted to enlist in the ranks of the Henrico Artillery under the command of Major Johnson H. Sands now encamped at Richmond College, one mile from this city, already mustered into service, and daily expecting orders to go into active duty. Rendezvous on Main street, between 7th and 8th. Hours between 10 A.M. and 3 P.M. H.Lansing Burrows, Recruiting Officer.

<!--
## Topic Modeling

The formal definition of *topic modeling* is "..." (Source: ...)
-->

## Implementation

**1. Preparing Data.** First, we need to make sure to convert the Dispatch into a suitable format. As I stressed before, TSV/CSV tabular format is one of the most universal. Let's go back to our reformatting script and change it in such a way that it collects data in the TSV format (TSV will help us to avoid the issue with commas in the CSV format) and aggregates all articles published in the same year in one file. Additionally, we will prepare texts for topic modeling in the following manner (See, section `PART II: PREPARING TEXT FOR TM ANALYSIS` in the script below):

- create a new column with texts of articles;
- remove all non word characters; convert everything to lower case; remove extra spaces;
- split resulting normalized text into lists of tokens;
- filter out stop words (provided in the long list `stop_words_custom`)
  - you can modify this list; as a rule of thumb, the stop word list is based on the frequency list of your data: you pick top 100 (or more) most frequent words; it is always a good idea to quickly check this list and may be keep some (for example, the word *Allãh* is one of the most frequent words in Arabic and you might want to keep it). 

Our script will create 6 files from our Dispatch data (one per each year; years 1860 and 1865 are incomplete) with all necessary columns:

- `id`: the unique ID of each text item ("article")
- `month`: month of publication
- `date`: day of publication
- `type`: type of the "article", if available
- `header`: header of the "article", if available
- `text`: text of the "article" in its original form (`\n` replaced with `;;;`)
- `textDataLists`: these are the filtered word lists that will be used in TM

You can get the script from here: [`topic_modeling_script01.py`](./files/tm/topic_modeling_script01.py). Make sure to change `source` and `target` folders accordingly; I would recommend to keep `target` as is, just make sure to create that folder.

Successful execution of this script will prepare you for the next step.

```{r engine='python', highlight=TRUE, eval=FALSE}
# the script converts Dispatch data into TSV and prepares text for topic modeling

import re, os, io
import gensim
from gensim.utils import simple_preprocess
import pandas as pd

source = "./Dispatch/"
target = "./Dispatch_Processed_TSV/"  # needs to be created beforehand!

def remove_words(texts, word_list_filter):
    return [[word for word in simple_preprocess(str(doc)) if word not in word_list_filter] for doc in texts]

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

def convertDispatchToCSV(source, target, YEAR):
    # PART I: AGGREGATING ALL into LARGE TSV
    print("Collecting data for year: %s" % YEAR)
    issueVar = []
    lof = os.listdir(source)
    for f in lof:
        if f.startswith("dltext"):  # fileName test
            c = 0  # technical counter
            with open(source + f, "r", encoding="utf8") as f1:
                text = f1.read()
                date = re.search(r'<date value="([\d-]+)"', text).group(1)

                if date[:4] == str(YEAR):
                    split = re.split("<div3 ", text)

                    for s in split[1:]:
                        c += 1
                        s = "<div3 " + s  # a step to restore the integrity of items

                        try:
                            unitType = re.search(
                                r'type="([^\"]+)"', s).group(1)
                        except:
                            unitType = "noType"

                        try:
                            header = re.search(
                                r'<head.*</head>', s).group(0)
                            header = re.sub("<[^<]+>", "", header)

                        except:
                            header = "NO HEADER"

                        text = s
                        text = re.sub("<[^<]+>", " ", text)
                        text = re.sub(r"\t", " ", text)
                        text = re.sub(" +\n|\n +", "\n", text)
                        text = text.strip()
                        text = re.sub("\n+", ";;; ", text)
                        text = re.sub(" +", " ", text)
                        text = re.sub(r" ([\.,:;!])", r"\1", text)

                        itemID = date + "_" + unitType + "_%04d" % c

                        if len(re.sub("\W+", "", text)) != 0:
                            var = "\t".join(
                                [itemID, date, unitType, header, text])
                            issueVar.append(var)

    print("\tcollected: %d items" % len(issueVar))
    issueNew = "\n".join(issueVar)
    header = "\t".join(["id", "date", "type", "header", "text"])
    final = header + "\n" + issueNew

    # PART II: PREPARING TEXT FOR TM ANALYSIS
    # Now, we prepare text data for TM (into a separate column)
    entitiesFinalStringIO = io.StringIO(final)
    df = pd.read_csv(entitiesFinalStringIO, sep="\t", header=0)

    dispatch = df
    dispatch = dispatch.reset_index(drop=True)
    dispatch["month"] = [re.sub("-\d\d$", "", str(i)) for i in dispatch["date"]]
    dispatch["month"] = pd.to_datetime(dispatch["month"], format="%Y-%m")
    dispatch["date"] = pd.to_datetime(dispatch["date"], format="%Y-%m-%d")

    # reorder columns
    dispatch = dispatch[["id", "month", "date", "type", "header", "text"]]

    # remove empty articles
    dispatch = dispatch[dispatch.type != "ad-blank"]

    # drop=True -- use it to avoid creating a new column with the old index values
    dispatch = dispatch.reset_index(drop=True)

    dispatch["textData"] = dispatch["text"]
    dispatch["textData"] = [re.sub("\W+", " ", str(i).lower()) for i in dispatch["textData"]]
    dispatch["textData"] = [re.sub(" +", " ", str(i).lower()) for i in dispatch["textData"]]

    dispatch["textDataLists"] = list(sent_to_words(dispatch["textData"].copy()))

    # you can expand the stop word list by adding more high frequency words
    stop_words_custom = ["the", "of", "and", "to", "in", "a", "that", "for", "on", "was", "is", "at", "be", "by",
                    "from", "his", "he", "it", "with", "as", "this", "will", "which", "have", "or", "are",
                    "they", "their", "not", "were", "been", "has", "our", "we", "all", "but", "one", "had",
                    "who", "an", "no", "i", "them", "about", "him", "two", "upon", "may", "there", "any",
                    "some", "so", "men", "when", "if", "day", "her", "under", "would", "c", "such", "made",
                    "up", "last", "j", "time", "years", "other", "into", "said", "new", "very", "five",
                    "after", "out", "these", "shall", "my", "w", "more", "its", "now", "before", "three",
                    "m", "than", "h", "th", "o'clock", "o", "old", "being", "left", "can", "s", "man", "only", "same",
                    "act", "first", "between", "above", "she", "you", "place", "following", "do", "per",
                    "every", "most", "near", "us", "good", "should", "having", "great", "also", "over",
                    "r", "could", "twenty", "people", "those", "e", "without", "four", "received", "p", "then",
                    "what", "well", "where", "must", "says", "g", "large", "against", "back", "through",
                    "b", "off", "few", "me", "sent", "while", "make", "number", "many", "much", "give",
                    "six", "down", "several", "high", "since", "little", "during", "away", "until",
                    "each", "year", "present", "own", "t", "here", "d", "found", "reported",
                    "right", "given", "age", "your", "way", "side", "did", "part", "long", "next", "fifty",
                    "another", "1st", "whole", "10", "still", "among", "3", "within", "get", "named", "f",
                    "l", "himself", "ten", "both", "nothing", "again", "n", "thirty", "eight", "took",
                    "never", "came", "called", "small", "passed", "just", "brought", "4", "further",
                    "yet", "half", "far", "held", "soon", "main", "8", "second", "however", "say",
                    "heavy", "thus", "hereby", "even", "ran", "come", "whom", "like", "cannot", "head",
                    "ever", "themselves", "put", "12", "cause", "known", "7", "go", "6", "once", "therefore",
                    "thursday", "full", "apply", "see", "though", "seven", "tuesday", "11", "done",
                    "whose", "let", "how", "making", "immediately", "forty", "early", "wednesday",
                    "either", "too", "amount", "fact", "heard", "receive", "short", "less", "100",
                    "know", "might", "except", "supposed", "others", "doubt", "set", "works"]

    # TEXT CLEANING
    dispatch["textDataLists"] = remove_words(dispatch["textDataLists"], stop_words_custom)
    dispatch = dispatch[["id", "month", "date", "type", "header", "text", "textDataLists"]]
    dispatch.to_csv(target + "Dispatch_%s_tmReady.tsv" % str(YEAR), sep="\t", index=False)


convertDispatchToCSV(source, target, 1860)
convertDispatchToCSV(source, target, 1861)
convertDispatchToCSV(source, target, 1862)
convertDispatchToCSV(source, target, 1863)
convertDispatchToCSV(source, target, 1864)
convertDispatchToCSV(source, target, 1865)
```

<!--
```{r comment=""}
cat(readLines('tm_1_processing_dispatch_data_to_TSV.py'), sep = '\n')
```
-->


**2. Necessary Libraries** ....

...

```{r engine='python', highlight=TRUE, eval=FALSE}

```

**3. Loading Data** ...

...

```{r engine='python', highlight=TRUE, eval=FALSE}

```

**4. Preprocessing Data** ...

...

```{r engine='python', highlight=TRUE, eval=FALSE}

```

**5. Training a Model** ...

```{r engine='python', highlight=TRUE, eval=FALSE}

```


**

## TF-IDF

TF-IDF is the most common automatic method for identifying keywords in texts. This method is the standard approach for identifying keywords. It was first proposed almost fifty years ago by @SPARCKJONESStatistical1972; @RamsayReading2011, in Chapter 1, offers a detailed humanistic explanation of this approach.

This approach is statistical in its nature and therefore requires a sizable collection of texts for meaningful results. In the case of small collections you are likely to observe that the selection of keywords for the same text change significantly with every new addition to the collection of texts. This is because the method takes into account frequencies of each word and the number of documents in which this word occurs. For example, if your collection includes 10 articles that discuss different aspects of the ʿAbbāsid caliphate, with some focused on politics, some on economics, and some on culture, the word "ʿAbbāsid" will most likely not be included into the list of suggested keywords (since all articles have this term), but words like "politics", "economics", and "culture" will be in this list, since they appear only in some articles. If you add ten other articles to your collection---and these will deal, say, with the Ottomans---then the word "ʿAbbāsid" will crawl up in the list of keywords for articles dealing with the ʿAbbasids. In other words, if your collection includes texts on the same broad topic (say, the ʿAbbāsids), the TF-IDF algorithm will assign high *keywordness* to those terms that point to narrower subjects within the broader subject of the ʿAbbāsids.

In cases when one has to run the topic modeling algorithm on a very large collection of texts, TF-IDF can be used in order to reduce the size of the corpus. That is to say, the corpus is first reduced to the TF-IDF abstractions and then the topic modeling algorithm is applied to these abstractions rather than to the complete initial texts.

TF-IDF abstractions can also be used to identify texts on similar topics across a large corpus of texts. In this approach one would calculate the distance between the vectors of TF-IDF abstractions. This approach mathematically checks whether to what extent two different documents share keywords and to what extent the numeric values of these keywords are similar: the more keywords they share and the more similar the keyword values are, the more similar the texts will be. ([More details...](https://maximromanov.github.io/univie2020/lesson-10.html).)

## Implementation in Jupyter Notebook

Jupyter notebooks is a very popular alternative to regular python scripts. You can try the same code with a jupyter notebook as well. There are some advantages and disadvantages to using them. JN can be quite useful when you are experimenting with some fast operations when you explore your dataset and want to try different things like graphs or data reshaping. If your code requires long time to run, JN maybe rather inconvenient to use. 

### jupyter notebook

From command line (in your working folder)

``` bash
# installing
pip install jupyter

# starting
jupyter notebook
```

<!--
## installing from a jupyter notebook

The required libraries can also be installed directly from your Jupyter notebook as shown below---note `!` in front of `pip`. (*Note*: You might need to use `pip3` instead of `pip`, depending on your overall `python` setup)

```python
# installing
!pip install nltk
!pip install gensim
!pip install spacy
!pip install pyLDAvis
```
-->

Your default browser should open something like this:

![](./files/jupyter01.png)

Click on an `*.ipynb` file to open a notebook.

## Files & Scripts

* [Script that converts the Dispatch to TSV](./files/tm/0_processing_dispatch_data_to_TSV.py) files necessary for the Jupyter Notebook; generate these TSV files first, then you can start the Notebook
* [TM - Jupyter Notebook](./files/tm/0_tModeling_Dispatch_clean.ipynb)
  * For Windows: you will need to deactivate `%time` in those blocks of code that have it (either delete it, or comment it out with `#`).

## Homework{#HWL12}

- Finish the assigned task;
- Annotate your script (i.e., add comment to every line of code describing what is happenning there);

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfolder in your repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)


## Solution{#SolutionL12}

The solutions will be added soon...

```{r engine='python', highlight=TRUE, eval=FALSE}
# the script converts Dispatch data into TSV and prepares text for topic modeling

import re, os, io
import gensim
from gensim.utils import simple_preprocess
import pandas as pd

source = "./Dispatch/"
target = "./Dispatch_Processed_TSV/"  # needs to be created beforehand!

def remove_words(texts, word_list_filter):
    return [[word for word in simple_preprocess(str(doc)) if word not in word_list_filter] for doc in texts]

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

def convertDispatchToCSV(source, target, YEAR):
    print("Collecting data for year: %s" % YEAR)
    issueVar = []
    lof = os.listdir(source)
    for f in lof:
        if f.startswith("dltext"):  # fileName test
            c = 0  # technical counter
            with open(source + f, "r", encoding="utf8") as f1:
                text = f1.read()
                date = re.search(r'<date value="([\d-]+)"', text).group(1)

                if date[:4] == str(YEAR):
                    split = re.split("<div3 ", text)

                    for s in split[1:]:
                        c += 1
                        s = "<div3 " + s  # a step to restore the integrity of items

                        try:
                            unitType = re.search(
                                r'type="([^\"]+)"', s).group(1)
                        except:
                            unitType = "noType"

                        try:
                            header = re.search(
                                r'<head.*</head>', s).group(0)
                            header = re.sub("<[^<]+>", "", header)

                        except:
                            header = "NO HEADER"

                        text = s
                        text = re.sub("<[^<]+>", " ", text)
                        text = re.sub(r"\t", " ", text)
                        text = re.sub(" +\n|\n +", "\n", text)
                        text = text.strip()
                        text = re.sub("\n+", ";;; ", text)
                        text = re.sub(" +", " ", text)
                        text = re.sub(r" ([\.,:;!])", r"\1", text)

                        itemID = date + "_" + unitType + "_%04d" % c

                        if len(re.sub("\W+", "", text)) != 0:
                            var = "\t".join(
                                [itemID, date, unitType, header, text])
                            issueVar.append(var)

    print("\tcollected: %d items" % len(issueVar))
    issueNew = "\n".join(issueVar)
    header = "\t".join(["id", "date", "type", "header", "text"])
    final = header + "\n" + issueNew


    # Now, we prepare text data for TM (into a separate column)
    entitiesFinalStringIO = io.StringIO(final)
    df = pd.read_csv(entitiesFinalStringIO, sep="\t", header=0)

    dispatch = df
    # drop=True -- use it to avoid creating a new column with the old index values
    dispatch = dispatch.reset_index(drop=True)

    # add a column with all dates of each month changed to 1 (we can use that to aggregate our data into months)
    dispatch["month"] = [re.sub("-\d\d$", "", str(i)) for i in dispatch["date"]]

    # reorder columns
    dispatch = dispatch[["id", "month", "date", "type", "header", "text"]]

    dispatch["month"] = pd.to_datetime(dispatch["month"], format="%Y-%m")
    dispatch["date"] = pd.to_datetime(dispatch["date"], format="%Y-%m-%d")

    dispatch = dispatch[dispatch.type != "ad-blank"]
    dispatch = dispatch.reset_index(drop=True)

    dispatch["textData"] = dispatch["text"]
    dispatch["textData"] = [re.sub("\W+", " ", str(i).lower()) for i in dispatch["textData"]]
    dispatch["textData"] = [re.sub(" +", " ", str(i).lower()) for i in dispatch["textData"]]

    dispatch["textDataLists"] = list(sent_to_words(dispatch["textData"].copy()))

    # you can expand the stop word list by adding more high frequency words
    stop_words_custom = ["the", "of", "and", "to", "in", "a", "that", "for", "on", "was", "is", "at", "be", "by", "from", "his", "he", "it", "with", "as", "this", "will", "which", "have", "or", "are", "amp",
                    "they", "their", "not", "were", "been", "has", "our", "we", "all", "but", "one", "had", "who", "an", "no", "i", "them", "about", "him", "two", "upon", "may", "there", "any",
                    "some", "so", "men", "when", "if", "day", "her", "under", "would", "c", "such", "made", "up", "last", "j", "time", "years", "other", "into", "said", "new", "very", "five",
                    "after", "out", "these", "shall", "my", "w", "more", "its", "now", "before", "three", "m", "than", "h", "th", "o'clock", "o", "old", "being", "left", "can", "s", "man", "only", "same",
                    "act", "first", "between", "above", "she", "you", "place", "following", "do", "per", "every", "most", "near", "us", "good", "should", "having", "great", "also", "over",
                    "r", "could", "twenty", "people", "those", "e", "without", "four", "received", "p", "then", "what", "well", "where", "must", "says", "g", "large", "against", "back", "through",
                    "b", "off", "few", "me", "sent", "while", "make", "number", "many", "much", "give", "six", "down", "several", "high", "since", "little", "during", "away", "until",
                    "each", "year", "present", "own", "t", "here", "d", "found", "reported", "right", "given", "age", "your", "way", "side", "did", "part", "long", "next", "fifty",
                    "another", "1st", "whole", "10", "still", "among", "3", "within", "get", "named", "f", "l", "himself", "ten", "both", "nothing", "again", "n", "thirty", "eight", "took",
                    "never", "came", "called", "small", "passed", "just", "brought", "4", "further", "yet", "half", "far", "held", "soon", "main", "8", "second", "however", "say",
                    "heavy", "thus", "hereby", "even", "ran", "come", "whom", "like", "cannot", "head", "ever", "themselves", "put", "12", "cause", "known", "7", "go", "6", "once", "therefore",
                    "thursday", "full", "apply", "see", "though", "seven", "tuesday", "11", "done", "whose", "let", "how", "making", "immediately", "forty", "early", "wednesday",
                    "either", "too", "amount", "fact", "heard", "receive", "short", "less", "100",
                    "know", "might", "except", "supposed", "others", "doubt", "set", "works"]

    # TEXT CLEANING
    dispatch["textDataLists"] = remove_words(dispatch["textDataLists"], stop_words_custom)
    dispatch = dispatch[["id", "month", "date", "type", "header", "text", "textDataLists"]]
    dispatch.to_csv(target + "Dispatch_%s_tmReady.tsv" % str(YEAR), sep="\t", index=False)


convertDispatchToCSV(source, target, 1860)
convertDispatchToCSV(source, target, 1861)
convertDispatchToCSV(source, target, 1862)
convertDispatchToCSV(source, target, 1863)
convertDispatchToCSV(source, target, 1864)
convertDispatchToCSV(source, target, 1865)
```


```{r engine='python', highlight=TRUE, eval=FALSE}


```

<!--chapter:end:02-Lesson12.Rmd-->

# Lesson 13: Social Network Analysis

## ...

...

## Homework{#HWL13}

- Finish the assigned task;
- Annotate your script (i.e., add comment to every line of code describing what is happenning there);

**Submitting homework:**

* Homework assignment must be submitted by the beginning of the next class;
* Now, that you know how to use GitHub, you will be submitting your homework pushing it to github:
  * Create a relevant subfolder in your repository and place your HW files there; push them to your GitHub account;
	* Email me the link to your repository with a short message (Something like: *I have completed homework for Lesson 3, which is uploaded to my repository ... in subfolder `L03`*)


## Solution{#SolutionL13}

Below is the solution to the homework...

```{r engine='python', highlight=TRUE, eval=FALSE}


```

<!--chapter:end:02-Lesson13.Rmd-->

`r if (knitr::is_html_output()) '
# Appendix {-}
'`


* [Article 01](./files/articles/1860-11-12_article_01.txt)
* [Article 02](./files/articles/1860-11-12_article_02.txt)
* [Article 03](./files/articles/1860-11-12_article_03.txt)
* [Article 04](./files/articles/1860-11-12_article_04.txt)
* [Article 05](./files/articles/1860-11-12_article_05.txt)
* [Article 06](./files/articles/1860-11-12_article_06.txt)
* [Article 07](./files/articles/1860-11-12_article_07.txt)
* [Article 08](./files/articles/1860-11-12_article_08.txt)
* [Article 09](./files/articles/1860-11-12_article_09.txt)
* [Article 10](./files/articles/1860-11-12_article_10.txt)
* [Article 11](./files/articles/1860-11-12_article_11.txt)
* [Article 12](./files/articles/1860-11-12_article_12.txt)
* [Article 13](./files/articles/1860-11-12_article_13.txt)
* [Article 14](./files/articles/1860-11-12_article_14.txt)
* [Article 15](./files/articles/1860-11-12_article_15.txt)

<!--chapter:end:99-appendix.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:99-references.Rmd-->

